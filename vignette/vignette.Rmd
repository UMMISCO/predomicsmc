---
title: "vignette"
author: "Fabien"
date: "2023-10-04"
output: html_document
knitr: 
  duplicate.label: "allow"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load the packages, message=FALSE, warning=FALSE}
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
```

# Loading and Analysis of Used Data

```{r}
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Create an index vector for data partitioning
set.seed(42)  
y = as.vector(yvec_trie)
X = X_general

# Number of desired samples in each class
nombre_echantillons_par_classe <- min(table(y))

# Function to balance the classes
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe,seed =123) {
  classes <- unique(y)
  indices_equilibres <- integer(0)
  
  for (classe in classes) {
    indices_classe <- which(y == classe)
    set.seed(seed)
    indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
  }
  
  return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}

donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X

# Verify the distribution after balancing

set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)

# Split yvec_trie into 80% train and 20% test
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[,indices_division]
X.test <- X_equilibre[,-indices_division]

table(y)
table(y.test)
dim(X)
dim(X.test)
```
#Classifying using the terga1 algorithm

```{r setting the classifier}

clf <- terga1_mc(nCores = 1,
              seed = 1,
              plot = TRUE
)
printy(clf) # print the object for more information
isClf(clf)  # test whether the object is a classifier
class(clf)  # the class of the classifier object
```



#Classifying using the terbeam algorithm

```{r setting the classifier}
clf <- terBeam_mc(sparsity = c( 2, 3, 4, 5), 
                                   max.nb.features = 1000,
                                   seed = 1,
                                   nCores = 1,
                                   evalToFit = "accuracy_",
                                   objective = "auc",
                                   experiment.id = "terBeam_mc",
                                   experiment.save = "nothing")
  
                      
  printy(clf)  

```


# Running the learner experiment

```{r running experiment, echo=TRUE, fig.width=5, warning=FALSE}
runit = TRUE
if(runit)
{
  res_clf <- fit_mc(X = X, y = y, clf = clf,approch="ova", cross.validate = TRUE, nfolds = 10); # class(res_clf)
  # save results
  save(res_clf, clf, file = "res_clf.rda", compression_level = 9)
}

# ... Database X is not a matrix! Converting ...
# ... Classification mode, computing factor(y) for speedup and robustness
# ... Loading feature correlation for speedup
# ... Correlation file loaded
# ... Storing data in the classifier object for speedup
# ... Computing ternary coefficients for speedup
# ... One seed found, setting by default
# ... Running the classifier terga2 with a single CPU
# ... Second and faster version of terga fitting based on Genetic Algorithm heuristics ...
# ... Cross validation mode
# ... Starting cross validation not in parallel
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ... Learning process is finished succesfuly
# ... Thank you for using Predomics. Don't forget to digest the results now.
# 
# [1] "experiment" "predomics" 

```

```{r load results, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
if(!runit)
{
  load("res_clf.rda")
}
```

# Digesting the results

```{r digesting results, fig.height=8, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE}
res_clf.dig_terga1ovobeamova <- digest_MC(obj = res_clf, penalty = 0.75/100, plot = TRUE)
```

### best model

```{r best model, fig.width=7, warning=FALSE}

# get the best model
mod <- res_clf.dig_terga1ova$best$model

```


```{r}
mod4 <- res_clf.dig_terga1ovobeamova$best$model
```







#Sauvegarde terga1 ova cv 10
```{r}
modterga1_ovacv = mod
```


```{r}
sauve1ovatergacv = modterga1_ovacv 
```



# Prediction one versus all

```{r}
predict_ova <- predict_ova(mod, y = y.test, X = X.test, clf, force.re.evaluation = TRUE)

```






```{r}
predict_ova4 <- predict_ova(mod4, y = y.test, X = X.test, clf, force.re.evaluation = TRUE)
```

# Aggregation Max Voting score one versus all and table  contingency
```{r}
aggregate_predictions_Max_Voting <- aggregate_predictions_Max_Voting_ova(list_predictions = predict_ova, y)
aggregate_predictions_Max_Voting
y_pred = aggregate_predictions_Max_Voting
table_contingency <- table(y.test, y_pred)
print(table_contingency)
```


#terbeam
```{r}
aggregate_predictions_Max_Voting4 <- aggregate_predictions_Max_Voting_ova(list_predictions = predict_ova4, y.test)
aggregate_predictions_Max_Voting4
y_pred = aggregate_predictions_Max_Voting4
table_contingency4 <- table(y.test, y_pred)
print(table_contingency4)
```

#Min

```{r}
aggregate_predictions_Min_Voting42 <- aggregate_predictions_Min_Voting_ova(list_predictions = predict_ova4, y.test)
aggregate_predictions_Min_Voting42
y_pred = aggregate_predictions_Min_Voting42
table_contingency42 <- table(y.test, y_pred)
print(table_contingency42)
```








# Aggregation Min Voting score one versus all and table  contingency
```{r}
aggregate_predictions_Min_Voting <- aggregate_predictions_Min_Voting_ova(list_predictions = predict_ova, y.test)
aggregate_predictions_Min_Voting
y_pred = aggregate_predictions_Min_Voting
table_contingency <- table(y.test, y_pred)
print(table_contingency)
```

#Evaluation of metrics for the one versus all approach

```{r}
Results <- EvaluateAdditionnelGlobaleMetrics(predictions = y_pred, actual_labels = y.test)
Results
```







```{r}
Results41 <- EvaluateAdditionnelGlobaleMetrics(predictions = y_pred, actual_labels = y.test)
Results41
```


#Prediction one versus one







```{r}
modovo <- res_clf.dig_terga1ovo$best$model

```


```{r}
predict_ovo <- predict_ovo(modovo, y = y.test, X = X.test, clf, force.re.evaluation = TRUE )

```

###  Aggregation : aggregate_predictions_Max_Voting one versus one and table  contingency
```{r}
aggregate_majoritaire_vote_ovo <- aggregate_majoritaire_vote_ovo(predictions_list = predict_ovo)
aggregate_majoritaire_vote_ovo
y_pred = aggregate_majoritaire_vote_ovo
table_contingency <- table(y.test, y_pred)
print(table_contingency)
```


###Evaluation of metrics for the one versus all approach

```{r}
Results <- EvaluateAdditionnelGlobaleMetrics(predictions = y_pred, actual_labels = y.test)
Results
```

```{r}
best.model.test_ovoterga <- evaluateModel_mc(mod = modovo, X = X.test, y = y.test, clf = clf, eval.all = TRUE, force.re.evaluation = TRUE, mode = "test")
best.model.test_ovoterga$auc_
```


```{r}
modovobeam = res_clf.dig_terga1ovobeam$best$model
```

```{r}
predict_ovo1 <- predict_ovo(modovobeam, y = y.test, X = X.test, clf, force.re.evaluation = TRUE )
```

```{r}
aggregate_majoritaire_vote_ovo1 <- aggregate_majoritaire_vote_ovo(predictions_list = predict_ovo1)
aggregate_majoritaire_vote_ovo1
y_pred = aggregate_majoritaire_vote_ovo1
table_contingency <- table(y.test, y_pred)
print(table_contingency)
```

```{r}
Results <- EvaluateAdditionnelGlobaleMetrics(predictions = y_pred, actual_labels = y.test)
Results
```


```{r}
best.model.test_ovobeamova <- evaluateModel_mc(mod = mod, X = X.test, y = y.test, clf = clf, eval.all = TRUE, force.re.evaluation = TRUE, mode = "test",approch = "ova")
best.model.test_ovobeamova$auc_
```

















# State of the art 


## Random Forest

```{r warning=FALSE}
library(randomForest)
library(e1071)
```

```{r warning=FALSE}
library(caret)
library(randomForest)
library(e1071)
X_train = t(X)
y_train = y
X_test = t(X.test)
y_test =y.test
y_train <- as.factor(y_train)
y_test <- as.factor(y_test)


# Define 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)

# Train the multiclass random forest model with 10-fold cross-validation
rf_model <- train(x = X_train, y = y_train, method = "rf", trControl = train_control)

# Prediction on the training data
train_pred <- predict(rf_model, X_train)

# Calculation of metrics for the training data
train_metrics <- confusionMatrix(train_pred, y_train)

# Calculation of the AUC for training
roc_train <- multiclass.roc(y_train, as.numeric(train_pred))
auc_train <- auc(roc_train)

# Displaying metrics for training
print(train_metrics)
print(auc_train)

# Prediction on the test data
test_pred <- predict(rf_model, X_test)

# Calculation of metrics for the test data
test_metrics <- confusionMatrix(test_pred, y_test)

# Calculation of the AUC for the test data.
roc_test <- multiclass.roc(y_test, as.numeric(test_pred))
auc_test <- auc(roc_test)

# Displaying metrics for the test data
print(test_metrics)
print(auc_test)

```




```{r warning=FALSE}
# Calculating metrics for the training data
train_accuracy <- train_metrics$overall["Accuracy"]
train_precision <- mean(train_metrics$byClass[,"Precision"], na.rm = TRUE)
train_recall <- mean(train_metrics$byClass[,"Recall"], na.rm = TRUE)
train_auc <- auc_train
# Calculer le score F1 pour les données d'entraînement
f1_train <- 2 * (train_precision * train_recall) / (train_precision + train_recall)

# Afficher le score F1 pour les données d'entraînement
print(paste("Train F1 Score:", f1_train))


 

# Displaying metrics for the training data
print(paste("Train Accuracy:", train_accuracy))
print(paste("Train Precision:", train_precision))
print(paste("Train Recall:", train_recall))
print(paste("Train AUC:", train_auc))

# Calculating metrics for the test data
test_metrics <- confusionMatrix(test_pred, y_test)
test_accuracy <- test_metrics$overall["Accuracy"]
test_precision <- mean(test_metrics$byClass[,"Precision"], na.rm = TRUE)
test_recall <- mean(test_metrics$byClass[,"Recall"], na.rm = TRUE)

# Calculating the AUC for the test data
roc_test <- multiclass.roc(y_test, as.numeric(test_pred))
test_auc <- auc(roc_test)
# Calculer et afficher le score F1 pour les données de test
test_precision <- mean(test_metrics$byClass[,"Precision"], na.rm = TRUE)
test_recall <- mean(test_metrics$byClass[,"Recall"], na.rm = TRUE)
f1_test <- 2 * (test_precision * test_recall) / (test_precision + test_recall)

print(paste("Test F1 Score:", f1_test))
# Displaying metrics for the test data
print(paste("Test Accuracy:", test_accuracy))
print(paste("Test Precision:", test_precision))
print(paste("Test Recall:", test_recall))
print(paste("Test AUC:", test_auc))

```


```{r warning=FALSE, fig.height=4.5, fig.width=6, message=FALSE}
library(ggplot2)

# Example of metrics for the training data
accuracy_train <- train_accuracy
precision_train <- train_precision
AUC_train <- train_auc
recall_train <- train_recall
f1_train <- f1_train

# Example of metrics for the test data
accuracy_test <- test_accuracy
precision_test <- test_precision
AUC_test <- test_auc
recall_test <- test_recall
f1_test <- f1_test

# Create a data frame for the training metrics
train_metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "Recall"),
  Value = c(accuracy_train, precision_train, f1_train, recall_train),
  Dataset = "Training"
)

# Create a data frame for the test metrics
test_metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "Recall"),
  Value = c(accuracy_test, precision_test, f1_test, recall_test),
  Dataset = "Test"
)

# Combine the training and test metrics
all_metrics <- rbind(train_metrics, test_metrics)

# Create the graph
plot <- ggplot(all_metrics, aes(x = Metric, y = Value, fill = Dataset)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
  labs(title = "Comparison of test and training metrics RF",
       x = "Metrics", y = "Values") +
  theme_minimal()

# Display the graph
print(plot)

```



## Regression Logistique

```{r warning=FALSE}
library(nnet)
# Create a data.frame from matrix X
library(glmnet)
# Utilize cv.glmnet with 10-fold cross-validation
df <- as.data.frame(t(X))
df$y <- factor(y)
cv_model <- cv.glmnet(as.matrix(df[, -ncol(df)]), df$y, family = "multinomial", alpha = 1, nfolds = 10)

# Get the best model based on cross-validation
best_model <- cv_model$glmnet.fit

# Obtain predictions directly from cross-validation
predictions <- predict(cv_model, newx = as.matrix(df[, -ncol(df)]), s = "lambda.min", type = "class")
predictions <- factor(predictions)

# Check the dimensions and create the confusion matrix
if(length(predictions) == length(df$y)) {
  confusionMatrix(predictions, df$y)
} else {
  print("The dimensions of the predictions do not match those of the ground truth")
}

```



```{r warning=FALSE}
conf_matrix <- confusionMatrix(predictions, df$y)

# Calcul de l'exactitude
accuracy_train <- conf_matrix$overall[["Accuracy"]]

# Calcul de la précision moyenne de chaque classe
precision_train <- mean(conf_matrix$byClass[ , "Precision"])

# Calcul de l'AUC en utilisant la précision équilibrée moyenne (Balanced Accuracy)
AUC_train <- mean(conf_matrix$byClass[ , "Balanced Accuracy"])

# Calcul du rappel moyen (Sensibilité) de chaque classe
recall_train <- mean(conf_matrix$byClass[ , "Sensitivity"])

# Calcul du score F1 pour les données d'entraînement
F1_train <- 2 * (precision_train * recall_train) / (precision_train + recall_train)

# Affichage des métriques pour les données d'entraînement
print(paste("Train Accuracy:", accuracy_train))
print(paste("Train Precision:", precision_train))
print(paste("Train Recall (Sensitivity):", recall_train))
print(paste("Train AUC (Balanced Accuracy):", AUC_train))
print(paste("Train F1 Score:", F1_train))


```



```{r warning=FALSE}
predictions_test1 <- predict(cv_model, newx = as.matrix(X_test), type = "class")
library(caret)
predictions_test1 <- as.factor(predictions_test1)


```


```{r warning=FALSE}
# Create the confusion matrix for predictions on the test dataset
conf_matrix_test <- confusionMatrix(predictions_test1, y_test)

# Calculate accuracy, precision, AUC, and recall for the test dataset
# Calculate accuracy, precision, AUC, and recall for the test dataset
accuracy_test1 <- conf_matrix_test$overall[["Accuracy"]]
precision_test1 <- mean(conf_matrix_test$byClass[ , "Precision"], na.rm = TRUE)
AUC_test1 <- mean(conf_matrix_test$byClass[ , "Balanced Accuracy"], na.rm = TRUE)
recall_test1 <- mean(conf_matrix_test$byClass[ ,"Sensitivity"], na.rm = TRUE)
# Calcul du score F1 pour les données d'entraînement
F1_test1 <- 2 * (precision_test1 * recall_test1) / (precision_test1 + recall_test1)

```



```{r warning=FALSE, fig.height=4.5, fig.width=6, message=FALSE}
library(ggplot2)

# Create a data frame for the training metrics
train_metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "Recall"),
  Value = c(accuracy_train, precision_train, F1_train, recall_train),
  Dataset = "Training"
)

# Create a data frame for the test metrics
test_metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "Recall"),
  Value = c(accuracy_test1, precision_test1, F1_test1, recall_test1),
  Dataset = "Test"
)

# Combine the training and test metrics
all_metrics <- rbind(test_metrics,train_metrics)

# Create the graph
plot <- ggplot(all_metrics, aes(x = Metric, y = Value, fill = Dataset)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
  labs(title = "Comparison of test and training metrics MLR",
       x = "Metrics", y = "Values") +
  theme_minimal()

# Display the graph
print(plot)

```


## SVM


```{r warning=FALSE}
library(caret)
library(pROC)






# Train an SVM model with radial kernel and 10-fold cross-validation
svm_model <- train(x = X_train, y = y_train, method = "svmRadial", trControl = trainControl(method = "cv", number = 10))

# Make predictions with the SVM model
svm_predictions_train <- predict(svm_model, X_train)
svm_predictions_test <- predict(svm_model, X_test)

# Calculate the accuracy for the training and test sets
accuracy_train <- confusionMatrix(data = svm_predictions_train, reference = y_train)$overall['Accuracy']
accuracy_test <- confusionMatrix(data = svm_predictions_test, reference = y_test)$overall['Accuracy']

# Calculate precision, recall, and AUC for the training and test sets
precision_train <- confusionMatrix(data = svm_predictions_train, reference = y_train)$byClass[, 'Pos Pred Value']
precision_test <- confusionMatrix(data = svm_predictions_test, reference = y_test)$byClass[, 'Pos Pred Value']

recall_train <- confusionMatrix(data = svm_predictions_train, reference = y_train)$byClass[, 'Sensitivity']
recall_test <- confusionMatrix(data = svm_predictions_test, reference = y_test)$byClass[, 'Sensitivity']
F1_train <- 2 * (precision_train * recall_train) / (precision_train + recall_train)
F1_train = mean(F1_train)
auc_train <- roc(y_train, as.numeric(svm_predictions_train))$auc
auc_test <- roc(y_test, as.numeric(svm_predictions_test))$auc

precision_train = mean(precision_train)
precision_test = mean(precision_test)
recall_train = mean(recall_train)
recall_test = mean(recall_test)
F1_test <- 2 * (precision_test * recall_test) / (precision_test + recall_test)
F1_test <- mean(F1_test)
```

```{r warning=FALSE, fig.height=4.5, fig.width=6, message=FALSE}
library(ggplot2)

# Create a data frame for the training metrics
train_metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "Recall"),
  Value = c(accuracy_train, precision_train, F1_train, recall_train),
  Dataset = "Training"
)

# Create a data frame for the test metrics
test_metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "Recall"),
  Value = c(accuracy_test, precision_test, F1_test, recall_test),
  Dataset = "Test"
)

# Combine the training and test metrics
all_metrics <- rbind(test_metrics,train_metrics)

# Create the graph
plot <- ggplot(all_metrics, aes(x = Metric, y = Value, fill = Dataset)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
  labs(title = "Comparison of test and training metrics SVM",
       x = "Metrics", y = "Values") +
  theme_minimal()

# Display the graph
print(plot)
```
























