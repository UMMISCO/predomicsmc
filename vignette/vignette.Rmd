---
title: "vignette"
author: "Fabien"
date: "2023-10-04"
output: html_document
knitr: 
  duplicate.label: "allow"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load the packages, message=FALSE, warning=FALSE}
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
```

# Loading and Analysis of Used Data

```{r}
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Create an index vector for data partitioning
set.seed(42)  
y = as.vector(yvec_trie)
X = X_general

# Number of desired samples in each class
nombre_echantillons_par_classe <- min(table(y))

# Function to balance the classes
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe,seed =123) {
  classes <- unique(y)
  indices_equilibres <- integer(0)
  
  for (classe in classes) {
    indices_classe <- which(y == classe)
    set.seed(seed)
    indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
  }
  
  return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}

donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X

# Verify the distribution after balancing

set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)

# Split yvec_trie into 80% train and 20% test
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[,indices_division]
X.test <- X_equilibre[,-indices_division]

table(y)
table(y.test)
dim(X)
dim(X.test)
```
#Classifying using the terga1 algorithm

```{r setting the classifier}

clf <- terga1_mc(nCores = 1,
              seed = 1,
              plot = TRUE
)
printy(clf) # print the object for more information
isClf(clf)  # test whether the object is a classifier
class(clf)  # the class of the classifier object
```



#Classifying using the terbeam algorithm

```{r setting the classifier}
clf <- terBeam_mc(sparsity = c( 2, 3, 4, 5), 
                                   max.nb.features = 1000,
                                   seed = 1,
                                   nCores = 1,
                                   evalToFit = "accuracy_",
                                   objective = "auc",
                                   experiment.id = "terBeam_mc",
                                   experiment.save = "nothing")
  
                      
  printy(clf)  

```


# Running the learner experiment

```{r running experiment, echo=TRUE, fig.width=5, warning=FALSE}
runit = TRUE
if(runit)
{
  res_clf <- fit_mc(X = X, y = y, clf = clf,approch="ova", cross.validate = TRUE, nfolds = 1); # class(res_clf)
  # save results
  save(res_clf, clf, file = "res_clf.rda", compression_level = 9)
}

# ... Database X is not a matrix! Converting ...
# ... Classification mode, computing factor(y) for speedup and robustness
# ... Loading feature correlation for speedup
# ... Correlation file loaded
# ... Storing data in the classifier object for speedup
# ... Computing ternary coefficients for speedup
# ... One seed found, setting by default
# ... Running the classifier terga2 with a single CPU
# ... Second and faster version of terga fitting based on Genetic Algorithm heuristics ...
# ... Cross validation mode
# ... Starting cross validation not in parallel
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ... Learning process is finished succesfuly
# ... Thank you for using Predomics. Don't forget to digest the results now.
# 
# [1] "experiment" "predomics" 

```

```{r load results, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
if(!runit)
{
  load("res_clf.rda")
}
```

# Digesting the results

```{r digesting results, fig.height=8, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE}
res_clf.dig_terga1ova <- digest(obj = res_clf, penalty = 0.75/100, plot = TRUE)
```

### best model

```{r best model, fig.width=7, warning=FALSE}

# get the best model
mod <- res_clf.dig_terga1ova$best$model

```


# Prediction one versus all

```{r}
predict_ova <- predict_ova(mod, y, X, clf, force.re.evaluation = TRUE)

```


# Aggregation Max Voting score one versus all and table  contingency
```{r}
aggregate_predictions_Max_Voting <- aggregate_predictions_Max_Voting_ova(list_predictions = predict_ova, y)
aggregate_predictions_Max_Voting
y_pred = aggregate_predictions_Max_Voting
table_contingency <- table(y, y_pred)
print(table_contingency)
```


# Aggregation Min Voting score one versus all and table  contingency
```{r}
aggregate_predictions_Min_Voting <- aggregate_predictions_Min_Voting_ova(list_predictions = predict_ova, y)
aggregate_predictions_Min_Voting
y_pred = aggregate_predictions_Min_Voting
table_contingency <- table(y, y_pred)
print(table_contingency)
```

#Evaluation of metrics for the one versus all approach

```{r}
Results <- EvaluateAdditionnelGlobaleMetrics(predictions = y_pred, actual_labels = y)
Results
```


#Prediction one versus one

```{r}
predict_ovo <- predict_ovo(mod, y, X, clf, force.re.evaluation = TRUE )

```

###  Aggregation : aggregate_predictions_Max_Voting one versus one and table  contingency
```{r}
aggregate_majoritaire_vote_ovo <- aggregate_majoritaire_vote_ovo(predictions_list = predict_ovo)
aggregate_majoritaire_vote_ovo
y_pred = aggregate_majoritaire_vote_ovo
table_contingency <- table(y, y_pred)
print(table_contingency)
```


###Evaluation of metrics for the one versus all approach

```{r}
Results <- EvaluateAdditionnelGlobaleMetrics(predictions = y_pred, actual_labels = y)
Results
```



# State of the art 


## Random Forest

```{r warning=FALSE}
library(randomForest)
library(e1071)
```

```{r warning=FALSE}
library(caret)
library(randomForest)
library(e1071)
X_train = t(X)
y_train = y
X_test = t(X.test)
y_test =y.test
y_train <- as.factor(y_train)
y_test <- as.factor(y_test)


# Define 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)

# Train the multiclass random forest model with 10-fold cross-validation
rf_model <- train(x = X_train, y = y_train, method = "rf", trControl = train_control)

# Prediction on the training data
train_pred <- predict(rf_model, X_train)

# Calculation of metrics for the training data
train_metrics <- confusionMatrix(train_pred, y_train)

# Calculation of the AUC for training
roc_train <- multiclass.roc(y_train, as.numeric(train_pred))
auc_train <- auc(roc_train)

# Displaying metrics for training
print(train_metrics)
print(auc_train)

# Prediction on the test data
test_pred <- predict(rf_model, X_test)

# Calculation of metrics for the test data
test_metrics <- confusionMatrix(test_pred, y_test)

# Calculation of the AUC for the test data.
roc_test <- multiclass.roc(y_test, as.numeric(test_pred))
auc_test <- auc(roc_test)

# Displaying metrics for the test data
print(test_metrics)
print(auc_test)

```




```{r warning=FALSE}
# Calculating metrics for the training data
train_accuracy <- train_metrics$overall["Accuracy"]
train_precision <- mean(train_metrics$byClass[,"Precision"], na.rm = TRUE)
train_recall <- mean(train_metrics$byClass[,"Recall"], na.rm = TRUE)
train_auc <- auc_train

# Displaying metrics for the training data
print(paste("Train Accuracy:", train_accuracy))
print(paste("Train Precision:", train_precision))
print(paste("Train Recall:", train_recall))
print(paste("Train AUC:", train_auc))

# Calculating metrics for the test data
test_metrics <- confusionMatrix(test_pred, y_test)
test_accuracy <- test_metrics$overall["Accuracy"]
test_precision <- mean(test_metrics$byClass[,"Precision"], na.rm = TRUE)
test_recall <- mean(test_metrics$byClass[,"Recall"], na.rm = TRUE)

# Calculating the AUC for the test data
roc_test <- multiclass.roc(y_test, as.numeric(test_pred))
test_auc <- auc(roc_test)

# Displaying metrics for the test data
print(paste("Test Accuracy:", test_accuracy))
print(paste("Test Precision:", test_precision))
print(paste("Test Recall:", test_recall))
print(paste("Test AUC:", test_auc))

```


```{r warning=FALSE, fig.height=4, fig.width=6, message=FALSE}
library(ggplot2)

# Example of metrics for the training data
accuracy_train <- train_accuracy
precision_train <- train_precision
AUC_train <- train_auc
recall_train <- train_recall

# Example of metrics for the test data
accuracy_test <- test_accuracy
precision_test <- test_precision
AUC_test <- test_auc
recall_test <- test_recall

# Create a data frame for the training metrics
train_metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "AUC", "Recall"),
  Value = c(accuracy_train, precision_train, AUC_train, recall_train),
  Dataset = "Training"
)

# Create a data frame for the test metrics
test_metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "AUC", "Recall"),
  Value = c(accuracy_test, precision_test, AUC_test, recall_test),
  Dataset = "Test"
)

# Combine the training and test metrics
all_metrics <- rbind(train_metrics, test_metrics)

# Create the graph
plot <- ggplot(all_metrics, aes(x = Metric, y = Value, fill = Dataset)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = round(Value, 2)), position = position_dodge(width = 1), vjust = -0.5) +
  labs(title = "Comparison of test and training metrics RF",
       x = "Metrics", y = "Values") +
  theme_minimal()

# Display the graph
print(plot)

```



## Regression Logistique

```{r warning=FALSE}
library(nnet)
# Create a data.frame from matrix X
library(glmnet)
# Utilize cv.glmnet with 10-fold cross-validation
df <- as.data.frame(t(X))
df$y <- factor(y)
cv_model <- cv.glmnet(as.matrix(df[, -ncol(df)]), df$y, family = "multinomial", alpha = 1, nfolds = 10)

# Get the best model based on cross-validation
best_model <- cv_model$glmnet.fit

# Obtain predictions directly from cross-validation
predictions <- predict(cv_model, newx = as.matrix(df[, -ncol(df)]), s = "lambda.min", type = "class")
predictions <- factor(predictions)

# Check the dimensions and create the confusion matrix
if(length(predictions) == length(df$y)) {
  confusionMatrix(predictions, df$y)
} else {
  print("The dimensions of the predictions do not match those of the ground truth")
}

```



```{r warning=FALSE}
# Create the confusion matrix
conf_matrix <- confusionMatrix(predictions, df$y)
accuracy_train <- conf_matrix$overall[["Accuracy"]]
# Calculate the mean precision of each class
precision_train <- mean(conf_matrix$byClass[ , "Precision"])
AUC_train <- mean(conf_matrix$byClass[ , "Balanced Accuracy"])
recall_train <- mean(conf_matrix$byClass[ , "Sensitivity"])

```



```{r warning=FALSE}
predictions_test <- predict(model, newx = as.matrix(X_test), type = "class")
library(caret)
predictions_test <- as.factor(predictions_test)


```


```{r warning=FALSE}
# Create the confusion matrix for predictions on the test dataset
conf_matrix_test <- confusionMatrix(predictions_test, y_test)

# Calculate accuracy, precision, AUC, and recall for the test dataset
# Calculate accuracy, precision, AUC, and recall for the test dataset
accuracy_test <- conf_matrix_test$overall[["Accuracy"]]
precision_test <- mean(conf_matrix_test$byClass[ , "Precision"], na.rm = TRUE)
AUC_test <- mean(conf_matrix_test$byClass[ , "Balanced Accuracy"], na.rm = TRUE)
recall_test <- mean(conf_matrix_test$byClass[ ,"Sensitivity"], na.rm = TRUE)

```



```{r warning=FALSE, fig.height=4, fig.width=6, message=FALSE}
library(ggplot2)

# Create a data frame for the training metrics
train_metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "AUC", "Recall"),
  Value = c(accuracy_train, precision_train, AUC_train, recall_train),
  Dataset = "Training"
)

# Create a data frame for the test metrics
test_metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "AUC", "Recall"),
  Value = c(accuracy_test, precision_test, AUC_test, recall_test),
  Dataset = "Test"
)

# Combine the training and test metrics
all_metrics <- rbind(test_metrics,train_metrics)

# Create the graph
plot <- ggplot(all_metrics, aes(x = Metric, y = Value, fill = Dataset)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = round(Value, 2)), position = position_dodge(width = 1), vjust = -0.5) +
  labs(title = "Comparison of test and training metrics MLR",
       x = "Metrics", y = "Values") +
  theme_minimal()

# Display the graph
print(plot)

```


## SVM


```{r warning=FALSE}
library(caret)
library(pROC)

# Train an SVM model with radial kernel and 10-fold cross-validation
svm_model <- train(x = X_train, y = y_train, method = "svmRadial", trControl = trainControl(method = "cv", number = 10))

# Make predictions with the SVM model
svm_predictions_train <- predict(svm_model, X_train)
svm_predictions_test <- predict(svm_model, X_test)

# Calculate the accuracy for the training and test sets
accuracy_train <- confusionMatrix(data = svm_predictions_train, reference = y_train)$overall['Accuracy']
accuracy_test <- confusionMatrix(data = svm_predictions_test, reference = y_test)$overall['Accuracy']

# Calculate precision, recall, and AUC for the training and test sets
precision_train <- confusionMatrix(data = svm_predictions_train, reference = y_train)$byClass[, 'Pos Pred Value']
precision_test <- confusionMatrix(data = svm_predictions_test, reference = y_test)$byClass[, 'Pos Pred Value']

recall_train <- confusionMatrix(data = svm_predictions_train, reference = y_train)$byClass[, 'Sensitivity']
recall_test <- confusionMatrix(data = svm_predictions_test, reference = y_test)$byClass[, 'Sensitivity']

auc_train <- roc(y_train, as.numeric(svm_predictions_train))$auc
auc_test <- roc(y_test, as.numeric(svm_predictions_test))$auc

precision_train = mean(precision_train)
precision_test = mean(precision_test)
recall_train = mean(recall_train)
recall_test = mean(recall_test)

```

```{r warning=FALSE, fig.height=4, fig.width=6, message=FALSE}
library(ggplot2)

# Create a data frame for the training metrics
train_metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "AUC", "Recall"),
  Value = c(accuracy_train, precision_train, AUC_train, recall_train),
  Dataset = "Training"
)

# Create a data frame for the test metrics
test_metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "AUC", "Recall"),
  Value = c(accuracy_test, precision_test, AUC_test, recall_test),
  Dataset = "Test"
)

# Combine the training and test metrics
all_metrics <- rbind(test_metrics,train_metrics)

# Create the graph
plot <- ggplot(all_metrics, aes(x = Metric, y = Value, fill = Dataset)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = round(Value, 2)), position = position_dodge(width = 1), vjust = -0.5) +
  labs(title = "Comparison of test and training metrics SVM",
       x = "Metrics", y = "Values") +
  theme_minimal()

# Display the graph
print(plot)
```



















