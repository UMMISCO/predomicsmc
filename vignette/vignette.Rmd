---
title: "vignette"
author: "Fabien"
date: "2023-10-04"
output: html_document
knitr: 
  duplicate.label: "allow"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load the packages, message=FALSE, warning=FALSE}
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
library(ggpubr)
library(dplyr)
library(tidyr)
library(tibble)
library(knitr)
library(kableExtra)
library(DT)
library(e1071)
library(glmnet)


```

## Loading and Analysis of Used Data

```{r}
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Create an index vector for data partitioning

X_general <- X_general[rowSums(X_general)!=0,]; dim(X_general) # filter out variables with only zero values
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE); dim(X_general)

set.seed(42)  
y = as.vector(yvec_trie)
X = X_general

# Number of desired samples in each class
nombre_echantillons_par_classe <- min(table(y))

# Function to balance the classes
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe,seed =123) {
  classes <- unique(y)
  indices_equilibres <- integer(0)
  
  for (classe in classes) {
    indices_classe <- which(y == classe)
    set.seed(seed)
    indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
  }
  
  return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}

donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X

# Verify the distribution after balancing

set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)

# Split yvec_trie into 80% train and 20% test
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[,indices_division]
X.test <- X_equilibre[,-indices_division]

table(y)
table(y.test)
dim(X)
dim(X.test)
```

## Classifying using the terbeam algorithm

```{r setting the classifier}
clf <- terBeam_mc(sparsity = c(2,3,4,5,6,7,8,9,10), 
                                   max.nb.features = 1000,
                                   seed = 1,
                                   nCores = 1,
                                   evalToFit = "accuracy_",
                                   objective = "auc",
                                   experiment.id = "terBeam_mc",
                                   experiment.save = "nothing")
  
                      
  printy(clf)  

```


## Execute the multiclass learning experiment.

```{r running experiment, echo=TRUE, fig.width=5, warning=FALSE}
runit = TRUE
if(runit)
{
Best_Unique_maximizationAggregation <- fit_mc(X = X, y = y, clf = clf,approch="ova", cross.validate = TRUE,aggregation_ = "maximizationAggregation", nfolds= 10); 
  save(Best_Unique_maximizationAggregation , clf, file ="Best_Unique_maximizationAggregation.rda", compression_level = 9)
}

# ... Database X is not a matrix! Converting ...
# ... Classification mode, computing factor(y) for speedup and robustness
# ... Loading feature correlation for speedup
# ... Correlation file loaded
# ... Storing data in the classifier object for speedup
# ... Computing ternary coefficients for speedup
# ... One seed found, setting by default
# ... Running the classifier terga2 with a single CPU
# ... Second and faster version of terga fitting based on Genetic Algorithm heuristics ...
# ... Cross validation mode
# ... Starting cross validation not in parallel
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ... Learning process is finished succesfuly
# ... Thank you for using Predomics. Don't forget to digest the results now.
# 
# [1] "experiment" "predomics" 

```

## Loading the experiments of different aggregation methods and the dataframe of results for the best models, predomics and SOTA.

```{r load results, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
load("Unique_Predomics_aggregation_ova.rda")
load("Unique_Predomics_aggregation_ovo.rda")
load("Unique_votingAggregation.rda")
load("Unique_maximizationAggregation.rda")
load("Unique_rankingAggregation.rda")
load("Unique_weightedAggregation.rda")
load("res_clf.rda")
load("analysis_table.rda")
```

## Visualization of the generalization performance of predomics and SOTA models.

```{r warning=FALSE}
# Select only the relevant columns for generalization
combined_generalization <- analysis_table %>%
  select(Methods, Accuracy.generalization, Precision.generalization, Recall.generalization, F1.generalization)

# Convert the columns to numeric, handling potential conversions
combined_generalization <- combined_generalization %>%
  mutate(across(Accuracy.generalization:F1.generalization, ~ as.numeric(as.character(.))))

# Create a new column for sorting methods, prioritizing those starting with 'Terbeam'
combined_generalization <- combined_generalization %>%
  mutate(Order = ifelse(grepl("^Terbeam", Methods), 0, 1)) %>%
  arrange(Order, Methods)  # Sort by Order first, then Methods

# Reshape the data into long format for easier visualization
combined_long_generalization <- combined_generalization %>%
  gather(key = "Metric", value = "Value", -Methods, -Order) %>%
  mutate(Type = "Generalization") %>%
  drop_na(Value)

# Calculate the standard deviation for each method and each metric
ecart_type_results_generalization <- combined_long_generalization %>%
  group_by(Methods, Metric) %>%
  summarise(EcartType = sd(Value, na.rm = TRUE), .groups = 'drop')

# Display the standard deviation results
print(ecart_type_results_generalization)

# Create the boxplot with facets for each metric
p_generalization <- ggplot(combined_long_generalization, aes(x = reorder(Methods, Order), y = Value, fill = Methods)) +
  geom_boxplot() +
  facet_wrap(~ Metric, scales = "free_y", nrow = 1) +  # Arrange horizontally in a single row
  labs(title = "Generalization Metrics Boxplots by Methods",
       x = "Methods",
       y = "Values") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.5),  # Rotate method names vertically
        panel.border = element_rect(colour = "black", fill = NA, size = 1),  # Encapsulate each facet
        strip.background = element_rect(colour = "black", fill = "lightgray"),  # Outline facet titles
        legend.position = "bottom",  # Position the legend at the bottom
        legend.justification = "left",  # Justify the legend to the left
        legend.title = element_blank()) +  # Remove the title of the legend
  guides(fill = guide_legend(nrow = 2, byrow = TRUE)) +  # Legend in two rows
  scale_fill_brewer(palette = "Set3")  # Palette of colors for the methods

# Save the plot as a PDF
ggsave("generalization_metrics_boxplots.pdf", plot = p_generalization, width = 12, height = 6)

# Confirmation of the save process
cat("The plot has been saved under the name 'generalization_metrics_boxplots.pdf'.\n")


```
## Visualization of the empirical and generalization performance of predomics and SOTA models.
```{r warning=FALSE}

# Select only the relevant columns for generalization
combined_generalization <- analysis_table %>%
  select(Methods, Accuracy.generalization, Precision.generalization, Recall.generalization, F1.generalization)

# Select only the relevant columns for empirical data
combined_empirique <- analysis_table %>%
  select(Methods, Accuracy.empirique, Precision.empirique, Recall.empirique, F1.empirique)

# Convert the columns to numeric, handling potential conversions
combined_generalization <- combined_generalization %>%
  mutate(across(Accuracy.generalization:F1.generalization, ~ as.numeric(as.character(.))))

combined_empirique <- combined_empirique %>%
  mutate(across(Accuracy.empirique:F1.empirique, ~ as.numeric(as.character(.))))

# Reshape the data into long format for easier visualization
combined_long_generalization <- combined_generalization %>%
  gather(key = "Metric", value = "Value", -Methods) %>%
  mutate(Type = "Generalization") %>%
  separate(Metric, into = c("MetricName", "TypeSuffix"), sep = "\\.") %>%
  drop_na(Value)

combined_long_empirique <- combined_empirique %>%
  gather(key = "Metric", value = "Value", -Methods) %>%
  mutate(Type = "Empirical") %>%
  separate(Metric, into = c("MetricName", "TypeSuffix"), sep = "\\.") %>%
  drop_na(Value)

# Combine both datasets
combined_long <- bind_rows(combined_long_generalization, combined_long_empirique)

# Adjust the order of the methods to prioritize those starting with "Terbeam"
combined_long <- combined_long %>%
  mutate(Methods = factor(Methods, levels = c(grep("^Terbeam", unique(Methods), value = TRUE), 
                                               setdiff(unique(Methods), grep("^Terbeam", unique(Methods), value = TRUE))) ))

# Create the boxplot with facets for each metric
p_combined <- ggplot(combined_long, aes(x = Methods, y = Value, fill = Type)) +
  geom_boxplot(position = position_dodge()) +
  facet_wrap(~ MetricName, scales = "free_y", nrow = 1) +  # Facet by metric name
  labs(title = "Empirical vs Generalization Metrics by Methods",
       x = "Methods",
       y = "Values") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.5),
        panel.border = element_rect(colour = "black", fill = NA, size = 1),
        strip.background = element_rect(colour = "black", fill = "lightgray"),
        legend.position = "bottom",
        legend.justification = "left",
        legend.title = element_blank()) +
  guides(fill = guide_legend(nrow = 1, byrow = TRUE)) +
  scale_fill_brewer(palette = "Set1")  # Distinctive palette to differentiate types

# Save the plot as a PDF
ggsave("combined_metrics_boxplots_with_facets.pdf", plot = p_combined, width = 12, height = 6)

# Confirmation of the save process
cat("The plot has been saved under the name 'combined_metrics_boxplots_with_facets.pdf'.\n")


```



## Family of Best Models (FBM)
A family of best models is defined as the set of models returned by the predomics algorithm, whose accuracy is within a given window of the best model's accuracy. This window is defined by computing a significance threshold assuming that accuracy follows a binomial distribution (p<0.05). An FBM can be analyzed in detail to distill biological information in the predictive context. First, we pool all the models together from the experiment object with `?modelCollectionToPopulation`. A population of 900 models is obtained, which can be transformed with `?populationToDataFrame` onto a dataframe for further exploration. Next we select the FBM composed of 361 models with `?selectBestPopulation`. The figure below displays the accuracy distribution by model-size before and after selection.


```{r warning=FALSE}
# Convert the model collection into a population of models scrambled by model size
pop <- modelCollectionToPopulation(res_clf$classifier$models)
printy_mc(pop)
# Function to create boxplot for a given data frame
create_boxplot <- function(data, title) {
  # Melt the dataframe for ggplot
  data.melt <- melt(data, id.vars = c("accuracy_", "eval.sparsity"))
  
  # Create ggplot
  plot <- ggplot(data = data.melt, aes(y = accuracy_, x = eval.sparsity)) +
    geom_boxplot(notch = FALSE, outlier.shape = NA, position = position_dodge(width = 0.9), alpha = 0.3) +
    geom_point(aes(color = eval.sparsity), position = position_jitterdodge(dodge.width = 0.9), size = 1, alpha = 0.5) +
    ylim(c(0, 1)) +
    xlab("Model Parsimony") +
    ggtitle(title) +
    theme_bw() +
    theme(legend.position = "bottom", legend.direction = "horizontal") +
    guides(colour = "none")
  
  return(plot)
}

# Convert the population to a data frame
pop.df <- populationToDataFrame_mc(pop)

# Plotting for the original population (single figure)
pop.dff <- as.data.frame(pop.df[[1]])  # Convert the first submodel to a data frame
g.before <- create_boxplot(pop.dff, title = "Original Population")
#print(g.before)  # Display the plot

# Select the best population models
fbm <- selectBestPopulation(pop)
#printy_mc(fbm)

# Convert the best population models to a data frame
fbm.df <- populationToDataFrame_mc(fbm)

# Plotting for the selected best models (single figure)
fbm.dff <- as.data.frame(fbm.df[[1]])  # Convert the first submodel to a data frame
g.after <- create_boxplot(fbm.dff, title = "FBM")
print(g.after)  # Display the plot

```
## Analyzing the FBM 

The analysis of the family of best models (FBM) can be very insightful for discovering the most important variables in the prediction process. Let's start by examining the usage of variables in the FBM models. For this, we run the ?makeFeatureAnnot_mc function to obtain the feature distribution in the model matrix, which is a list of pop.noz elements. The models in the FBM are ranked by accuracy, and this order will be preserved in the coefficient dataframe. There are 20 features found in the FBM models. The figure below shows that some of these features are very prevalent in the FBM, making them likely important. The abundance and prevalence distribution of these features can be explored with ?plotAbundanceByClass_mc and ?plotPrevalence_mc, respectively. Gray stars on the right side of the graphics indicate significant differences between the prediction groups.

```{r warning=FALSE, paged.print=FALSE, message=FALSE,fig.height=7, fig.width=7}
fa <- makeFeatureAnnot_mc(pop = fbm, 
                       X = X, 
                       y = y, 
                       clf = clf,
                       approch = "ova")
```


```{r warning=FALSE, paged.print=FALSE, message=FALSE,fig.height=10, fig.width=16}
plot_distribution <- plotFeatureModelCoeffs_mc(feat.model.coeffs = fa, y, approch = "OVA")
```


```{r warning=FALSE, paged.print=FALSE, message=FALSE,fig.height=10, fig.width=16}
plots_abundance <- plotAbundanceByClass_mc(features = fa, X, y, approch = "OVA")
```

```{r warning=FALSE, paged.print=FALSE, message=FALSE,fig.height=10, fig.width=16}
plots_prevalence <- plotPrevalence_mc(features = fa, X = X, y = y, approch = "OVA")

```

## Regenerate the clf object
```{r warning=FALSE}
clf <- regenerate_clf(clf, X, y, approch = "ova")
```


## Visualizing model performance AUC
We will now visualize the AUC of our various sub-models by selecting one of our best models in the FBM.

```{r roc analyses, fig.height=7, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALS}
best.model <- fbm[[1]]
tmp <- plotAUC_mc(best.model$score_, y, percent = TRUE, approch = "ova"); rm(tmp)
```



## Visualizing the best model
The best-learned model can be visualized simply by printing it along with a summary of information on performance and model size. Furthermore, we can further explore the model by visualizing it using the barcode plot ?plotModel_mc. Finally, the importance of each feature can also be displayed using the same function, as illustrated in the figure below.

```{r fig.height=12, fig.width=16, message=FALSE, warning=FALSE, paged.print=FALSE}
# get the best model

# Visualize the model information (if needed)
printy_mc(best.model)

# Generate the plots
plots1 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ova")
plots2 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE, approch = "ova")

# Open a PDF device to save the grid of plots
pdf("model_visualization_plots.pdf", width = 12, height = 8)  # Adjust width and height as needed

# Extract the plots from each list and pass them to `grid.arrange`
grid.arrange(grobs = c(plots1, plots2), ncol = 4)

# Close the PDF device
dev.off()

# Confirmation of the save process
cat("The plots have been saved under the name 'model_visualization_plots.pdf'.\n")


```





































# ANNEXE



```{r}
### Best_Model predomics

# Récupération des scores pour 'k_5'
Accuracy.empirique <- Unique_maximizationAggregation$crossVal$scores$empirical.acc["k_2", ]  # Valeurs de l'accuracy empirique
Accuracy.generalization <- Unique_maximizationAggregation$crossVal$scores$generalization.acc["k_2", ]  # Valeurs d'accuracy de généralisation
Precision.empirique <- Unique_maximizationAggregation$crossVal$scores$empirical.prc["k_2", ]  # Valeurs de précision empirique
Precision.generalization <- Unique_maximizationAggregation$crossVal$scores$generalization.prc["k_2", ]
Recall.empirique <- Unique_maximizationAggregation$crossVal$scores$empirical.rec["k_2", ]  # Valeurs du rappel empirique
Recall.generalization <- Unique_maximizationAggregation$crossVal$scores$generalization.rec["k_2", ]  # Valeurs du rappel de généralisation

# Valeurs de précision de généralisation
F1.empirique <-Unique_maximizationAggregation$crossVal$scores$empirical.f1s["k_2", ]  # Valeurs de F1 empirique
F1.generalization <- Unique_maximizationAggregation$crossVal$scores$generalization.f1s["k_2", ]  # Valeurs de F1 de généralisation

# Création du DataFrame
ranking_data <- data.frame(
  Fold = paste0("fold", 1:10),  # Noms des folds de fold1 à fold10
  Accuracy.empirique = as.numeric(Accuracy.empirique),  # Valeurs d'accuracy empirique
  Accuracy.generalization = as.numeric(Accuracy.generalization),  # Valeurs d'accuracy généralisation
  Precision.empirique  = as.numeric(Precision.empirique),  # Valeurs de précision empirique
  Precision.generalization = as.numeric(Precision.generalization),  # Valeurs de précision généralisation
  Recall.empirique  = as.numeric(Recall.empirique),  # Valeurs de rappel empirique
  Recall.generalization = as.numeric(Recall.generalization),  # Valeurs de rappel généralisation
  F1.empirique = as.numeric(F1.empirique),  # Valeurs de F1 empirique
  F1.generalization = as.numeric(F1.generalization),  # Valeurs de F1 généralisation
  Methods = rep("Terbeam_ranking", 10)  # Méthode répétée pour chaque ligne (10 fois)
)

# Renommer les colonnes pour utiliser des tirets
colnames(ranking_data) <- gsub("_", "-", colnames(ranking_data))
# Sauvegarder le DataFrame dans un fichier .rda
save(ranking_data, file = "ranking_data.rda")

```



```{r}
transform_model_data <- function(model_data, model_name) {
  testlist <- model_data$crossVal$scores[grep("generalization", names(model_data$crossVal$scores))]
  testlist2 <- list()
  for(i in names(testlist)) {
    idf <- testlist[[i]]
    if(unique(colSums(is.na(idf))) != nrow(idf)) {
      idf$sparsity <- rownames(idf)
      idf <- melt(idf)
      idf$source <- i
      testlist2[[i]] <- idf
    }
  }
  testlist2.df <- do.call("rbind", testlist2)
  testlist2.df$emp_gen <- gsub("\\..*$", "", testlist2.df$source)
  testlist2.df$metric <- gsub("^.*\\.", "", testlist2.df$source)
  testlist2.df$model <- model_name
  testlist2.df <- testlist2.df[is.finite(testlist2.df$value), ]
  testlist2.df <- testlist2.df[testlist2.df$metric %in% c("acc", "prc"), ]
  return(testlist2.df)
}

df1 <- transform_model_data(Unique_maximizationAggregation, "Unique_maximizationAggregation")
df2 <- transform_model_data(Unique_Predomics_aggregation_ova, "Unique_Predomics_aggregation_ova")

df3 <- transform_model_data(Unique_rankingAggregation, "Unique_rankingAggregation")
df4 <- transform_model_data(Unique_weightedAggregation, "Unique_weightedAggregation")
df5 <- transform_model_data(Unique_votingAggregation, "Unique_votingAggregation")
df6 <- transform_model_data(Unique_votingAggregation, "res_clf")

alldf <- rbind(df1, df2, df3, df4, df5,df6)


```


















```{r}
# Assigner le DataFrame combiné à une nouvelle variable si nécessaire
#analysis_table <- combined_df

# Sauvegarder le DataFrame en tant que fichier RDA
#save(analysis_table, file = "analysis_table.rda")





```





























































