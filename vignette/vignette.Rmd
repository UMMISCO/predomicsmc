---
title: "vignette"
author: "Fabien"
date: "2023-10-04"
output: html_document
knitr: 
  duplicate.label: "allow"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load the packages, message=FALSE, warning=FALSE}
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
library(ggpubr)
library(dplyr)
library(tidyr)
library(tibble)
library(knitr)
library(kableExtra)
library(DT)
library(e1071)
library(glmnet)


```

# Loading and Analysis of Used Data

```{r}
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Create an index vector for data partitioning

X_general <- X_general[rowSums(X_general)!=0,]; dim(X_general) # filter out variables with only zero values
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE); dim(X_general)

set.seed(42)  
y = as.vector(yvec_trie)
X = X_general

# Number of desired samples in each class
nombre_echantillons_par_classe <- min(table(y))

# Function to balance the classes
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe,seed =123) {
  classes <- unique(y)
  indices_equilibres <- integer(0)
  
  for (classe in classes) {
    indices_classe <- which(y == classe)
    set.seed(seed)
    indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
  }
  
  return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}

donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X

# Verify the distribution after balancing

set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)

# Split yvec_trie into 80% train and 20% test
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[,indices_division]
X.test <- X_equilibre[,-indices_division]

table(y)
table(y.test)
dim(X)
dim(X.test)
```

#Classifying using the terbeam algorithm

```{r setting the classifier}
clf <- terBeam_mc(sparsity = c(2,3,4), 
                                   max.nb.features = 1000,
                                   seed = 1,
                                   nCores = 1,
                                   evalToFit = "accuracy_",
                                   objective = "auc",
                                   experiment.id = "terBeam_mc",
                                   experiment.save = "nothing")
  
                      
  printy(clf)  

```


# Run the multiclass learning experiment using the one-versus-one binarization strategy.

```{r running experiment, echo=TRUE, fig.width=5, warning=FALSE}
runit = TRUE
if(runit)
{
res_clf <- fit_mc(X = X, y = y, clf = clf,approch="ova", cross.validate = TRUE,aggregation_ = "Predomics_aggregation_ova", nfolds= 10); 
  save(res_clf , clf, file ="res_clf.rda", compression_level = 9)
}

# ... Database X is not a matrix! Converting ...
# ... Classification mode, computing factor(y) for speedup and robustness
# ... Loading feature correlation for speedup
# ... Correlation file loaded
# ... Storing data in the classifier object for speedup
# ... Computing ternary coefficients for speedup
# ... One seed found, setting by default
# ... Running the classifier terga2 with a single CPU
# ... Second and faster version of terga fitting based on Genetic Algorithm heuristics ...
# ... Cross validation mode
# ... Starting cross validation not in parallel
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ... Learning process is finished succesfuly
# ... Thank you for using Predomics. Don't forget to digest the results now.
# 
# [1] "experiment" "predomics" 

```

# Load the results of the one-versus-one training.

```{r load results, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
#load("Unique_Predomics_aggregation_ova.rda")
#load("Unique_Predomics_aggregation_ovo.rda")
#load("Unique_votingAggregation.rda")
#load("Unique_maximizationAggregation.rda")
#load("Unique_rankingAggregation.rda")
#load("Unique_weightedAggregation.rda")
#load("Multi_Predomics_aggregation_ova.rda")
#load("Multi_maximizationAggregation.rda")
#load("Multi_rankingAggregation.rda")
#load("Multi_weightedAggregation.rda")
#load("Multi_votingAggregation.rda")
#load("Multi_Predomics_aggregation_ovo.rda")
#load("res_clf.rda")

```


# Use of the same variables for all our sub-models

## Digesting the results


```{r digesting results, fig.height=8, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE}
res_clf.dig <- digestmc(obj = res_clf, penalty = 0.75/100, plot = TRUE)
```
## Visualizing the best model

```{r fig.height=12, fig.width=12, message=FALSE, warning=FALSE, paged.print=FALSE}
# get the best model
best.model <- res_clf.dig$best$model
printy_mc(best.model)

plots1 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ova")
plots2 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE, approch = "ova")

# Extract the plots from each list and pass them to `grid.arrange
grid.arrange(grobs = c(plots1, plots2), ncol = 2)

```



```{r fig.height=12, fig.width=12, message=FALSE, warning=FALSE, paged.print=FALSE}
grid.arrange(grobs = plots1, ncol = 2)
```


```{r fig.height=12, fig.width=12, message=FALSE, warning=FALSE, paged.print=FALSE}
grid.arrange(grobs = plots2, ncol = 2)
```








## Regenerate the clf object

```{r warning=FALSE}
clf <- regenerate_clf(clf, X, y, approch = "ova")
```

## Testing the model in another dataset
```{r}
  best.model.test <- evaluateModel_mc(
    mod = best.model,
    X = X.test,
    y = y.test,
    clf = clf,
    eval.all = TRUE,
    force.re.evaluation = TRUE,
    approch = "ova",
    aggregation_ = "Predomics_aggregation_ova",
    mode = "test"
  )
printy_mc(best.model.test)
```

## Visualizing model performance AUC
```{r roc analyses, fig.height=7, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
tmp <- plotAUC_mc(best.model$score_, y, percent = TRUE, approch = "ova"); rm(tmp)
library(pROC)
library(ggplot2)

# Create ROC objects for the training set
roc_objects_train <- lapply(best.model$score_, function(score) {
  roc(response = y, predictor = score)
})

# Create ROC objects for the test set
roc_objects_test <- lapply(best.model.test$score_, function(score) {
  roc(response = y.test, predictor = score)
})

# Assign labels and combine ROC objects
roc_objects_train <- lapply(seq_along(roc_objects_train), function(i) {
  roc_obj <- roc_objects_train[[i]]
  roc_obj$dataset <- "Train"
  roc_obj$submodel <- paste("Submodel", i)
  return(roc_obj)
})

roc_objects_test <- lapply(seq_along(roc_objects_test), function(i) {
  roc_obj <- roc_objects_test[[i]]
  roc_obj$dataset <- "Test"
  roc_obj$submodel <- paste("Submodel", i)
  return(roc_obj)
})

# Combine ROC objects into a single list
all_roc_objects <- c(roc_objects_train, roc_objects_test)

# Convert to a format compatible with ggplot
roc_df <- do.call(rbind, lapply(all_roc_objects, function(roc_obj) {
  data.frame(
    FPR = 1 - roc_obj$specificities,  # False Positive Rate
    TPR = roc_obj$sensitivities,       # True Positive Rate
    Submodel = roc_obj$submodel,
    Dataset = roc_obj$dataset
  )
}))

# Plot ROC curves using ggplot
ggplot(roc_df, aes(x = FPR, y = TPR, color = Dataset)) +
  geom_line() +
  facet_wrap(~ Submodel, scales = "free", labeller = label_both) +
  labs(title = "ROC Curves", x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal() +
  theme(legend.position = "bottom")


```




## Family of Best Models (FBM)

```{r}
# Convert the model collection into a population of models scrambled by model size
pop <- modelCollectionToPopulation(res_clf$classifier$models)
printy_mc(pop)

# Convert the population to a data frame
pop.df <- populationToDataFrame_mc(pop)

# Plotting for the original population
g.before <- list()  # Initialize a list to store the plots
for(i in 1: length(pop.df)) {
  pop.dff  <- as.data.frame(pop.df[[i]])  # Convert each submodel to a data frame
  
  # Display the head of the dataframe excluding some columns
  head(pop.dff[,-c(3, 4, 7, 8, 14)])
  
  # Melt the dataframe for ggplot
  pop.df.melt <- melt(pop.dff, id.vars = c("accuracy_", "eval.sparsity"))
  
  # Create the ggplot for the original population
  g.before[[i]] <- ggplot(data = pop.df.melt, aes(y = accuracy_, x = eval.sparsity)) + 
    geom_boxplot(notch = FALSE, outlier.shape = NA, position = position_dodge(width=0.9), alpha = 0.3) + 
    geom_point(aes(color = eval.sparsity), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
    ylim(c(0,1)) +
    xlab("Model parsimony") +
    ggtitle("Original population") +
    theme_bw() +
    theme(legend.position="bottom", legend.direction="horizontal") +
    guides(colour="none")
}

# Select the best population models
fbm <- selectBestPopulation(pop)
printy_mc(fbm)

# Convert the best population models to a data frame
fbm.df <- populationToDataFrame_mc(fbm)

# Plotting for the selected best models
g.after <- list()  # Initialize a list to store the plots
for(j in 1:length(fbm.df)) {
  fbm.dff  <- as.data.frame(fbm.df[[j]])  # Convert each submodel to a data frame
  
  # Melt the dataframe for ggplot
  fbm.df.melt <- melt(fbm.dff, id.vars = c("accuracy_", "eval.sparsity"))
  
  # Create the ggplot for the best population models
  g.after[[j]] <- ggplot(data = fbm.df.melt, aes(y = accuracy_, x = eval.sparsity)) + 
    geom_boxplot(notch = FALSE, position = position_dodge(width=0.9), alpha = 0.3) + 
    geom_point(aes(color = eval.sparsity), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
    ylim(c(0,1)) +
    xlab("Model parsimony") +
    ggtitle("FBM") +
    theme_bw() +
    theme(legend.position="bottom", legend.direction="horizontal") +
    guides(colour="none")
}

# Arrange the plots side by side
grid.arrange(g.before[[1]], g.after[[1]], ncol = 2)

```
## Analyzing the FBM 

```{r fig.height=7, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
fa <- makeFeatureAnnot_mc(pop = fbm, 
                       X = X, 
                       y = y, 
                       clf = clf,
                       approch = "ova")
g <- list()
dim(fa[[1]]$pop.noz)
for (i in 1:length(fa)) {

  g[[i]] <- plotFeatureModelCoeffs(feat.model.coeffs = fa[[i]]$pop.noz)
  
  print(g)
}
(g2 <- plotAbundanceByClass_mc(features = fa, X = X, y = y, approch = "ova"))
(g3 <- plotPrevalence_mc(features = fa, X = X, y = y , approch = "ova"))

```


```{r fig.height=10, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE}
gridExtra::grid.arrange(grobs = g, ncol = 2)  # Ici, ncol spécifie 2 colonnes
```




```{r fig.height=7, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE}
gridExtra::grid.arrange(grobs = g2, ncol = 2)  # Ici, ncol spécifie 2 colonnes
```



```{r fig.height=7, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE }
gridExtra::grid.arrange(grobs = g3, ncol = 2)  # Ici, ncol spécifie 2 colonnes
```





# Use of specific variables for each of our sub-models
## Digesting the results

```{r digesting results, fig.height=8, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE}
res_clf.dig_multi <- digestmc(obj = Multi_Predomics_Aggregation_ova , penalty = 0.75/100, plot = TRUE)
```


## Visualizing the best model
```{r fig.height=12, fig.width=12, message=FALSE, warning=FALSE, paged.print=FALSE}
# get the best model
best.model_multi <- res_clf.dig_multi$best$model
printy_mc(best.model_multi)

plots11 <- plotModel_mc(best.model_multi, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ova")
plots22 <- plotModel_mc(best.model_multi, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE, approch = "ova")

# Extract the plots from each list and pass them to `grid.arrange
grid.arrange(grobs = c(plots11, plots22), ncol = 2)
```

## Regenerate the clf object
```{r warning=FALSE}
clf <- regenerate_clf(clf, X, y, approch = "ova")
```

## Testing the model in another dataset

```{r}
  best.model.test_multi <- evaluateModel_mc(
    mod = best.model_multi,
    X = X.test,
    y = y.test,
    clf = clf,
    eval.all = TRUE,
    force.re.evaluation = TRUE,
    approch = "ova",
    aggregation_ = "Predomics_aggregation_ova",
    mode = "test"
  )
printy_mc(best.model.test_multi)
```

## Visualizing model performance AUC
```{r roc analyses, fig.height=7, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
library(pROC)
library(ggplot2)
tmp <- plotAUC_mc(best.model_multi$score_, y, percent = TRUE, approch = "ova"); rm(tmp)
# Create ROC objects for the training set
roc_objects_train <- lapply(best.model_multi$score_, function(score) {
  roc(response = y, predictor = score)
})

# Create ROC objects for the test set
roc_objects_test <- lapply(best.model.test_multi$score_, function(score) {
  roc(response = y.test, predictor = score)
})

# Function to extract TPR and FPR for ggplot
extract_roc_data <- function(roc_obj, dataset_name, submodel_name) {
  data.frame(
    FPR = 1 - roc_obj$specificities,  # False Positive Rate
    TPR = roc_obj$sensitivities,       # True Positive Rate
    Submodel = submodel_name,
    Dataset = dataset_name
  )
}

# Extract data from ROC objects for the training set
roc_df_train <- do.call(rbind, lapply(seq_along(roc_objects_train), function(i) {
  extract_roc_data(roc_objects_train[[i]], "Train", paste("Submodel", i))
}))

# Extract data from ROC objects for the test set
roc_df_test <- do.call(rbind, lapply(seq_along(roc_objects_test), function(i) {
  extract_roc_data(roc_objects_test[[i]], "Test", paste("Submodel", i))
}))

# Combine training and test ROC data
roc_df <- rbind(roc_df_train, roc_df_test)

# Make sure there are no NULL values in FPR and TPR
roc_df <- roc_df[!is.na(roc_df$FPR) & !is.na(roc_df$TPR), ]

# Plot ROC curves using ggplot
ggplot(roc_df, aes(x = FPR, y = TPR, color = Dataset)) +
  geom_line() +
  facet_wrap(~ Submodel, scales = "free", labeller = label_both) +
  labs(title = "ROC Curves", x = "False Positive Rate", y = "True Positive Rate") +
  theme_minimal() +
  theme(legend.position = "bottom")


```
## Family of Best Models (FBM)
```{r}
# Convert the model collection into a population of models scrambled by model size
pop <- modelCollectionToPopulation(Multi_Predomics_Aggregation_ova$classifier$models)
printy_mc(pop)

# Convert the population to a data frame
pop.df <- populationToDataFrame_mc(pop)

# Plotting for the original population
g.before <- list()  # Initialize a list to store the plots
for(i in 1: length(pop.df)) {
  pop.dff  <- as.data.frame(pop.df[[i]])  # Convert each submodel to a data frame
  
  # Display the head of the dataframe excluding some columns
  head(pop.dff[,-c(3, 4, 7, 8, 14)])
  
  # Melt the dataframe for ggplot
  pop.df.melt <- melt(pop.dff, id.vars = c("accuracy_", "eval.sparsity"))
  
  # Create the ggplot for the original population
  g.before[[i]] <- ggplot(data = pop.df.melt, aes(y = accuracy_, x = eval.sparsity)) + 
    geom_boxplot(notch = FALSE, outlier.shape = NA, position = position_dodge(width=0.9), alpha = 0.3) + 
    geom_point(aes(color = eval.sparsity), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
    ylim(c(0,1)) +
    xlab("Model parsimony") +
    ggtitle("Original population") +
    theme_bw() +
    theme(legend.position="bottom", legend.direction="horizontal") +
    guides(colour="none")
}

# Select the best population models
fbm <- selectBestPopulation(pop)
printy_mc(fbm)

# Convert the best population models to a data frame
fbm.df <- populationToDataFrame_mc(fbm)

# Plotting for the selected best models
g.after <- list()  # Initialize a list to store the plots
for(j in 1:length(fbm.df)) {
  fbm.dff  <- as.data.frame(fbm.df[[j]])  # Convert each submodel to a data frame
  
  # Melt the dataframe for ggplot
  fbm.df.melt <- melt(fbm.dff, id.vars = c("accuracy_", "eval.sparsity"))
  
  # Create the ggplot for the best population models
  g.after[[j]] <- ggplot(data = fbm.df.melt, aes(y = accuracy_, x = eval.sparsity)) + 
    geom_boxplot(notch = FALSE, position = position_dodge(width=0.9), alpha = 0.3) + 
    geom_point(aes(color = eval.sparsity), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
    ylim(c(0,1)) +
    xlab("Model parsimony") +
    ggtitle("FBM") +
    theme_bw() +
    theme(legend.position="bottom", legend.direction="horizontal") +
    guides(colour="none")
}

# Arrange the plots side by side
grid.arrange(g.before[[1]], g.after[[1]], ncol = 2)
```

## Analyzing the FBM 

```{r fig.height=7, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
fa <- makeFeatureAnnot_mc(pop = fbm, 
                       X = X, 
                       y = y, 
                       clf = clf,
                       approch = "ova")
dim(fa[[1]]$pop.noz)
for (i in 1:length(fa)) {

  g <- plotFeatureModelCoeffs(feat.model.coeffs = fa[[i]]$pop.noz)
  
  print(g)
}
(g2 <- plotAbundanceByClass_mc(features = fa, X = X, y = y, approch = "ova"))
(g3 <- plotPrevalence_mc(features = fa, X = X, y = y , approch = "ova"))
```


#ANNEX

```{r fig.height=5, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}

transform_model_data <- function(model_data, model_name) {
  testlist <- model_data$crossVal$scores[grep("generalization|empirical", names(model_data$crossVal$scores))]
  testlist2 <- list()
  for(i in names(testlist)) {
    idf <- testlist[[i]]
    if(unique(colSums(is.na(idf))) != nrow(idf)) {
      idf$sparsity <- rownames(idf)
      idf <- melt(idf)
      idf$source <- i
      testlist2[[i]] <- idf
    }
  }
  testlist2.df <- do.call("rbind", testlist2)
  testlist2.df$emp_gen <- gsub("\\..*$", "", testlist2.df$source)
  testlist2.df$metric <- gsub("^.*\\.", "", testlist2.df$source)
  testlist2.df$model <- model_name
  testlist2.df <- testlist2.df[is.finite(testlist2.df$value), ]
  testlist2.df <- testlist2.df[testlist2.df$metric %in% c("acc", "prc"), ]
  return(testlist2.df)
}

df1 <- transform_model_data(Unique_maximizationAggregation, "Unique_maximizationAggregation")
df2 <- transform_model_data(Unique_Predomics_aggregation_ova, "Unique_Predomics_aggregation_ova")

df3 <- transform_model_data(Unique_rankingAggregation, "Unique_rankingAggregation")
df4 <- transform_model_data(Unique_weightedAggregation, "Unique_weightedAggregation")
df5 <- transform_model_data(Unique_votingAggregation, "Unique_votingAggregation")
df6 <- transform_model_data(Unique_votingAggregation, "Unique_Predomics_aggregation_ovo")

alldf <- rbind(df1, df2, df3, df4, df5,df6)

ggplot(alldf, aes(x=model, y=value)) + 
  geom_point(aes(colour=sparsity)) + 
  facet_grid(.~metric+emp_gen) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


```{r fig.height=5, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
# Fonction pour transformer les données des modèles
transform_model_data <- function(model_data, model_name) {
  testlist <- model_data$crossVal$scores[grep("generalization|empirical", names(model_data$crossVal$scores))]
  testlist2 <- list()
  for(i in names(testlist)) {
    idf <- testlist[[i]]
    if(unique(colSums(is.na(idf))) != nrow(idf)) {
      idf$sparsity <- rownames(idf)
      idf <- melt(idf)
      idf$source <- i
      testlist2[[i]] <- idf
    }
  }
  testlist2.df <- do.call("rbind", testlist2)
  testlist2.df$emp_gen <- gsub("\\..*$", "", testlist2.df$source)
  testlist2.df$metric <- gsub("^.*\\.", "", testlist2.df$source)
  testlist2.df$model <- model_name
  testlist2.df <- testlist2.df[is.finite(testlist2.df$value), ]
  testlist2.df <- testlist2.df[testlist2.df$metric %in% c("f1s", "rec"), ]
  return(testlist2.df)
}

df1 <- transform_model_data(Unique_maximizationAggregation, "Unique_maximizationAggregation")
df2 <- transform_model_data(Unique_Predomics_aggregation_ova, "Unique_Predomics_aggregation_ova")

df3 <- transform_model_data(Unique_rankingAggregation, "Unique_rankingAggregation")
df4 <- transform_model_data(Unique_weightedAggregation, "Unique_weightedAggregation")
df5 <- transform_model_data(Unique_votingAggregation, "Unique_votingAggregation")
df6 <- transform_model_data(Unique_votingAggregation, "Unique_Predomics_aggregation_ovo")

alldf <- rbind(df1, df2, df3, df4, df5,df6)

ggplot(alldf, aes(x=model, y=value)) + 
  geom_point(aes(colour=sparsity)) + 
  facet_grid(.~metric+emp_gen) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r fig.height=10, fig.width=10, message=FALSE, warning=FALSE, paged.print=FALSE}
Unique_weighted <- Unique_weightedAggregation$crossVal$scores$mean.acc
Unique_maximization <- Unique_maximizationAggregation$crossVal$scores$mean.acc
Unique_Predomics_Aggre <- Unique_Predomics_aggregation_ova$crossVal$scores$mean.acc 
Unique_ranking <- Unique_rankingAggregation$crossVal$scores$mean.acc
Unique_voting <- Unique_votingAggregation$crossVal$scores$mean.acc
Unique_Predomics_ovo <- Unique_Predomics_aggregation_ovo$crossVal$scores$mean.acc
Unique_weighted$k <- rownames(Unique_weighted)
Unique_maximization$k <- rownames(Unique_maximization)
Unique_Predomics_Aggre$k <- rownames(Unique_Predomics_Aggre)
Unique_ranking$k <- rownames(Unique_ranking)
Unique_voting$k <- rownames(Unique_voting)
Unique_Predomics_ovo$k <- rownames(Unique_Predomics_ovo)
Unique_weighted$algorithm <- "Weighted voting"
Unique_maximization$algorithm <- "Maximization"
Unique_Predomics_Aggre$algorithm <- "Predomics_Aggregation_ova"
Unique_voting$algorithm <- "Voting"
Unique_ranking$algorithm <- "Ranking"
Unique_Predomics_ovo$algorithm <- "Predomics_Aggregation_ovo"
Unique_weighted <- Unique_weighted[, c("k", "empirical", "generalization", "algorithm")]
Unique_maximization <- Unique_maximization[, c("k", "empirical", "generalization", "algorithm")]
Unique_Predomics_Aggre <-Unique_Predomics_Aggre[, c("k", "empirical", "generalization", "algorithm")]
Unique_voting <- Unique_voting[, c("k", "empirical", "generalization", "algorithm")]
Unique_ranking <- Unique_ranking[, c("k", "empirical", "generalization", "algorithm")]
Unique_Predomics_ovo <- Unique_Predomics_ovo[, c("k", "empirical", "generalization", "algorithm")]
combined_df <- rbind(Unique_weighted,Unique_maximization, 
                     Unique_Predomics_Aggre, Unique_ranking, 
                     Unique_voting, Unique_Predomics_ovo)

combined_df <- na.omit(combined_df)
data_melted <- melt(combined_df, id.vars = c("k", "algorithm"), variable.name = "type", value.name = "accuracy")
ggplot(data_melted, aes(x = k, y = accuracy, color = type)) +
  geom_point(size = 3) +
  geom_line(aes(group = interaction(algorithm, type), linetype = type)) +
  geom_text(aes(label = round(accuracy, 3)), vjust = -0.5, size = 3) +  # Ajouter les valeurs à côté des points
  facet_wrap(~ algorithm, ncol = 2) +  # Afficher deux graphiques par ligne
  theme_minimal() +
  labs(title = "Mean accuracy: empirical vs. generalization for cross-validation using the same variable",
       x = "Valeur de k",
       y = "Accuracy",
       color = "Type") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_manual(values = c("empirical" = "#377EB8", "generalization" = "#E41A1C"))
```


```{r fig.height=6, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE}

transform_model_data <- function(model_data, model_name) {
  testlist <- model_data$crossVal$scores[grep("generalization|empirical", names(model_data$crossVal$scores))]
  testlist2 <- list()
  for(i in names(testlist)) {
    idf <- testlist[[i]]
    if(unique(colSums(is.na(idf))) != nrow(idf)) {
      idf$sparsity <- rownames(idf)
      idf <- melt(idf)
      idf$source <- i
      testlist2[[i]] <- idf
    }
  }
  testlist2.df <- do.call("rbind", testlist2)
  testlist2.df$emp_gen <- gsub("\\..*$", "", testlist2.df$source)
  testlist2.df$metric <- gsub("^.*\\.", "", testlist2.df$source)
  testlist2.df$model <- model_name
  testlist2.df <- testlist2.df[is.finite(testlist2.df$value), ]
  testlist2.df <- testlist2.df[testlist2.df$metric %in% c("acc", "prc"), ]
  return(testlist2.df)
}

df1 <- transform_model_data(Unique_maximizationAggregation, "Multi_maximizationAggregation")
df2 <- transform_model_data(Unique_Predomics_aggregation_ova, "Multi_Predomics_aggregation_ova")

df3 <- transform_model_data(Unique_rankingAggregation, "Multi_rankingAggregation")
df4 <- transform_model_data(Unique_weightedAggregation, "Multi_weightedAggregation")
df5 <- transform_model_data(Unique_votingAggregation, "Multi_votingAggregation")

alldf <- rbind(df1, df2, df3, df4, df5)

ggplot(alldf, aes(x=model, y=value)) + 
  geom_point(aes(colour=sparsity)) + 
  facet_grid(.~metric+emp_gen) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r fig.height=6, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE}
transform_model_data <- function(model_data, model_name) {
  testlist <- model_data$crossVal$scores[grep("generalization|empirical", names(model_data$crossVal$scores))]
  testlist2 <- list()
  for(i in names(testlist)) {
    idf <- testlist[[i]]
    if(unique(colSums(is.na(idf))) != nrow(idf)) {
      idf$sparsity <- rownames(idf)
      idf <- melt(idf)
      idf$source <- i
      testlist2[[i]] <- idf
    }
  }
  testlist2.df <- do.call("rbind", testlist2)
  testlist2.df$emp_gen <- gsub("\\..*$", "", testlist2.df$source)
  testlist2.df$metric <- gsub("^.*\\.", "", testlist2.df$source)
  testlist2.df$model <- model_name
  testlist2.df <- testlist2.df[is.finite(testlist2.df$value), ]
  testlist2.df <- testlist2.df[testlist2.df$metric %in% c("f1s", "rec"), ]
  return(testlist2.df)
}

df1 <- transform_model_data(Unique_maximizationAggregation, "Multi_maximizationAggregation")
df2 <- transform_model_data(Unique_Predomics_aggregation_ova, "Multi_Predomics_aggregation_ova")

df3 <- transform_model_data(Unique_rankingAggregation, "Multi_rankingAggregation")
df4 <- transform_model_data(Unique_weightedAggregation, "Multi_weightedAggregation")
df5 <- transform_model_data(Unique_votingAggregation, "Multi_votingAggregation")

alldf <- rbind(df1, df2, df3, df4, df5)

ggplot(alldf, aes(x=model, y=value)) + 
  geom_point(aes(colour=sparsity)) + 
  facet_grid(.~metric+emp_gen) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r fig.height=10, fig.width=10, message=FALSE, warning=FALSE, paged.print=FALSE}
# Données fournies
Multi_weighted <- Multi_weightedAggregation$crossVal$scores$mean.acc
Multi_maximization <- Multi_maximizationAggregation$crossVal$scores$mean.acc
Multi_Predomics_Aggre <- Multi_Predomics_Aggregation_ova$crossVal$scores$mean.acc
Multi_ranking <- Multi_rankingAggregation$crossVal$scores$mean.acc
Multi_voting <- Multi_votingAggregation$crossVal$scores$mean.acc
Multi_Predomics_Aggre_ovo <- Multi_Predomics_aggregation_ovo$crossVal$scores$mean.acc

# Ajouter la colonne 'k'
Multi_weighted$k <- rownames(Multi_weighted)
Multi_maximization$k <- rownames(Multi_maximization)
Multi_Predomics_Aggre$k <- rownames(Multi_Predomics_Aggre)
Multi_ranking$k <- rownames(Multi_ranking)
Multi_voting$k <- rownames(Multi_voting)
Multi_Predomics_Aggre_ovo$k <- rownames(Multi_Predomics_Aggre_ovo)

# Ajouter une colonne 'algorithm' pour chaque dataframe
Multi_weighted$algorithm <- "Multi_weightedAggregation"
Multi_maximization$algorithm <- "Multi_maximizationAggregation"
Multi_Predomics_Aggre$algorithm <- "Multi_Predomics_Aggregation_ova"
Multi_voting$algorithm <- "Multi_votingAggregation"
Multi_ranking$algorithm <- "Multi_rankingAggregation"
Multi_Predomics_Aggre_ovo$algorithm <- "Multi_Predomics_Aggregation_ovo"

# Réordonner les colonnes pour avoir "k" et "algorithm" en premier
Multi_weighted <- Multi_weighted[, c("k", "empirical", "generalization", "algorithm")]
Multi_maximization <- Multi_maximization[, c("k", "empirical", "generalization", "algorithm")]
Multi_Predomics_Aggre <- Multi_Predomics_Aggre[, c("k", "empirical", "generalization", "algorithm")]
Multi_voting <- Multi_voting[, c("k", "empirical", "generalization", "algorithm")]
Multi_ranking <- Multi_ranking[, c("k", "empirical", "generalization", "algorithm")]
Multi_Predomics_Aggre_ovo <- Multi_Predomics_Aggre_ovo[, c("k", "empirical", "generalization", "algorithm")]


# Combiner les dataframes
combined_df <- rbind(Multi_weighted, Multi_maximization, 
                     Multi_Predomics_Aggre, Multi_ranking, 
                     Multi_voting,Multi_Predomics_Aggre_ovo)

# Supprimer les lignes contenant des valeurs NaN
combined_df <- na.omit(combined_df)

# Transformer les données pour ggplot2
data_melted <- melt(combined_df, id.vars = c("k", "algorithm"), variable.name = "type", value.name = "accuracy")

# Plotting
ggplot(data_melted, aes(x = k, y = accuracy, color = type)) +
  geom_point(size = 3) +
  geom_line(aes(group = interaction(algorithm, type), linetype = type)) +
  geom_text(aes(label = round(accuracy, 3)), vjust = -0.5, size = 3) +  # Ajouter les valeurs à côté des points
  facet_wrap(~ algorithm, ncol = 2) +  # Afficher deux graphiques par ligne
  theme_minimal() +
  labs(title = "Mean accuracy: empirical vs. generalization for cross-validation using the specific variables of each combination.",
       x = "Valeur de k",
       y = "Accuracy",
       color = "Type") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_manual(values = c("empirical" = "#377EB8", "generalization" = "#E41A1C"))

```



# Etat de l'art
## RF

```{r}
# Fixer une graine aléatoire pour assurer la reproductibilité
set.seed(123)

# Préparation des données
X_train <- t(X)
y_train <- as.factor(y)
X_test <- t(X.test)
y_test <- as.factor(y.test)

# Définir la validation croisée à 10 plis
train_control <- trainControl(method = "cv", number = 10, savePredictions = TRUE)

# Entraîner le modèle Random Forest avec validation croisée à 10 plis
rf_model <- train(x = X_train, y = y_train, method = "rf", trControl = train_control)

# Prédictions sur les données d'entraînement
train_pred <- predict(rf_model, X_train)
train_metrics <- confusionMatrix(train_pred, y_train)

# Calcul de l'AUC pour les données d'entraînement
roc_train <- multiclass.roc(y_train, as.numeric(train_pred))
auc_train <- auc(roc_train)

# Affichage des métriques pour les données d'entraînement
print(train_metrics)
print(auc_train)

# Prédictions sur les données de test
test_pred <- predict(rf_model, X_test)
test_metrics <- confusionMatrix(test_pred, y_test)

# Calcul de l'AUC pour les données de test
roc_test <- multiclass.roc(y_test, as.numeric(test_pred))
auc_test <- auc(roc_test)

# Affichage des métriques pour les données de test
print(test_metrics)
print(auc_test)

# Calcul des métriques pour l'entraînement et le test
train_accuracy <- train_metrics$overall["Accuracy"]
train_precision <- mean(train_metrics$byClass[,"Precision"], na.rm = TRUE)
train_recall <- mean(train_metrics$byClass[,"Recall"], na.rm = TRUE)
f1_train <- 2 * (train_precision * train_recall) / (train_precision + train_recall)

test_accuracy <- test_metrics$overall["Accuracy"]
test_precision <- mean(test_metrics$byClass[,"Precision"], na.rm = TRUE)
test_recall <- mean(test_metrics$byClass[,"Recall"], na.rm = TRUE)
f1_test <- 2 * (test_precision * test_recall) / (test_precision + test_recall)

# Calcul de l'écart type des métriques des 10 folds de la validation croisée
accuracy_sd <- sd(rf_model$resample$Accuracy)
precision_sd <- sd(rf_model$resample$Precision, na.rm = TRUE)
recall_sd <- sd(rf_model$resample$Recall, na.rm = TRUE)
f1_sd <- sd(rf_model$resample$F1, na.rm = TRUE)

# Affichage de l'écart type des métriques
print(paste("Ecart type de l'Accuracy:", accuracy_sd))
print(paste("Ecart type de la Précision:", precision_sd))
print(paste("Ecart type du Rappel:", recall_sd))
print(paste("Ecart type du F1 Score:", f1_sd))

# Création des dataframes pour les métriques d'entraînement et de test
train_metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "Recall"),
  Value = c(train_accuracy, train_precision, f1_train, train_recall),
  Dataset = "Training"
)

test_metrics_df <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "Recall"),
  Value = c(test_accuracy, test_precision, f1_test, test_recall),
  Dataset = "Test"
)

# Combiner les métriques d'entraînement et de test
all_metrics <- rbind(train_metrics_df, test_metrics_df)

# Création du graphique comparant les métriques d'entraînement et de test
plot <- ggplot(all_metrics, aes(x = Metric, y = Value, fill = Dataset)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
  labs(title = "Comparison of test and training metrics - Random Forest",
       x = "Metrics", y = "Values") +
  theme_minimal()

# Affichage du graphique
print(plot)

```


##Regression Logistique

```{r warning=FALSE}
# Fixer une graine aléatoire pour assurer la reproductibilité
set.seed(123)  # Choisissez n'importe quel nombre

# Préparation des données
df <- as.data.frame(t(X))
df$y <- factor(y)

# Vérifiez la répartition des classes
class_distribution <- table(df$y)
print(class_distribution)

# Filtrer les classes avec suffisamment d'observations (par exemple, au moins 5)
min_observations <- 5
valid_classes <- names(class_distribution[class_distribution >= min_observations])
df_filtered <- df[df$y %in% valid_classes, ]
df_filtered$y <- factor(df_filtered$y)

# Vérification après filtrage
print(table(df_filtered$y))

# Répartition des données pour la validation croisée manuelle
set.seed(123)  # Pour la reproductibilité
folds <- createFolds(df_filtered$y, k = 10, list = TRUE)

# Initialiser un vecteur pour stocker les précisions de chaque fold
fold_accuracies <- numeric(length(folds))

# Calculer l'accuracy pour chaque fold
for (i in seq_along(folds)) {
  # Indices des données de validation pour ce fold
  val_indices <- folds[[i]]
  
  # Données d'entraînement pour ce fold
  train_data <- df_filtered[-val_indices, ]
  
  # Données de validation pour ce fold
  val_data <- df_filtered[val_indices, ]
  
  # Entraînement du modèle sur les données d'entraînement du fold
  model_fold <- glmnet(as.matrix(train_data[, -ncol(train_data)]), train_data$y, family = "multinomial", alpha = 1)
  
  # Prédictions pour les données de validation du fold
  fold_pred <- predict(model_fold, newx = as.matrix(val_data[, -ncol(val_data)]), type = "class", s = model_fold$lambda.min)
  
  # Convertir les prédictions en facteur avec les mêmes niveaux que les données originales
  fold_pred <- factor(fold_pred, levels = levels(df_filtered$y))
  
  # Vérité terrain pour ce fold
  fold_truth <- val_data$y
  
  # Calculer l'accuracy pour ce fold
  fold_accuracies[i] <- mean(fold_pred == fold_truth)
}

# Calcul de la moyenne des accuracies
mean_accuracy <- mean(fold_accuracies, na.rm = TRUE)

# Calcul de l'écart type des accuracies
sd_accuracy <- sd(fold_accuracies, na.rm = TRUE)

# Afficher les résultats de validation croisée
print(paste("Cross-Validation Mean Accuracy:", round(mean_accuracy, 3)))
print(paste("Cross-Validation Accuracy Standard Deviation:", round(sd_accuracy, 3)))

# Création du modèle final sur l'ensemble des données d'entraînement
best_model_rl <- cv.glmnet(as.matrix(df_filtered[, -ncol(df_filtered)]), df_filtered$y, family = "multinomial", alpha = 1, nfolds = 10)

# Prédictions sur les données d'entraînement complètes
train_predictions <- predict(best_model_rl, newx = as.matrix(df_filtered[, -ncol(df_filtered)]), s = best_model_rl$lambda.min, type = "class")
train_predictions <- factor(train_predictions, levels = levels(df_filtered$y))

# Création de la matrice de confusion pour les données d'entraînement
conf_matrix_train_rl <- confusionMatrix(train_predictions, df_filtered$y)

# Calcul des métriques pour les données d'entraînement
accuracy_train_rl <- conf_matrix_train_rl$overall[["Accuracy"]]
precision_train_rl <- mean(conf_matrix_train_rl$byClass[ , "Precision"], na.rm = TRUE)
recall_train_rl <- mean(conf_matrix_train_rl$byClass[ , "Sensitivity"], na.rm = TRUE)
AUC_train_rl <- mean(conf_matrix_train_rl$byClass[ , "Balanced Accuracy"], na.rm = TRUE)
F1_train_rl <- 2 * (precision_train_rl * recall_train_rl) / (precision_train_rl + recall_train_rl)

# Affichage des métriques pour les données d'entraînement
print(paste("Train Accuracy:", accuracy_train_rl))
print(paste("Train Precision:", precision_train_rl))
print(paste("Train Recall (Sensitivity):", recall_train_rl))
print(paste("Train AUC (Balanced Accuracy):", AUC_train_rl))
print(paste("Train F1 Score:", F1_train_rl))

# Prédictions sur les données de test
predictions_test1_rl <- predict(best_model_rl, newx = as.matrix(X_test), s = best_model_rl$lambda.min, type = "class")
predictions_test1_rl <- factor(predictions_test1_rl, levels = levels(df_filtered$y))

# Création de la matrice de confusion pour les données de test
conf_matrix_test_rl <- confusionMatrix(predictions_test1_rl, y_test)

# Calcul des métriques pour les données de test
accuracy_test1_rl <- conf_matrix_test_rl$overall[["Accuracy"]]
precision_test1_rl <- mean(conf_matrix_test_rl$byClass[ , "Precision"], na.rm = TRUE)
recall_test1_rl <- mean(conf_matrix_test_rl$byClass[ , "Sensitivity"], na.rm = TRUE)
AUC_test1_rl <- mean(conf_matrix_test_rl$byClass[ , "Balanced Accuracy"], na.rm = TRUE)
F1_test1_rl <- 2 * (precision_test1_rl * recall_test1_rl) / (precision_test1_rl + recall_test1_rl)

# Création du data frame pour les métriques d'entraînement
train_metrics_rl <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "Recall"),
  Value = c(accuracy_train_rl, precision_train_rl, F1_train_rl, recall_train_rl),
  Dataset = "Training"
)

# Création du data frame pour les métriques de test
test_metrics_rl <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "Recall"),
  Value = c(accuracy_test1_rl, precision_test1_rl, F1_test1_rl, recall_test1_rl),
  Dataset = "Test"
)

# Combinaison des métriques d'entraînement et de test
all_metrics_rl <- rbind(test_metrics_rl, train_metrics_rl)

# Création du graphique comparant les métriques de test et d'entraînement
plot <- ggplot(all_metrics_rl, aes(x = Metric, y = Value, fill = Dataset)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
  labs(title = "Comparison of Test and Training Metrics - Logistic Regression",
       x = "Metrics", y = "Values") +
  theme_minimal()

# Affichage du graphique
print(plot)


```

##SVM

```{r warning=FALSE}

# Préparer les données pour la validation croisée
set.seed(123)  # Pour la reproductibilité
folds <- createFolds(y_train, k = 10, list = TRUE, returnTrain = TRUE)

# Initialiser des vecteurs pour stocker les métriques
fold_accuracies <- numeric(length(folds))
fold_precisions <- matrix(0, nrow = length(folds), ncol = length(levels(y_train)))
fold_recalls <- matrix(0, nrow = length(folds), ncol = length(levels(y_train)))
fold_f1_scores <- matrix(0, nrow = length(folds), ncol = length(levels(y_train)))

# Calculer les métriques pour chaque fold
for (i in seq_along(folds)) {
  train_indices <- folds[[i]]
  val_indices <- which(!1:nrow(X_train) %in% train_indices)
  
  train_data <- data.frame(x = X_train[train_indices, ], y = y_train[train_indices])
  val_data <- data.frame(x = X_train[val_indices, ], y = y_train[val_indices])
  
  # Entraînement du modèle SVM pour ce fold
  svm_model_fold <- train(x = train_data[, -ncol(train_data)], y = train_data$y, method = "svmRadial", trControl = trainControl(method = "none"))
  
  # Prédictions pour les données de validation
  predictions_val <- predict(svm_model_fold, newdata = val_data[, -ncol(val_data)])
  
  # Calculer les métriques pour ce fold
  conf_matrix <- confusionMatrix(predictions_val, val_data$y)
  
  fold_accuracies[i] <- conf_matrix$overall['Accuracy']
  
  # Assurez-vous d'utiliser les bons noms de colonnes
  if (!is.null(conf_matrix$byClass)) {
    precision_values <- conf_matrix$byClass[, 'Pos Pred Value']
    recall_values <- conf_matrix$byClass[, 'Sensitivity']
    
    # S'assurer que les dimensions sont correctes
    if (length(precision_values) == ncol(fold_precisions) && length(recall_values) == ncol(fold_recalls)) {
      fold_precisions[i, 1:length(precision_values)] <- precision_values
      fold_recalls[i, 1:length(recall_values)] <- recall_values
      fold_f1_scores[i, 1:length(precision_values)] <- 2 * (precision_values * recall_values) / (precision_values + recall_values)
    }
  }
}

# Calcul de la moyenne et de l'écart type pour les metrics
mean_accuracy <- mean(fold_accuracies, na.rm = TRUE)
sd_accuracy <- sd(fold_accuracies, na.rm = TRUE)

# Entraînement du modèle SVM avec validation croisée complète
svm_model <- train(x = X_train, y = y_train, method = "svmRadial", trControl = trainControl(method = "cv", number = 10))

# Prédictions avec le modèle SVM
svm_predictions_train <- predict(svm_model, X_train)
svm_predictions_test <- predict(svm_model, X_test)

# Calculer les métriques pour les ensembles d'entraînement et de test
conf_matrix_train_svm <- confusionMatrix(data = svm_predictions_train, reference = y_train)
conf_matrix_test_svm <- confusionMatrix(data = svm_predictions_test, reference = y_test)

accuracy_train_svm <- conf_matrix_train_svm$overall['Accuracy']
accuracy_test_svm <- conf_matrix_test_svm$overall['Accuracy']

precision_train_svm <- mean(conf_matrix_train_svm$byClass[, 'Pos Pred Value'], na.rm = TRUE)
precision_test_svm <- mean(conf_matrix_test_svm$byClass[, 'Pos Pred Value'], na.rm = TRUE)
recall_train_svm <- mean(conf_matrix_train_svm$byClass[, 'Sensitivity'], na.rm = TRUE)
recall_test_svm <- mean(conf_matrix_test_svm$byClass[, 'Sensitivity'], na.rm = TRUE)
F1_train_svm <- mean(2 * (precision_train_svm * recall_train_svm) / (precision_train_svm + recall_train_svm), na.rm = TRUE)
F1_test_svm <- mean(2 * (precision_test_svm * recall_test_svm) / (precision_test_svm + recall_test_svm), na.rm = TRUE)

# Calculer l'AUC
auc_train_svm <- roc(y_train, as.numeric(svm_predictions_train))$auc
auc_test_svm <- roc(y_test, as.numeric(svm_predictions_test))$auc

# Créer un data frame pour les métriques d'entraînement
train_metrics_svm <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "Recall"),
  Value = c(accuracy_train_svm, precision_train_svm, F1_train_svm, recall_train_svm),
  Dataset = "Training"
)

# Créer un data frame pour les métriques de test
test_metrics_svm <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "Recall"),
  Value = c(accuracy_test_svm, precision_test_svm, F1_test_svm, recall_test_svm),
  Dataset = "Test"
)

# Combiner les métriques d'entraînement et de test
all_metrics_svm <- rbind(test_metrics_svm, train_metrics_svm)

# Créer le graphique comparant les métriques de test et d'entraînement
plot <- ggplot(all_metrics_svm, aes(x = Metric, y = Value, fill = Dataset)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
  labs(title = "Comparison of Test and Training Metrics - SVM",
       x = "Metrics", y = "Values") +
  theme_minimal()

# Afficher le graphique
print(plot)

# Afficher les résultats de validation croisée
print(paste("Cross-Validation Mean Accuracy:", round(mean_accuracy, 3)))
print(paste("Cross-Validation Accuracy Standard Deviation:", round(sd_accuracy, 3)))



```
Ma nouvelle méthode Predomics_aggregation_ovo a été développé pour combiner le vote majoritaire avec un départage basé sur les scores cumulés en cas d'égalité.

Explication :
Comptage des votes : La fonction commence par compter les votes pour chaque échantillon.
Vérification des égalités : Si plusieurs classes obtiennent le même nombre de votes, on passe à l'étape de départage.
Départage avec scores : Pour les classes à égalité, la fonction calcule la somme des scores pour chaque classe et choisit celle avec la somme la plus élevée.
Retour du modèle : Le modèle avec les prédictions agrégées est renvoyé.
Cette méthode garantit que les prédictions sont basées sur une majorité, tout en utilisant les scores pour résoudre les égalités de manière judicieuse.

```{r}
pop <- "p"
```



