---
title: "vignette"
author: "Fabien"
date: "2023-10-04"
output: html_document
knitr: 
  duplicate.label: "allow"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load the packages, message=FALSE, warning=FALSE}
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
```

## Setting the learner context

```{r loading filtering the data, message=FALSE, warning=FALSE, paged.print=FALSE}
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
#recover the vector y
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Créer un vecteur d'indices pour la division des données
set.seed(42)  # Vous pouvez définir une graine pour la reproductibilité
indices_division <- createDataPartition(yvec_trie, p = 0.8, list = FALSE)

# Diviser yvec_trie en 80% train et 20% test
y <- as.vector(yvec_trie[indices_division])
y.test <- as.vector(yvec_trie[-indices_division])
X <- X_general[,indices_division]
X.test <- X_general[,-indices_division]

# Vérifier la répartition dans chaque ensemble
table(y)
table(y.test)
dim(X)
dim(X.test)

```

`
```{r}
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Créer un vecteur d'indices pour la division des données
set.seed(42)  # Vous pouvez définir une graine pour la reproductibilité
y = as.vector(yvec_trie)
X = X_general

# Nombre d'échantillons désiré dans chaque classe
nombre_echantillons_par_classe <- min(table(y))

# Fonction pour équilibrer les classes
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe,seed =123) {
  classes <- unique(y)
  indices_equilibres <- integer(0)
  
  for (classe in classes) {
    indices_classe <- which(y == classe)
    set.seed(seed)
    indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
  }
  
  return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}

# Équilibrer les classes dans l'ensemble d'entraînement
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)

# Utiliser les données équilibrées
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X

# Vérifier la répartition après équilibrage
#table(y_equilibre)

set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.96, list = FALSE)

# Diviser yvec_trie en 80% train et 20% test
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[,indices_division]
X.test <- X_equilibre[,-indices_division]

# Vérifier la répartition dans chaque ensemble
table(y)
table(y.test)
dim(X)
dim(X.test)
```


```{r setting the classifier}

clf <- terga1_mc(nCores = 1,
              seed = 1,
              plot = TRUE
)
printy(clf) # print the object for more information
isClf(clf)  # test whether the object is a classifier
class(clf)  # the class of the classifier object
```

## Running the learner experiment

```{r running experiment, echo=TRUE, fig.width=5, warning=FALSE}
runit = TRUE
if(runit)
{
  res_clf <- fit_mc(X = X.test, y = y.test, clf = clf,approch="ova", cross.validate = TRUE, nfolds = 1); # class(res_clf)
  # save results
  save(res_clf, clf, file = "res_clf.rda", compression_level = 9)
}

# ... Database X is not a matrix! Converting ...
# ... Classification mode, computing factor(y) for speedup and robustness
# ... Loading feature correlation for speedup
# ... Correlation file loaded
# ... Storing data in the classifier object for speedup
# ... Computing ternary coefficients for speedup
# ... One seed found, setting by default
# ... Running the classifier terga2 with a single CPU
# ... Second and faster version of terga fitting based on Genetic Algorithm heuristics ...
# ... Cross validation mode
# ... Starting cross validation not in parallel
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ... Learning process is finished succesfuly
# ... Thank you for using Predomics. Don't forget to digest the results now.
# 
# [1] "experiment" "predomics" 

```

```{r load results, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
if(!runit)
{
  load("res_clf.rda")
}
```

### Digesting the results

```{r digesting results, fig.height=8, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE}
res_clf.dig_terga1ova <- digest(obj = res_clf, penalty = 0.75/100, plot = TRUE)
```

### Visualizing the best model

```{r best model, fig.width=7, warning=FALSE}

# get the best model
best.model_terga1_mcovo <- res_clf.dig_terga1ova$best$model

```




















```{r}
model_test_terga1 <- list()
model_test_terga1$accuracy_ =best.model.test_terga1ovo$accuracy_
model_test_terga1$auc_ = best.model.test_terga1ovo$auc_
model_test_terga1$recall_ = best.model.test_terga1ovo$recall_
model_test_terga1$precision_ = best.model.test_terga1ovo$precision_
```


```{r}


# Sélectionner uniquement les métriques nécessaires
model_test_terga1 <- list(
  accuracy_ = model_test_terga1$accuracy_,
  auc_ = model_test_terga1$auc_,
  recall_ = model_test_terga1$recall_,
  precision_ = model_test_terga1$precision_
)

# Convertir la liste en un data frame
df <- data.frame(
  Metric = names(model_test_terga1),
  Value = unlist(model_test_terga1)
)

# Modifier la couleur pour chaque métrique
df$Color <- c("red", "green", "blue", "orange")

# Créer le graphique de barres avec ggplot2
library(ggplot2)
plot <- ggplot(df, aes(x = Metric, y = Value, fill = Color)) +
  geom_bar(stat = "identity", alpha = 0.7) +
  geom_text(aes(label = round(Value, 3)), vjust = -0.3) +
  ylim(0, 1) +
  labs(title = "Performance of model test terbeam one versus all",
       x = "Metrics",
       y = "Values") +
  theme_minimal()

# Afficher le graphique
print(plot)

```





```{r loading filtering the data, message=FALSE, warning=FALSE, paged.print=FALSE}
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
#recover the vector y
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Créer un vecteur d'indices pour la division des données
set.seed(42)  # Vous pouvez définir une graine pour la reproductibilité
indices_division <- createDataPartition(yvec_trie, p = 0.8, list = FALSE)

# Diviser yvec_trie en 80% train et 20% test
y <- as.vector(yvec_trie[indices_division])
y.test <- as.vector(yvec_trie[-indices_division])
X <- X_general[,indices_division]
X.test <- X_general[,-indices_division]

# Vérifier la répartition dans chaque ensemble
table(y)
table(y.test)
```


```{r}
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Créer un vecteur d'indices pour la division des données
set.seed(42)  # Vous pouvez définir une graine pour la reproductibilité
#indices_division <- createDataPartition(yvec_trie, p = 0.8, list = FALSE)

# Diviser yvec_trie en 80% train et 20% test
#y <- as.vector(yvec_trie[indices_division])
#y.test <- as.vector(yvec_trie[-indices_division])
#X <- X_general[,indices_division]
#X.test <- X_general[,-indices_division]

# Vérifier la répartition dans chaque ensemble
#table(y)
#table(y.test)
y = as.vector(yvec_trie)
X = X_general
```










































```{r roc analyses, fig.height=7, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}

tmp <- plotAUC(best.model$score_, y, percent = TRUE); rm(tmp)

# create the roc objects
rocobj.train <- roc(y ~ best.model$score_)
rocobj.test <- roc(y.test ~ best.model.test$score_)

# make the plot
ggroc(list(train = rocobj.train, test = rocobj.test))

```

###Family of Best Models (FBM)

```{r roc analyses, fig.height=7, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
# get the population of models scrambled by model size
pop <- modelCollectionToPopulation(res_clf$classifier$models)
printy(pop)
pop.df <- populationToDataFrame(pop)
head(pop.df[,-c(3,4,7,8,14)])
pop.df.melt <- melt(pop.df, id.vars = c("accuracy_","eval.sparsity"))

g.before <- ggplot(data = pop.df.melt, aes(y = accuracy_, x = eval.sparsity)) + 
  geom_boxplot(notch = FALSE, outlier.shape = " ", position = position_dodge(width=0.9), alpha = 0.3) + 
  geom_point(aes(color = eval.sparsity), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
  ylim(c(0,1)) +
  xlab("Model parsimony") +
  ggtitle("Original population") +
  theme_bw() +
  theme(legend.position="bottom", legend.direction="horizontal") +
  guides(colour="none")

# select the best 
fbm <- selectBestPopulation(pop)
printy(fbm)
fbm.df <- populationToDataFrame(fbm)
fbm.df.melt <- melt(fbm.df, id.vars = c("accuracy_","eval.sparsity"))#; head(fbm.df.melt)

g.after <- ggplot(data = fbm.df.melt, aes(y = accuracy_, x = eval.sparsity)) + 
  geom_boxplot(notch = FALSE, position = position_dodge(width=0.9), alpha = 0.3) + 
  geom_point(aes(color = eval.sparsity), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
  ylim(c(0,1)) +
  xlab("Model parsimony") +
  ggtitle("FBM") +
  theme_bw() +
  theme(legend.position="bottom", legend.direction="horizontal") +
  guides(colour="none")

grid.arrange(g.before, g.after, ncol =2)
```

###Analyzing the FBM

```{r feature exploration, fig.height=7, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
fa <- makeFeatureAnnot_ovo(pop = fbm, 
                       X = X, 
                       y = y, 
                       clf = clf)
dim(fa[[1]]$pop.noz)
(g1 <- plotFeatureModelCoeffs(feat.model.coeffs = fa[[1]]$pop.noz))
(g2 <- plotAbundanceByClass(features = rownames(fa[[1]]$pop.noz), X = list_X[[1]], y = list_y[[1]]))
(g3 <- plotPrevalence(features = rownames(fa[[1]]$pop.noz), X = list_X[[1]], y = list_y[[1]]))
```

### Random Forest sota

```{r}
fa[[1]]$feature.df
```

```{r}
clf <- sota.rf_ovo(sparsity = c(2:10, 50, 70, 100, 150, 200, 300), 
                                   nrow(X),
                                   max.nb.features = 10000,
                                   seed = (1:20),
                                   nCores = 1,
                                   evalToFit = "accuracy_",
                                   objective = "auc",
                                   ntree=500,
                                   experiment.id = "sota_rf_ovo",
                                   experiment.save = "nothing")
  
                      
  printy(clf) # 

```

### Running the learner experiment sota.RF ovo

```{r running experiment, echo=TRUE, fig.width=5, warning=FALSE}
runit = TRUE
if(runit)
{
  res_clf_sota <- fit_OVO(X = X, y = y, clf = clf, cross.validate = TRUE, nfolds = 1); # class(res_clf)
  # save results
  save(res_clf_sota, clf, file = "res_clf_sota.rda", compression_level = 9)
}
```

##Digesting the results

```{r digesting results, fig.height=10, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
res_clf.dig_sota <- digest(obj = res_clf_sota, penalty = 0.75/100, plot = TRUE)
```

```{r}
# get the best model sota.rf_ovo
# get the best model
best.modelsota <- res_clf.dig_sota$best$model
##printy(best.model)

```

### Testing the model in another dataset

```{r}
best.model.test_sota <- evaluateModel_ovo(mod = best.modelsota, X = X.test, y = y.test, clf = clf, eval.all = TRUE, force.re.evaluation = TRUE, mode = "test")
##printy(best.model.test)
```

###One-versus-one class distribution train

```{r One-versus-one class distribution}
  nClasse <- unique(y)
  list_y <- list()
  list_X <- list()
  k <- 1
  for (i in 1:(length(nClasse)-1)) {
    for (j in (i+1):length(nClasse)) {
      class_i <- nClasse[i]
      class_j <- nClasse[j]
      indices <- which(y == class_i | y == class_j)
      y_pair <- y[indices]
      X_pair <- X[,indices]
      list_y[[k]] <- y_pair
      list_X[[k]] <- X_pair
      k <- k + 1
    }
  }
```

###One-versus-one class distribution test

```{r One-versus-one class distribution}
 nClasse <- unique(y.test)
  list_y.test <- list()
  list_X.test <- list()
  k <- 1
  for (i in 1:(length(nClasse)-1)) {
    for (j in (i+1):length(nClasse)) {
      class_i <- nClasse[i]
      class_j <- nClasse[j]
      indicess <- which(y.test == class_i | y.test == class_j)
      y_paire <- y.test[indicess]
      X_paire <- X.test[,indicess]
      list_y.test[[k]] <- y_paire
      list_X.test[[k]] <- X_paire
      k <- k + 1
    }
  }
```

```{r}
  # we recover the first output to apply the plot
  X <- list_X[[1]]
  y <- list_y[[1]]
  X.test <- list_X.test[[1]]
  y.test <- list_y.test[[1]]
```

### Visualizing model performance AUC

```{r roc analyses, fig.height=7, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALS}
tmp <- plotAUC(best.model$score_, y, percent = TRUE); rm(tmp)

# create the roc objects
rocobj.train <- roc(y ~ best.model$score_)
rocobj.test <- roc(y.test ~ best.model.test$score_)

# make the plot
ggroc(list(train = rocobj.train, test = rocobj.test))
```

###Family of Best Models (FBM)

```{r roc analyses, fig.height=7, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
# get the population of models scrambled by model size
pop <- modelCollectionToPopulation(res_clf$classifier$models)
printy(pop)
pop.df <- populationToDataFrame(pop)
head(pop.df[,-c(3,4,7,8,14)])
pop.df.melt <- melt(pop.df, id.vars = c("accuracy_","eval.sparsity"))

g.before <- ggplot(data = pop.df.melt, aes(y = accuracy_, x = eval.sparsity)) + 
  geom_boxplot(notch = FALSE, outlier.shape = " ", position = position_dodge(width=0.9), alpha = 0.3) + 
  geom_point(aes(color = eval.sparsity), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
  ylim(c(0,1)) +
  xlab("Model parsimony") +
  ggtitle("Original population") +
  theme_bw() +
  theme(legend.position="bottom", legend.direction="horizontal") +
  guides(colour="none")

# select the best 
fbm <- selectBestPopulation(pop)
printy(fbm)
fbm.df <- populationToDataFrame(fbm)
fbm.df.melt <- melt(fbm.df, id.vars = c("accuracy_","eval.sparsity"))#; head(fbm.df.melt)

g.after <- ggplot(data = fbm.df.melt, aes(y = accuracy_, x = eval.sparsity)) + 
  geom_boxplot(notch = FALSE, position = position_dodge(width=0.9), alpha = 0.3) + 
  geom_point(aes(color = eval.sparsity), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
  ylim(c(0,1)) +
  xlab("Model parsimony") +
  ggtitle("FBM") +
  theme_bw() +
  theme(legend.position="bottom", legend.direction="horizontal") +
  guides(colour="none")

grid.arrange(g.before, g.after, ncol =2)
```

###Analyzing the FBM

```{r}
clf <- terBeam_mc(sparsity = c( 2, 3, 4, 5), 
                                   max.nb.features = 1000,
                                   seed = 1,
                                   nCores = 1,
                                   evalToFit = "accuracy_",
                                   objective = "auc",
                                   experiment.id = "terBeam_mc",
                                   experiment.save = "nothing")
  
                      
  printy(clf) # 
```

```{r running experiment, echo=TRUE, fig.width=5, warning=FALSE}
runit = TRUE
if(runit)
{
  res_clfterbeamova <- fit_OVO(X = X, y = y, clf = clf,approch="ova", cross.validate = TRUE, nfolds = 1); # class(res_clf)

}
```

```{r digesting results, fig.height=8, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE}
res_clf.dig_terbeamova <- digest(obj = res_clfterbeamova, penalty = 0.75/100, plot = TRUE)
```

```{r}
mod <- res_clf.dig_terbeamova$best$model
```




```{r}
# Vecteur original
y <- c("BACT1", "BACT1", "BACT2", "BACT2", "PREV", "PREV", "RUM", "RUM")

# Prédictions de différents modèles
model1 <- c("ALL", "BACT1", "ALL", "ALL", "ALL", "ALL", "ALL", "ALL")
model2 <- c("ALL", "ALL", "BACT2", "BACT2", "ALL", "ALL", "ALL", "BACT2")
model3 <- c("ALL", "ALL", "ALL", "ALL", "PREV", "PREV", "ALL", "ALL")
model4 <- c("ALL", "ALL", "ALL", "ALL", "ALL", "ALL", "RUM", "RUM")

# Liste des prédictions des modèles
predictions_list <- list(model1, model2, model3, model4)

# Fonction pour agréger les prédictions par vote majoritaire
aggregate_predictions <- function(predictions_list) {
  num_models <- length(predictions_list)
  num_samples <- length(predictions_list[[1]])

  aggregated_predictions <- character(num_samples)

  for (i in 1:num_samples) {
    votes <- table(unlist(lapply(predictions_list, function(model) model[i])))
    aggregated_predictions[i] <- names(votes)[which.min(votes)]
  }

  return(aggregated_predictions)
}

# Utilisation de la fonction avec vos prédictions
final_predictions <- aggregate_predictions(predictions_list)

# Afficher les prédictions agrégées
print(final_predictions)



```

```{r}
# Fonction pour agréger les prédictions en utilisant la méthode du score maximum
aggregate_predictions <- function(predictions) {
  result <- character(length(predictions[[1]]))
  
  for (i in seq_along(predictions[[1]])) {
    # Extraire les scores pour chaque classe
    scores <- sapply(predictions, `[`, i)
    
    # Vérifier si tous les scores sont égaux ou manquants
    if (length(unique(scores[!is.na(scores)])) == 1) {
      result[i] <- "TIE"
    } else {
      # Obtenir les fréquences des scores
      score_counts <- table(scores)
      
      # Trouver les indices des scores les plus fréquents
      max_indices <- which(score_counts == max(score_counts))
      
      # Choisir de manière aléatoire parmi les scores les plus fréquents
      selected_score <- as.numeric(names(sample(score_counts[max_indices], 1)))
      
      # Trouver l'indice de la classe correspondante pour le score choisi
      selected_index <- which(scores == selected_score)
      
      # Récupérer le nom de la classe correspondante
      result[i] <- names(predictions[[1]])[selected_index[1]]
    }
  }
  
  return(result)
}



# Exemple de prédictions one-versus-all avec des scores aléatoires
set.seed(123)  # Pour la reproductibilité des résultats
predictions_list <- list(
  runif(4),  # Scores pour BACT1
  runif(4),  # Scores pour BACT2
  runif(4),  # Scores pour PREV
  runif(4)   # Scores pour RUM
)

# Appliquer la méthode du score maximum à chaque observation
aggregated_predictions <- aggregate_predictions(predictions_list)

# Vraies valeurs (remplacez par vos vraies valeurs)
true_values <- c("BACT1", "BACT2", "PREV", "RUM")

# Assurer que les prédictions agrégées ont la même longueur que les vraies valeurs
aggregated_predictions <- aggregated_predictions[1:length(true_values)]

# Créer la table de contingence
confusion_matrix <- table(Prédiction = aggregated_predictions, Réalité = true_values)

# Afficher la table de contingence
print(confusion_matrix)


```




```{r}
# Using the function to aggregate predictions.
aggregated_predictions <- aggregate_predictions(classes_list, score_list)

# Affichage des prédictions agrégées
print(aggregated_predictions)
```






