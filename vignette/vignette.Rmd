---
title: "vignette"
author: "Fabien"
date: "2023-10-04"
output: html_document
knitr: 
  duplicate.label: "allow"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load the packages, message=FALSE, warning=FALSE}
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
```

# Loading and Analysis of Used Data

```{r}
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Create an index vector for data partitioning

X_general <- X_general[rowSums(X_general)!=0,]; dim(X_general) # filter out variables with only zero values
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE); dim(X_general)

set.seed(42)  
y = as.vector(yvec_trie)
X = X_general

# Number of desired samples in each class
nombre_echantillons_par_classe <- min(table(y))

# Function to balance the classes
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe,seed =123) {
  classes <- unique(y)
  indices_equilibres <- integer(0)
  
  for (classe in classes) {
    indices_classe <- which(y == classe)
    set.seed(seed)
    indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
  }
  
  return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}

donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X

# Verify the distribution after balancing

set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)

# Split yvec_trie into 80% train and 20% test
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[,indices_division]
X.test <- X_equilibre[,-indices_division]

table(y)
table(y.test)
dim(X)
dim(X.test)
```



#Classifying using the terbeam algorithm

```{r setting the classifier}
clf <- terBeam_mc(sparsity = c(2,3,4), 
                                   max.nb.features = 1000,
                                   seed = 1,
                                   nCores = 1,
                                   evalToFit = "accuracy_",
                                   objective = "auc",
                                   experiment.id = "terBeam_mc",
                                   experiment.save = "nothing")
  
                      
  printy(clf)  

```


# Run the multiclass learning experiment using the one-versus-one binarization strategy.

```{r running experiment, echo=TRUE, fig.width=5, warning=FALSE}
runit = TRUE
if(runit)
{
Unique_rankingAggregation <- fit_mc(X = X, y = y, clf = clf,approch="ova", cross.validate = TRUE,aggregation_ = "rankingAggregation", nfolds= 10); 
  save(Unique_rankingAggregation , clf, file ="Unique_rankingAggregation.rda", compression_level = 9)
}

# ... Database X is not a matrix! Converting ...
# ... Classification mode, computing factor(y) for speedup and robustness
# ... Loading feature correlation for speedup
# ... Correlation file loaded
# ... Storing data in the classifier object for speedup
# ... Computing ternary coefficients for speedup
# ... One seed found, setting by default
# ... Running the classifier terga2 with a single CPU
# ... Second and faster version of terga fitting based on Genetic Algorithm heuristics ...
# ... Cross validation mode
# ... Starting cross validation not in parallel
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ... Learning process is finished succesfuly
# ... Thank you for using Predomics. Don't forget to digest the results now.
# 
# [1] "experiment" "predomics" 

```

# Load the results of the one-versus-one training.

```{r load results, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

  load("res_clf_ovo.rda")

```

## Digesting the results

```{r digesting results, fig.height=8, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE}

clf_ovo_we <- digestmc(obj =Unique_rankingAggregation   , penalty = 0.75/100, plot = TRUE)

```

## Explore the training and generalization scores using cross-validation with the same variables.


```{r load results, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
 load("Unique_Predomics_aggregation_ova.rda")
load("Unique_maximizationAggregation.rda")
load("Unique_rankingAggregation.rda")
load("Unique_weightedAggregation.rda")
load("Unique_votingAggregation.rda")
```


```{r fig.height=6, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE}

library(reshape2)
library(ggplot2)
library(ggpubr)

transform_model_data <- function(model_data, model_name) {
  testlist <- model_data$crossVal$scores[grep("generalization|empirical", names(model_data$crossVal$scores))]
  testlist2 <- list()
  for(i in names(testlist)) {
    idf <- testlist[[i]]
    if(unique(colSums(is.na(idf))) != nrow(idf)) {
      idf$sparsity <- rownames(idf)
      idf <- melt(idf)
      idf$source <- i
      testlist2[[i]] <- idf
    }
  }
  testlist2.df <- do.call("rbind", testlist2)
  testlist2.df$emp_gen <- gsub("\\..*$", "", testlist2.df$source)
  testlist2.df$metric <- gsub("^.*\\.", "", testlist2.df$source)
  testlist2.df$model <- model_name
  testlist2.df <- testlist2.df[is.finite(testlist2.df$value), ]
  testlist2.df <- testlist2.df[testlist2.df$metric %in% c("acc", "prc"), ]
  return(testlist2.df)
}

df1 <- transform_model_data(Unique_maximizationAggregation, "Unique_maximizationAggregation")
df2 <- transform_model_data(Unique_Predomics_aggregation_ova, "Unique_Predomics_aggregation_ova")

df3 <- transform_model_data(Unique_rankingAggregation, "Unique_rankingAggregation")
df4 <- transform_model_data(Unique_weightedAggregation, "Unique_weightedAggregation")
df5 <- transform_model_data(Unique_votingAggregation, "Unique_votingAggregation")

alldf <- rbind(df1, df2, df3, df4, df5)

ggplot(alldf, aes(x=model, y=value)) + 
  geom_point(aes(colour=sparsity)) + 
  facet_grid(.~metric+emp_gen) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


```{r fig.height=6, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE}
# Fonction pour transformer les données des modèles
transform_model_data <- function(model_data, model_name) {
  testlist <- model_data$crossVal$scores[grep("generalization|empirical", names(model_data$crossVal$scores))]
  testlist2 <- list()
  for(i in names(testlist)) {
    idf <- testlist[[i]]
    if(unique(colSums(is.na(idf))) != nrow(idf)) {
      idf$sparsity <- rownames(idf)
      idf <- melt(idf)
      idf$source <- i
      testlist2[[i]] <- idf
    }
  }
  testlist2.df <- do.call("rbind", testlist2)
  testlist2.df$emp_gen <- gsub("\\..*$", "", testlist2.df$source)
  testlist2.df$metric <- gsub("^.*\\.", "", testlist2.df$source)
  testlist2.df$model <- model_name
  testlist2.df <- testlist2.df[is.finite(testlist2.df$value), ]
  testlist2.df <- testlist2.df[testlist2.df$metric %in% c("f1s", "rec"), ]
  return(testlist2.df)
}

df1 <- transform_model_data(Unique_maximizationAggregation, "Unique_maximizationAggregation")
df2 <- transform_model_data(Unique_Predomics_aggregation_ova, "Unique_Predomics_aggregation_ova")

df3 <- transform_model_data(Unique_rankingAggregation, "Unique_rankingAggregation")
df4 <- transform_model_data(Unique_weightedAggregation, "Unique_weightedAggregation")
df5 <- transform_model_data(Unique_votingAggregation, "Unique_votingAggregation")

alldf <- rbind(df1, df2, df3, df4, df5)

ggplot(alldf, aes(x=model, y=value)) + 
  geom_point(aes(colour=sparsity)) + 
  facet_grid(.~metric+emp_gen) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```





```{r fig.height=7, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE}
library(ggplot2)
library(reshape2)

Unique_weighted <- Unique_weightedAggregation$crossVal$scores$mean.acc
Unique_maximization <- Unique_maximizationAggregation$crossVal$scores$mean.acc
Unique_Predomics_Aggre <- Unique_Predomics_aggregation_ova$crossVal$scores$mean.acc 
Unique_ranking <- Unique_rankingAggregation$crossVal$scores$mean.acc
Unique_voting <- Unique_votingAggregation$crossVal$scores$mean.acc

Unique_weighted$k <- rownames(Unique_weighted)
Unique_maximization$k <- rownames(Unique_maximization)
Unique_Predomics_Aggre$k <- rownames(Unique_Predomics_Aggre)
Unique_ranking$k <- rownames(Unique_ranking)
Unique_voting$k <- rownames(Unique_voting)

Unique_weighted$algorithm <- "Unique_weightedAggregation"
Unique_maximization$algorithm <- "Unique_maximizationAggregation"
Unique_Predomics_Aggre$algorithm <- "Unique_Predomics_Aggregation_ova"
Unique_voting$algorithm <- "Unique_votingAggregation"
Unique_ranking$algorithm <- "Unique_rankingAggregation"

Unique_weighted <- Unique_weighted[, c("k", "empirical", "generalization", "algorithm")]
Unique_maximization <- Unique_maximization[, c("k", "empirical", "generalization", "algorithm")]
Unique_Predomics_Aggre <-Unique_Predomics_Aggre[, c("k", "empirical", "generalization", "algorithm")]
Unique_voting <- Unique_voting[, c("k", "empirical", "generalization", "algorithm")]
Unique_ranking <- Unique_ranking[, c("k", "empirical", "generalization", "algorithm")]

combined_df <- rbind(Unique_weighted,Unique_maximization, 
                     Unique_Predomics_Aggre, Unique_ranking, 
                     Unique_voting)

combined_df <- na.omit(combined_df)


data_melted <- melt(combined_df, id.vars = c("k", "algorithm"), variable.name = "type", value.name = "accuracy")


ggplot(data_melted, aes(x = k, y = accuracy, color = type)) +
  geom_point(size = 3) +
  geom_line(aes(group = interaction(algorithm, type), linetype = type)) +
  geom_text(aes(label = round(accuracy, 3)), vjust = -0.5, size = 3) +  # Ajouter les valeurs à côté des points
  facet_wrap(~ algorithm, ncol = 2) +  # Afficher deux graphiques par ligne
  theme_minimal() +
  labs(title = "Empirical vs Generalization Accuracy for Different Algorithms",
       x = "Valeur de k",
       y = "Accuracy",
       color = "Type") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_manual(values = c("empirical" = "#377EB8", "generalization" = "#E41A1C"))
```


##Explore the training and generalization scores using cross-validation with different variables.

```{r load results, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
 load("Multi_Predomics_aggregation_ova.rda")
load("Multi_maximizationAggregation.rda")
load("Multi_rankingAggregation.rda")
load("Multi_weightedAggregation.rda")
load("Multi_votingAggregation.rda")
```


```{r fig.height=6, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE}
library(reshape2)
library(ggplot2)
library(ggpubr)

transform_model_data <- function(model_data, model_name) {
  testlist <- model_data$crossVal$scores[grep("generalization|empirical", names(model_data$crossVal$scores))]
  testlist2 <- list()
  for(i in names(testlist)) {
    idf <- testlist[[i]]
    if(unique(colSums(is.na(idf))) != nrow(idf)) {
      idf$sparsity <- rownames(idf)
      idf <- melt(idf)
      idf$source <- i
      testlist2[[i]] <- idf
    }
  }
  testlist2.df <- do.call("rbind", testlist2)
  testlist2.df$emp_gen <- gsub("\\..*$", "", testlist2.df$source)
  testlist2.df$metric <- gsub("^.*\\.", "", testlist2.df$source)
  testlist2.df$model <- model_name
  testlist2.df <- testlist2.df[is.finite(testlist2.df$value), ]
  testlist2.df <- testlist2.df[testlist2.df$metric %in% c("acc", "prc"), ]
  return(testlist2.df)
}

df1 <- transform_model_data(Unique_maximizationAggregation, "Multi_maximizationAggregation")
df2 <- transform_model_data(Unique_Predomics_aggregation_ova, "Multi_Predomics_aggregation_ova")

df3 <- transform_model_data(Unique_rankingAggregation, "Multi_rankingAggregation")
df4 <- transform_model_data(Unique_weightedAggregation, "Multi_weightedAggregation")
df5 <- transform_model_data(Unique_votingAggregation, "Multi_votingAggregation")

alldf <- rbind(df1, df2, df3, df4, df5)

ggplot(alldf, aes(x=model, y=value)) + 
  geom_point(aes(colour=sparsity)) + 
  facet_grid(.~metric+emp_gen) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r fig.height=6, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE}
transform_model_data <- function(model_data, model_name) {
  testlist <- model_data$crossVal$scores[grep("generalization|empirical", names(model_data$crossVal$scores))]
  testlist2 <- list()
  for(i in names(testlist)) {
    idf <- testlist[[i]]
    if(unique(colSums(is.na(idf))) != nrow(idf)) {
      idf$sparsity <- rownames(idf)
      idf <- melt(idf)
      idf$source <- i
      testlist2[[i]] <- idf
    }
  }
  testlist2.df <- do.call("rbind", testlist2)
  testlist2.df$emp_gen <- gsub("\\..*$", "", testlist2.df$source)
  testlist2.df$metric <- gsub("^.*\\.", "", testlist2.df$source)
  testlist2.df$model <- model_name
  testlist2.df <- testlist2.df[is.finite(testlist2.df$value), ]
  testlist2.df <- testlist2.df[testlist2.df$metric %in% c("f1s", "rec"), ]
  return(testlist2.df)
}

df1 <- transform_model_data(Unique_maximizationAggregation, "Multi_maximizationAggregation")
df2 <- transform_model_data(Unique_Predomics_aggregation_ova, "Multi_Predomics_aggregation_ova")

df3 <- transform_model_data(Unique_rankingAggregation, "Multi_rankingAggregation")
df4 <- transform_model_data(Unique_weightedAggregation, "Multi_weightedAggregation")
df5 <- transform_model_data(Unique_votingAggregation, "Multi_votingAggregation")

alldf <- rbind(df1, df2, df3, df4, df5)

ggplot(alldf, aes(x=model, y=value)) + 
  geom_point(aes(colour=sparsity)) + 
  facet_grid(.~metric+emp_gen) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```





```{r fig.height=7, fig.width=8, message=FALSE, warning=FALSE, paged.print=FALSE}

library(ggplot2)
library(reshape2)

# Données fournies
Multi_weighted <- Multi_weightedAggregation$crossVal$scores$mean.acc
Multi_maximization <- Multi_maximizationAggregation$crossVal$scores$mean.acc
Multi_Predomics_Aggre <- Multi_Predomics_Aggregation_ova$crossVal$scores$mean.acc
Multi_ranking <- Multi_rankingAggregation$crossVal$scores$mean.acc
Multi_voting <- Multi_votingAggregation$crossVal$scores$mean.acc

# Ajouter la colonne 'k'
Multi_weighted$k <- rownames(Multi_weighted)
Multi_maximization$k <- rownames(Multi_maximization)
Multi_Predomics_Aggre$k <- rownames(Multi_Predomics_Aggre)
Multi_ranking$k <- rownames(Multi_ranking)
Multi_voting$k <- rownames(Multi_voting)

# Ajouter une colonne 'algorithm' pour chaque dataframe
Multi_weighted$algorithm <- "Multi_weightedAggregation"
Multi_maximization$algorithm <- "Multi_maximizationAggregation"
Multi_Predomics_Aggre$algorithm <- "Multi_Predomics_Aggregation_ova"
Multi_voting$algorithm <- "Multi_votingAggregation"
Multi_ranking$algorithm <- "Multi_rankingAggregation"


# Réordonner les colonnes pour avoir "k" et "algorithm" en premier
Multi_weighted <- Multi_weighted[, c("k", "empirical", "generalization", "algorithm")]
Multi_maximization <- Multi_maximization[, c("k", "empirical", "generalization", "algorithm")]
Multi_Predomics_Aggre <- Multi_Predomics_Aggre[, c("k", "empirical", "generalization", "algorithm")]
Multi_voting <- Multi_voting[, c("k", "empirical", "generalization", "algorithm")]
Multi_ranking <- Multi_ranking[, c("k", "empirical", "generalization", "algorithm")]


# Combiner les dataframes
combined_df <- rbind(Multi_weighted, Multi_maximization, 
                     Multi_Predomics_Aggre, Multi_ranking, 
                     Multi_voting)

# Supprimer les lignes contenant des valeurs NaN
combined_df <- na.omit(combined_df)

# Transformer les données pour ggplot2
data_melted <- melt(combined_df, id.vars = c("k", "algorithm"), variable.name = "type", value.name = "accuracy")

# Plotting
ggplot(data_melted, aes(x = k, y = accuracy, color = type)) +
  geom_point(size = 3) +
  geom_line(aes(group = interaction(algorithm, type), linetype = type)) +
  geom_text(aes(label = round(accuracy, 3)), vjust = -0.5, size = 3) +  # Ajouter les valeurs à côté des points
  facet_wrap(~ algorithm, ncol = 2) +  # Afficher deux graphiques par ligne
  theme_minimal() +
  labs(title = "Empirical vs Generalization Accuracy for Different Algorithms",
       x = "Valeur de k",
       y = "Accuracy",
       color = "Type") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_color_manual(values = c("empirical" = "#377EB8", "generalization" = "#E41A1C"))




```


















```{r}
best.model.test <- evaluateModel_mc(mod = best.model, X = X.test, y = y.test, clf = clf, eval.all = TRUE, force.re.evaluation = TRUE,  approch = approch, aggregation_ = aggregation_, mode = "test")

```


# State of the art 


## Random Forest


```{r warning=FALSE}
library(caret)
library(e1071)
X_train = t(X)
y_train = y
X_test = t(X.test)
y_test =y.test
y_train <- as.factor(y_train)
y_test <- as.factor(y_test)


# Define 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)

# Train the multiclass random forest model with 10-fold cross-validation
rf_model <- train(x = X_train, y = y_train, method = "rf", trControl = train_control)


```


```{r warning=FALSE}
# Prediction on the training data
train_pred <- predict(rf_model, X_train)

# Calculation of metrics for the training data
train_metrics <- confusionMatrix(train_pred, y_train)

# Calculation of the AUC for training
roc_train <- multiclass.roc(y_train, as.numeric(train_pred))
auc_train <- auc(roc_train)

# Displaying metrics for training
print(train_metrics)
print(auc_train)

# Prediction on the test data
test_pred <- predict(rf_model, X_test)

# Calculation of metrics for the test data
test_metrics <- confusionMatrix(test_pred, y_test)

# Calculation of the AUC for the test data.
roc_test <- multiclass.roc(y_test, as.numeric(test_pred))
auc_test <- auc(roc_test)

# Displaying metrics for the test data
print(test_metrics)
print(auc_test)
```


```{r warning=FALSE}
# Calculating metrics for the training data
train_accuracy <- train_metrics$overall["Accuracy"]
train_precision <- mean(train_metrics$byClass[,"Precision"], na.rm = TRUE)
train_recall <- mean(train_metrics$byClass[,"Recall"], na.rm = TRUE)
train_auc <- auc_train
# Calculer le score F1 pour les données d'entraînement
f1_train <- 2 * (train_precision * train_recall) / (train_precision + train_recall)

# Afficher le score F1 pour les données d'entraînement
print(paste("Train F1 Score:", f1_train))

# Displaying metrics for the training data
print(paste("Train Accuracy:", train_accuracy))
print(paste("Train Precision:", train_precision))
print(paste("Train Recall:", train_recall))
print(paste("Train AUC:", train_auc))

# Calculating metrics for the test data
test_metrics <- confusionMatrix(test_pred, y_test)
test_accuracy <- test_metrics$overall["Accuracy"]
test_precision <- mean(test_metrics$byClass[,"Precision"], na.rm = TRUE)
test_recall <- mean(test_metrics$byClass[,"Recall"], na.rm = TRUE)

# Calculating the AUC for the test data
roc_test <- multiclass.roc(y_test, as.numeric(test_pred))
test_auc <- auc(roc_test)
# Calculer et afficher le score F1 pour les données de test
test_precision <- mean(test_metrics$byClass[,"Precision"], na.rm = TRUE)
test_recall <- mean(test_metrics$byClass[,"Recall"], na.rm = TRUE)
f1_test <- 2 * (test_precision * test_recall) / (test_precision + test_recall)

print(paste("Test F1 Score:", f1_test))
# Displaying metrics for the test data
print(paste("Test Accuracy:", test_accuracy))
print(paste("Test Precision:", test_precision))
print(paste("Test Recall:", test_recall))
print(paste("Test AUC:", test_auc))

```


```{r warning=FALSE, fig.height=4.5, fig.width=6, message=FALSE}
library(ggplot2)

# Example of metrics for the training data
accuracy_train <- train_accuracy
precision_train <- train_precision
AUC_train <- train_auc
recall_train <- train_recall
f1_train <- f1_train

# Example of metrics for the test data
accuracy_test <- test_accuracy
precision_test <- test_precision
AUC_test <- test_auc
recall_test <- test_recall
f1_test <- f1_test

# Create a data frame for the training metrics
train_metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "Recall"),
  Value = c(accuracy_train, precision_train, f1_train, recall_train),
  Dataset = "Training"
)

# Create a data frame for the test metrics
test_metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "Recall"),
  Value = c(accuracy_test, precision_test, f1_test, recall_test),
  Dataset = "Test"
)

# Combine the training and test metrics
all_metrics <- rbind(train_metrics, test_metrics)

# Create the graph
plot <- ggplot(all_metrics, aes(x = Metric, y = Value, fill = Dataset)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
  labs(title = "Comparison of test and training metrics Sota RF",
       x = "Metrics", y = "Values") +
  theme_minimal()

# Display the graph
print(plot)

```


## Regression Logistique

```{r warning=FALSE}
library(nnet)
# Create a data.frame from matrix X
library(glmnet)
# Utilize cv.glmnet with 10-fold cross-validation
df <- as.data.frame(t(X))
df$y <- factor(y)
cv_model_rl <- cv.glmnet(as.matrix(df[, -ncol(df)]), df$y, family = "multinomial", alpha = 1, nfolds = 10)

# Get the best model based on cross-validation
best_model_rl <- cv_model_rl$glmnet.fit

# Obtain predictions directly from cross-validation
predictions_rl <- predict(cv_model_rl, newx = as.matrix(df[, -ncol(df)]), s = "lambda.min", type = "class")
predictions_rl <- factor(predictions_rl)

# Check the dimensions and create the confusion matrix
if(length(predictions_rl) == length(df$y)) {
  confusionMatrix(predictions_rl, df$y)
} else {
  print("The dimensions of the predictions do not match those of the ground truth")
}

```



```{r warning=FALSE}
conf_matrix_rl <- confusionMatrix(predictions_rl, df$y)

# Calcul de l'exactitude
accuracy_train_rl <- conf_matrix_rl$overall[["Accuracy"]]

# Calcul de la précision moyenne de chaque classe
precision_train_rl <- mean(conf_matrix_rl$byClass[ , "Precision"])

# Calcul de l'AUC en utilisant la précision équilibrée moyenne (Balanced Accuracy)
AUC_train_rl <- mean(conf_matrix_rl$byClass[ , "Balanced Accuracy"])

# Calcul du rappel moyen (Sensibilité) de chaque classe
recall_train_rl <- mean(conf_matrix_rl$byClass[ , "Sensitivity"])

# Calcul du score F1 pour les données d'entraînement
F1_train_rl <- 2 * (precision_train_rl * recall_train_rl) / (precision_train_rl + recall_train_rl)

# Affichage des métriques pour les données d'entraînement
print(paste("Train Accuracy:", accuracy_train_rl))
print(paste("Train Precision:", precision_train_rl))
print(paste("Train Recall (Sensitivity):", recall_train_rl))
print(paste("Train AUC (Balanced Accuracy):", AUC_train_rl))
print(paste("Train F1 Score:", F1_train_rl))


```



```{r warning=FALSE}
predictions_test1_rl <- predict(cv_model_rl, newx = as.matrix(X_test), type = "class")
library(caret)
predictions_test1_rl <- as.factor(predictions_test1_rl)


```


```{r warning=FALSE}
# Create the confusion matrix for predictions on the test dataset
conf_matrix_test_rl <- confusionMatrix(predictions_test1_rl, y_test)

# Calculate accuracy, precision, AUC, and recall for the test dataset
# Calculate accuracy, precision, AUC, and recall for the test dataset
accuracy_test1_rl <- conf_matrix_test_rl$overall[["Accuracy"]]
precision_test1_rl <- mean(conf_matrix_test_rl$byClass[ , "Precision"], na.rm = TRUE)
AUC_test1_rl <- mean(conf_matrix_test_rl$byClass[ , "Balanced Accuracy"], na.rm = TRUE)
recall_test1_rl <- mean(conf_matrix_test_rl$byClass[ ,"Sensitivity"], na.rm = TRUE)
# Calcul du score F1 pour les données d'entraînement
F1_test1_rl <- 2 * (precision_test1_rl * recall_test1_rl) / (precision_test1_rl + recall_test1_rl)

```



```{r warning=FALSE, fig.height=4.5, fig.width=6, message=FALSE}
library(ggplot2)

# Create a data frame for the training metrics
train_metrics_rl <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "Recall"),
  Value = c(accuracy_train_rl, precision_train_rl, F1_train_rl, recall_train_rl),
  Dataset = "Training"
)

# Create a data frame for the test metrics
test_metrics_rl <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "Recall"),
  Value = c(accuracy_test1_rl, precision_test1_rl, F1_test1_rl, recall_test1_rl),
  Dataset = "Test"
)

# Combine the training and test metrics
all_metrics_rl <- rbind(test_metrics_rl,train_metrics_rl)

# Create the graph
plot <- ggplot(all_metrics_rl, aes(x = Metric, y = Value, fill = Dataset)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
  labs(title = "Comparison of test and training metrics Sota LR",
       x = "Metrics", y = "Values") +
  theme_minimal()

# Display the graph
print(plot)

```


## SVM

```{r warning=FALSE}
library(caret)
library(pROC)

# Train an SVM model with radial kernel and 10-fold cross-validation
svm_model <- train(x = X_train, y = y_train, method = "svmRadial", trControl = trainControl(method = "cv", number = 10))

# Make predictions with the SVM model
svm_predictions_train <- predict(svm_model, X_train)
svm_predictions_test <- predict(svm_model, X_test)

# Calculate the accuracy for the training and test sets
accuracy_train_svm <- confusionMatrix(data = svm_predictions_train, reference = y_train)$overall['Accuracy'][1]
accuracy_test_svm <- confusionMatrix(data = svm_predictions_test, reference = y_test)$overall['Accuracy'][1]

# Calculate precision, recall, and AUC for the training and test sets
precision_train_svm <- confusionMatrix(data = svm_predictions_train, reference = y_train)$byClass[, 'Pos Pred Value']
precision_test_svm <- confusionMatrix(data = svm_predictions_test, reference = y_test)$byClass[, 'Pos Pred Value']

recall_train_svm <- confusionMatrix(data = svm_predictions_train, reference = y_train)$byClass[, 'Sensitivity']
recall_test_svm <- confusionMatrix(data = svm_predictions_test, reference = y_test)$byClass[, 'Sensitivity']
F1_train_svm <- 2 * (precision_train_svm * recall_train_svm) / (precision_train_svm + recall_train_svm)
F1_train = mean(F1_train_svm)
auc_train_svm <- roc(y_train, as.numeric(svm_predictions_train))$auc
auc_test_svm <- roc(y_test, as.numeric(svm_predictions_test))$auc



```


```{r}
precision_train_svm = mean(precision_train_svm)
precision_test_svm = mean(precision_test_svm)
recall_train_svm = mean(recall_train_svm)
recall_test_svm = mean(recall_test_svm)
F1_test_svm <- 2 * (precision_test_svm * recall_test_svm) / (precision_test_svm + recall_test_svm)

```

```{r}
F1_test_svm <- mean(F1_test_svm)
```


```{r warning=FALSE, fig.height=4.5, fig.width=6, message=FALSE}
library(ggplot2)

# Create a data frame for the training metrics
train_metrics_svm <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "Recall"),
  Value = c(accuracy_train_svm, precision_train_svm, F1_train, recall_train_svm),
  Dataset = "Training"
)

# Create a data frame for the test metrics
test_metrics_svm <- data.frame(
  Metric = c("Accuracy", "Precision", "F1", "Recall"),
  Value = c(accuracy_test_svm, precision_test_svm, F1_test_svm, recall_test_svm),
  Dataset = "Test"
)

# Combine the training and test metrics
all_metrics_svm <- rbind(test_metrics_svm,train_metrics_svm)

# Create the graph
plot <- ggplot(all_metrics_svm, aes(x = Metric, y = Value, fill = Dataset)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
  labs(title = "Comparison of test and training metrics Sota SVM",
       x = "Metrics", y = "Values") +
  theme_minimal()

# Display the graph
print(plot)

```
