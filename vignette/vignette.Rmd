---
title: "vignette"
author: "Fabien"
date: "2023-10-04"
output: html_document
knitr: 
  duplicate.label: "allow"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load the packages, message=FALSE, warning=FALSE}
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
```

## Setting the learner context

```{r}
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Créer un vecteur d'indices pour la division des données
set.seed(42)  # Vous pouvez définir une graine pour la reproductibilité
y = as.vector(yvec_trie)
X = X_general

# Nombre d'échantillons désiré dans chaque classe
nombre_echantillons_par_classe <- min(table(y))

# Fonction pour équilibrer les classes
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe,seed =123) {
  classes <- unique(y)
  indices_equilibres <- integer(0)
  
  for (classe in classes) {
    indices_classe <- which(y == classe)
    set.seed(seed)
    indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
  }
  
  return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}

# Équilibrer les classes dans l'ensemble d'entraînement
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)

# Utiliser les données équilibrées
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X

# Vérifier la répartition après équilibrage
#table(y_equilibre)

set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.1, list = FALSE)

# Diviser yvec_trie en 80% train et 20% test
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[,indices_division]
X.test <- X_equilibre[,-indices_division]

# Vérifier la répartition dans chaque ensemble
table(y)
table(y.test)
dim(X)
dim(X.test)
```


```{r setting the classifier}

clf <- terga1_mc(nCores = 1,
              seed = 1,
              plot = TRUE
)
printy(clf) # print the object for more information
isClf(clf)  # test whether the object is a classifier
class(clf)  # the class of the classifier object
```

## Running the learner experiment

```{r running experiment, echo=TRUE, fig.width=5, warning=FALSE}
runit = TRUE
if(runit)
{
  res_clf <- fit_mc(X = X, y = y, clf = clf,approch="ovo", cross.validate = TRUE, nfolds = 1); # class(res_clf)
  # save results
  save(res_clf, clf, file = "res_clf.rda", compression_level = 9)
}

# ... Database X is not a matrix! Converting ...
# ... Classification mode, computing factor(y) for speedup and robustness
# ... Loading feature correlation for speedup
# ... Correlation file loaded
# ... Storing data in the classifier object for speedup
# ... Computing ternary coefficients for speedup
# ... One seed found, setting by default
# ... Running the classifier terga2 with a single CPU
# ... Second and faster version of terga fitting based on Genetic Algorithm heuristics ...
# ... Cross validation mode
# ... Starting cross validation not in parallel
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ... Learning process is finished succesfuly
# ... Thank you for using Predomics. Don't forget to digest the results now.
# 
# [1] "experiment" "predomics" 

```

```{r load results, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
if(!runit)
{
  load("res_clf.rda")
}
```

### Digesting the results

```{r digesting results, fig.height=8, fig.width=6, message=FALSE, warning=FALSE, paged.print=FALSE}
res_clf.dig_terga1ova <- digest(obj = res_clf, penalty = 0.75/100, plot = TRUE)
```

### best model

```{r best model, fig.width=7, warning=FALSE}

# get the best model
mod <- res_clf.dig_terga1ova$best$model

```


### Prediction one versus all
```{r}
predict_ova <- predict_ova(mod, y, X, clf, force.re.evaluation = TRUE)
predict_ova
```


### `Aggregation Max Voting one versus all and table  contingency
```{r}
aggregate_predictions_Max_Voting <- aggregate_predictions_Max_Voting(list_predictions = predict_ova, y)
aggregate_predictions_Max_Voting
y_pred = aggregate_predictions_Max_Voting
table_contingency <- table(y, y_pred)
print(table_contingency)
```

###  Prediction one versus one

```{r}
predict_ovo <- predict_ovo(mod, y, X, clf, force.re.evaluation = TRUE )
predict_ovo
```

###  Aggregation : aggregate_predictions_Max_Voting one versus one and table  contingency
```{r}
aggregate_majoritaire_vote_ovo <- aggregate_majoritaire_vote_ovo(predictions_list = predict_ovo)
aggregate_majoritaire_vote_ovo
y_pred = aggregate_majoritaire_vote_ovo
table_contingency <- table(y, y_pred)
print(table_contingency)
```

