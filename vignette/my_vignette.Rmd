---
title: "my_vignette"
author: "Fabien"
date: "2024-12-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# The mcpredomics package is a supervised learning method for interpretable
# multiclass classification of metagenomic data. This method is derived 
# from one of the genetic algorithm-based heuristics in the predomics package.
# More details: https://github.com/predomics/predomicspkg

# Installation Instructions

## Step 1: Install dependencies
install.packages(c("doSNOW", "foreach", "snow", "doRNG", "gtools",
                   "glmnet", "pROC", "viridis", "kernlab", "randomForest"))

# Ensure BiocManager is installed
if (!requireNamespace("BiocManager", quietly = TRUE)) {
  install.packages("BiocManager")
}

# Install BioQC (a Bioconductor package)
BiocManager::install("BioQC")

# Optionally install additional packages
install.packages(c("testthat", "roxygen2", "predomics", "mcpredomics"))



# Introduction

The mcpredomics package is designed to streamline the development, evaluation, and visualization of machine learning models in a multiclass context. It provides a comprehensive suite of functions to assist users in every stage of the modeling process, from visualizing model characteristics to assessing their performance and managing associated populations. This documentation serves as a guide to introduce the package’s functionality and provide users with the tools they need for effective implementation.

## Visualization Functions

The package includes a variety of general-purpose visualization functions to explore models, their performance metrics, and the underlying populations. These functions allow users to gain valuable insights into their data and models with ease.
The implemented functions include:
printy_mc
printModel_mc
printPopulation_mc
printModelCollection_mc
printExperiment_mc
printClassifier_mc
plotAUC_mc
plotPrevalence_mc
plotAbundanceByClass_mc
mergeMeltImportanceCV_mc
plotFeatureModelCoeffs_mc
makeFeatureAnnot_mc
plotModel_mc
To learn more about the functionality of each visualization tool, use the ?? command followed by the function name. For example, ??printy_mc provides detailed information about the printy_mc function.

## Binary Sub-Model Evaluation

To support the evaluation of binary sub-models, we have implemented several global functions that provide insights into accuracy, AUC, regression metrics, feature importance, and more. These tools are crucial for assessing the quality and relevance of individual sub-models within the multiclass framework.
The following functions are available:
getSign_mc,
evaluateAccuracy_mc,
evaluateAUC_mc,
evaluateAdditionnalMetrics_mc,
evaluateModelRegression_mc,
evaluateFit_mc,
evaluateModel_mc,
evaluatePopulation_mc,
listOfSparseVecToListOfModels_mc,
evaluateFeatureImportanceInPopulation_mc,
generate_combinations_with_factors.
For a detailed description of any of these functions, you can use the ?? command. For instance, ??evaluateAccuracy_mc provides a comprehensive overview of the evaluateAccuracy_mc function.

## Multiclass Predictions and Aggregation Methods

The package supports multiclass modeling using one-versus-all (OVA) and one-versus-one (OVO) approaches. Additionally, it provides multiple methods for aggregating predictions from these approaches, as well as tools for evaluating the aggregated models.
The available functions are:
Prediction: predictModel_ova, predictModel_ovo,
Aggregation: voting, Predomics_aggregation_ovo, weighted, Predomics_aggregation_ova, maximization, ranking
Evaluation: evaluateModel_aggregation, evaluateModels_aggregation.
Each function serves a unique role, whether it involves generating predictions, combining results, or assessing model performance. For more details, use the ?? command, such as ??predictModel_ova for the predictModel_ova function.

## Multiclass Algorithm Implementations

We have extended the terBeam and terga1 algorithms to handle multiclass problems, introducing a set of specialized functions for model generation, feature combination management, and evolution optimization. These tools are designed to create scalable and efficient multiclass models.
The implemented functions include:
terBeam_mc, terBeam_fit_mc
generateAllSingleFeatureModel_mc, generateAllCombinations_mc
countEachFeatureApperance_mc, getFeatures2Keep_mc
terga1_mc, terga1_mc_fit
evolve_mc
You can explore the purpose and functionality of each function using the ?? command. For example, ??terBeam_fit_mc will provide a detailed explanation of the terBeam_fit_mc function.

## Core Functionalities

To ensure flexibility and efficiency in training, testing, and validating multiclass models, we have implemented the following core functions:
fit_mc: Fits models to the provided dataset.
runClassifier_mc: Executes the classifier to generate predictions.
runCrossval_mc: Runs cross-validation to evaluate model performance.
These foundational functions make it easier to integrate multiclass workflows into your projects. For detailed information on any of these functions, use the ?? command, such as ??runClassifier_mc for the runClassifier_mc function.



# Experimental Phase

## Loading libraries

```{r warning=FALSE}
# Package multi class predomics
library(mcpredomics)
# Package predomics
library(predomics)
# Visualization library for creating complex plots
library(ggplot2) 
# Arranges multiple ggplot objects on a single page
library(gridExtra) 
# ROC curve analysis and AUC calculation
library(pROC) 
# Reshaping and melting data frames
library(reshape2) 
# Implementation of the Random Forest algorithm for classification and regression
library(randomForest) 
# Comprehensive library for classification and regression training
library(caret) 
# Various R programming tools and functions, including data manipulation
library(gtools) 
# Adding statistical comparisons and publication-ready visualizations
library(ggpubr) 
# Data manipulation and transformation (part of the tidyverse)
library(dplyr) 
# Tidying messy data by gathering and spreading
library(tidyr) 
# Enhanced data frames with row names as a column (tibble format)
library(tibble) 
# Dynamic report generation and displaying results in tables
library(knitr) 
# Creating aesthetically pleasing and customizable HTML tables
library(kableExtra) 
# Interactive tables for data visualization and exploration
library(DT) 
# Functions for statistical learning, including SVM and Naive Bayes
library(e1071) 
# Lasso and ridge regression via generalized linear models
library(glmnet) 
# Reading data from files (including CSV and text files)
library(readr) 
# String manipulation and regular expression functions
library(stringr) 

```


## Dataset

We experimented with three datasets. The first one was drawn from the MetaCardis project and consists of four enterotype groups: **Bacteroidetes1**, **Bacteroidetes2**, **Prevotella**, and **Ruminococcus**. To do this, we created an initial balanced dataset by randomly extracting the same number of samples from each group. Additionally, we used the original unbalanced dataset. 

Next, we experimented with two other datasets: 
1. A three-class dataset (**Adenoma**, **Colorectal Cancer**, **Control**) from the study by **Qiang Feng et al.**
2. A three-class dataset (**T2D**, **IGT**, and **Control**) from the study by **Fredrik H. Karlsson**.


### Balance the dataset Metacardis.

```{r}
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]

# Filter null values
X_general <- mc.input$X[, colSums(mc.input$X) != 0]
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE)

set.seed(42)
y = as.vector(yvec)
X = X_general

# Determine the number of samples per class
nombre_echantillons_par_classe <- min(table(y))

# Function to balance classes and maintain order
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe, seed = 123) {
  classes <- unique(y)
  indices_equilibres <- integer(0)
  
  for (classe in classes) {
    indices_classe <- which(y == classe)
    set.seed(seed)
    indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
  }

  # Sort balanced indices to maintain original order
  indices_equilibres <- sort(indices_equilibres)
  return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}

# Get balanced data
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X

# Check distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)

# Split balanced data into 80% for training and 20% for testing
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[, indices_division, drop = FALSE]
X.test <- X_equilibre[, -indices_division, drop = FALSE]

table(y)
dim(X)

```


### Original unbalanced dataset Metacardis

```{r}
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]

# Filter null values
X_general <- mc.input$X[, colSums(mc.input$X) != 0]
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE)

set.seed(42)
y = as.vector(yvec)
X = X_general
# Check distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y, p = 0.8, list = FALSE)

# Split balanced data into 80% for training and 20% for testing
y <- as.vector(y[indices_division])
y.test <- as.vector(y[-indices_division])
X <- X[, indices_division, drop = FALSE]
X.test <- X[, -indices_division, drop = FALSE]

table(y)
dim(X)
```

Load Other Datasets

```{r}
# Load the file
load("../data/predomics.inputs.ExperimentHubMulticlass.Rda")
lapply(predomics.inputs, function(x){summary(colSums(x[["X"]]))})
```


### Dataset Colorectal cancer


```{r}
# Extract data
data1 <- predomics.inputs$FengQ_2015
df <- data1$y.df
y <- df$study_condition
X <- data1$X
table(y)
dim(X)

```
### T2D

```{r}
data1 <- predomics.inputs$KarlssonFH_2013
df <- data1$y.df
y <- df$study_condition
X <- data1$X
table(y)
dim(X)
```



## Heuristics

Heuristics: We have integrated the possibility of calling multiclass functions while incorporating two heuristics from Predomics: TerGa1, based on genetic algorithms, and TerBeam, which performs beam search. This led to the development of the multiclass versions TerGa1 Multiclass and TerBeam Multiclass, applied in our package.  

### terbeam_mc

```{r}
clf <- terBeam_mc(sparsity = c(10), 
                                   max.nb.features = 1000,
                                   seed = 1,
                                   nCores = 1,
                                   evalToFit = "accuracy_",
                                   objective = "auc",
                                   experiment.id = "terBeam_mc",
                                   experiment.save = "nothing")
  
                      
  printy(clf)  
```


### Terga1_mc


```{r}
clf <- terga1_mc(nCores = 1,
              seed = 1,
              plot = TRUE
)
printy(clf)
```



## Running experiment

Launching the experiment using the TerBeam_MC heuristic, with the Predomics Aggregation method "One Versus All", following the One Versus All approach. This experiment is launched by setting the parameter `constrained` to `false`, meaning that each binary submodel will use its own variables for model construction.



```{r running experiment, echo=TRUE, fig.width=5, warning=FALSE}
runit = TRUE
if(runit)
{
terbeam_maximization_CRC_unconstrained_no_balance <- fit_mc(X = X, y = y, clf = clf,approch="ova", cross.validate = TRUE,aggregation_ = "maximization", nfolds= 10, constrained = FALSE); 
save(terbeam_maximization_CRC_unconstrained_no_balance, clf, file ="terbeam_maximization_CRC_unconstrained_no_balance.rda", compression_level = 9)
}


```




## Exploring the results

```{r load results, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

  load("res_clf_mc.rda")

```


```{r warning=FALSE}
res_clf.dig <- digest(obj = resclf2 , penalty = 0.75/100, plot = TRUE)
```





### Regenerate object Clf 

```{r}
clf <- regenerate_clf(clf, X, y, approch = "ova")
```


### Family of Best Models (FBM)

A family of best models is defined as the set of models returned by the predomics algorithm, whose accuracy is within a given window of the best model's accuracy. This window is defined by computing a significance threshold assuming that accuracy follows a binomial distribution (p<0.05). 


```{r}
# Convert the model collection into a population of models scrambled by model size
pop <- modelCollectionToPopulation(terbeam_Majority_Voting_with_Tie_Breaking_metacardis_unconstrained_balance$classifier$models)
printy_mc(pop)
# Function to create boxplot for a given data frame
create_boxplot <- function(data, title) {
  # Melt the dataframe for ggplot
  data.melt <- melt(data, id.vars = c("accuracy_", "eval.sparsity"))
  
  # Create ggplot
  plot <- ggplot(data = data.melt, aes(y = accuracy_, x = eval.sparsity)) +
    geom_boxplot(notch = FALSE, outlier.shape = NA, position = position_dodge(width = 0.9), alpha = 0.3) +
    geom_point(aes(color = eval.sparsity), position = position_jitterdodge(dodge.width = 0.9), size = 1, alpha = 0.5) +
    ylim(c(0, 1)) +
    xlab("Model Parsimony") +
    ggtitle(title) +
    theme_bw() +
    theme(legend.position = "bottom", legend.direction = "horizontal") +
    guides(colour = "none")
  
  return(plot)
}

# Convert the population to a data frame
pop.df <- populationToDataFrame_mc(pop)

# Plotting for the original population (single figure)
pop.dff <- as.data.frame(pop.df[[1]])  # Convert the first submodel to a data frame
g.before <- create_boxplot(pop.dff, title = "Original Population")
#print(g.before)  # Display the plot

# Select the best population models
fbm <- selectBestPopulation(pop)
#printy_mc(fbm)

# Convert the best population models to a data frame
fbm.df <- populationToDataFrame_mc(fbm)

# Plotting for the selected best models (single figure)
fbm.dff <- as.data.frame(fbm.df[[1]])  # Convert the first submodel to a data frame
g.after <- create_boxplot(fbm.dff, title = "FBM")
print(g.after)  # Display the plot
```

###Analyzing the FBM

The analysis of the family of best models (FBM) can be very insightful for discovering the most important variables in the prediction process. Let's start by examining the usage of variables in the FBM models. For this, we run the ?makeFeatureAnnot_mc function to obtain the feature distribution in the model matrix, which is a list of pop.noz elements. The models in the FBM are ranked by accuracy, and this order will be preserved in the coefficient dataframe. There are 20 features found in the FBM models. The figure below shows that some of these features are very prevalent in the FBM, making them likely important. The abundance and prevalence distribution of these features can be explored with ?plotAbundanceByClass_mc and ?plotPrevalence_mc, respectively. Gray stars on the right side of the graphics indicate significant differences between the prediction groups.


```{r warning=FALSE, paged.print=FALSE, message=FALSE,fig.height=9, fig.width=25}
fa <- makeFeatureAnnot_mc(pop = fbm, 
                       X = X, 
                       y = y, 
                       clf = clf,
                       approch = "ovo")
plot_distribution <- plotFeatureModelCoeffs_mc(feat.model.coeffs = fa, y, approch = "ovo")
plots_abundance <- plotAbundanceByClass_mc(features = fa, X, y, approch = "ovo")
plots_prevalence <- plotPrevalence_mc(features = fa, X = X, y = y, approch = "ovo")
```


### Feature importance

Finally, we can explore the feature importance using mean decrease accuracy information computed during the cross-validation process. It is worth noting that the order of variables based on the prevalence in FBM, is concordant with the order of mean decrease accuracy (i.e. importance) in the cross-validation. But when we look at the variables based on their importance indipendetly on the empirical learning FBM, the picture changes somewhow. There are some important variables that were not selected in the FBM of the whole dataset. It is thus important to explore the cross validation importance.

```{r warning=FALSE, paged.print=FALSE, message=FALSE,fig.height=9, fig.width=25}
  feat1.import <- mergeMeltImportanceCV_mc(list.results = list(terBeam = terbeam_Majority_Voting_with_Tie_Breaking_metacardis_unconstrained_balance ),
                                        filter.cv.prev = 0,
                                        min.kfold.nb = FALSE,
                                        learner.grep.pattern = "*",
                                        #nb.top.features = 50,
                                        feature.selection = fa,
                                        scaled.importance = TRUE,
                                        make.plot = TRUE,
                                        cv.prevalence = FALSE)


  feat2.import <- mergeMeltImportanceCV_mc(list.results = list(terBeam = terbeam_Majority_Voting_with_Tie_Breaking_metacardis_unconstrained_balance),
                                        filter.cv.prev = 0,
                                        min.kfold.nb = FALSE,
                                        learner.grep.pattern = "*",
                                        nb.top.features = 148,
                                        #feature.selection = rownames(fa$pop.noz),
                                        scaled.importance = TRUE,
                                        make.plot = TRUE,
                                        cv.prevalence = FALSE)
```



### Visualizing the best model

The best-learned model can be visualized simply by printing it along with a summary of information on performance and model size. Furthermore, we can further explore the model by visualizing it using the barcode plot ?plotModel_mc. Finally, the importance of each feature can also be displayed using the same function, as illustrated in the figure below.

```{r best model, fig.width=12, warning=FALSE}
# get the best model
best.model = fbm[[1]]
# Visualize the model information (if needed)
printy_mc(best.model)

# Generate the plots
plots1 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ovo")
plots2 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE, approch = "ovo")

# Open a PDF device to save the grid of plots
pdf("model_visualization_plots2.pdf", width = 15, height = 10)  # Adjust width and height as needed

# Extract the plots from each list and pass them to `grid.arrange`
grid.arrange(grobs = c(plots1, plots2), ncol = 6)

# Close the PDF device
dev.off()

# Confirmation of the save process
cat("The plots have been saved under the name 'model_visualization_plots2.pdf'.\n")

```
### Visualizing model performance AUC

```{r roc analyses, fig.height=7, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
tmp <- plotAUC_mc(scores = best.model$score_, y = y, percent = TRUE, approch = "ovo"); rm(tmp)


```










##########################################################################################################
############################################################################################################################






















## ANNEXES
### Analysys results predomics and sota


The data in the **analysis_table_final1** dataframe comes from various experiments conducted using **Predomics** and state-of-the-art methods. For the **Predomics** approach, two heuristics were used: **Terga1** and **Terbeam**. For each heuristic, 12 experiments were conducted, each involving 6 aggregation methods and 2 variable selection methods. The variables were selected in two ways: (i) a constrained selection, where the same variables are used for all models, and (ii) a model-specific selection, called "unconstrained".

Regarding the aggregation methods, two new approaches were developed: **Predomics_Aggrégation_ova**, adapted to the "one versus all" approach, and **Predomics_Aggrégation_ovo**, adapted to the "one versus one" approach. In parallel, we explored four state-of-the-art methods for each approach ("one versus all" and "one versus one"): **Maximization** and **Ranking** for the "one versus all" approach, and **Voting** and **Weighted** for the "one versus one" approach.

In addition to the **Predomics** heuristics, we also evaluated seven state-of-the-art methods: **Random Forest**, **SVM**, **Logistic Regression**, **KNN**, **Artificial Neural Networks**, **Gradient Boosting**, and **Decision Tree**.

All experiments were conducted using 10-fold cross-validation. The **analysis_table_final1** dataframe contains several columns, including **Fold**, which identifies the folds used, as well as columns for different performance metrics: **Accuracy.empirique**, **Accuracy.generalization**, **Precision.empirique**, **Precision.generalization**, **Recall.empirique**, **Recall.generalization**, **F1.empirique**, **F1.generalization**, and **Methods**, the latter indicating the method used for each evaluation.

To construct this dataframe, the results from the different experiments were merged into a single structure using the **bind_rows()** function, making it easier to analyze and compare the results obtained with the various approaches and methods used.

```{r warning=FALSE, fig.height=10, fig.width=12, message=FALSE, warning=FALSE, paged.print=FALSE}

# Charger les bibliothèques nécessaires
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)  # Pour str_starts

# Imputer les valeurs manquantes avec la moyenne
analysis_table_final <- analysis_table_final2_no_balance %>% 
  mutate(across(everything(), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Convertir en format long
df <- analysis_table_final %>% 
  pivot_longer(
    cols = -c(Fold, Methods),
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>% 
  mutate(
    Partition = recode(Partition,
                       'empirique' = 'train',
                       'generalization' = 'test'
    ),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Predomics Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Predomics Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Predomics Terga1 constrained",
      TRUE ~ "SOTA"  # Pour toutes les autres méthodes
    )
  )

# Agréger les données pour le résumé
df_summary <- df %>% 
  group_by(Methods, Partition, Metric) %>% 
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  ) %>% 
  left_join(df %>% select(Methods, Group) %>% distinct(), by = "Methods")  # Assurez-vous que Group est présent

# Vérifiez les groupes présents (optionnel pour le débogage)
#print(unique(df_summary$Group))

# Tracez le graphique
df_summary %>% 
  ggplot(aes(x = Methods, y = mean_value, color = Partition)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +  # Taille des points
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value),
                position = position_dodge(width = 0.5), width = 0.2) +
  facet_grid(Metric ~ Group, scales = "free") +  # Facettage par Metric et Group
  ylab("Value") +
  xlab("Methods") +  # Réglage des labels
  theme_bw() +
  scale_colour_manual(values = c("seagreen", "firebrick3")) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )


```



```{r}
# Charger les bibliothèques nécessaires
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)  # Pour str_starts

# Imputer les valeurs manquantes avec la moyenne
analysis_table_final <- analysis_table_final2 %>% 
  mutate(across(everything(), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Convertir en format long
df <- analysis_table_final %>% 
  pivot_longer(
    cols = -c(Fold, Methods),
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>% 
  mutate(
    Partition = recode(Partition,
                       'empirique' = 'train',
                       'generalization' = 'test'
    ),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Predomics Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Predomics Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Predomics Terga1 constrained",
      TRUE ~ "SOTA"  # Pour toutes les autres méthodes
    )
  )

# Agréger les données pour le résumé
df_summary <- df %>% 
  group_by(Methods, Partition, Metric) %>% 
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  ) %>% 
  left_join(df %>% select(Methods, Group) %>% distinct(), by = "Methods")  # Assurez-vous que Group est présent

# Définir le fichier PDF de sortie
output_pdf <- "analysis_plot2.pdf"  # Nom du fichier PDF
pdf(file = output_pdf, width = 12, height = 10)  # Dimensions du graphique

# Générer le graphique
df_summary %>% 
  ggplot(aes(x = Methods, y = mean_value, color = Partition)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +  # Taille des points
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value),
                position = position_dodge(width = 0.5), width = 0.2) +
  facet_grid(Metric ~ Group, scales = "free") +  # Facettage par Metric et Group
  ylab("Value") +
  xlab("Methods") +  # Réglage des labels
  theme_bw() +
  scale_colour_manual(values = c("seagreen", "firebrick3")) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )

# Fermer le fichier PDF
dev.off()

# Message de confirmation
cat("Le fichier PDF a été généré : ", output_pdf, "\n")

```








```{r warning=FALSE}
str(analysis_table_final2)

# Aperçu des premières lignes
analysis_table_final2

```


```{r}
# Charger les bibliothèques nécessaires
library(dplyr)

# Regrouper par méthode et calculer les moyennes et écarts-types
summary_metrics <- analysis_table_final2 %>%
  group_by(Methods) %>%
  summarize(
    accuracy_empirical_mean = mean(Accuracy.empirique, na.rm = TRUE),
    accuracy_empirical_sd = sd(Accuracy.empirique, na.rm = TRUE),
    accuracy_generalization_mean = mean(Accuracy.generalization, na.rm = TRUE),
    accuracy_generalization_sd = sd(Accuracy.generalization, na.rm = TRUE),
    
    precision_empirical_mean = mean(Precision.empirique, na.rm = TRUE),
    precision_empirical_sd = sd(Precision.empirique, na.rm = TRUE),
    precision_generalization_mean = mean(Precision.generalization, na.rm = TRUE),
    precision_generalization_sd = sd(Precision.generalization, na.rm = TRUE),
    
    recall_empirical_mean = mean(Recall.empirique, na.rm = TRUE),
    recall_empirical_sd = sd(Recall.empirique, na.rm = TRUE),
    recall_generalization_mean = mean(Recall.generalization, na.rm = TRUE),
    recall_generalization_sd = sd(Recall.generalization, na.rm = TRUE),
    
    f1_empirical_mean = mean(F1.empirique, na.rm = TRUE),
    f1_empirical_sd = sd(F1.empirique, na.rm = TRUE),
    f1_generalization_mean = mean(F1.generalization, na.rm = TRUE),
    f1_generalization_sd = sd(F1.generalization, na.rm = TRUE)
  )

# Afficher les résultats
print(summary_metrics)
```



```{r warning=FALSE}
# Charger les bibliothèques nécessaires
library(dplyr)
library(tidyr)

# Vérifier les noms de colonnes du dataframe pour identifier la structure
colnames(df)

# Liste des métriques à analyser
metrics <- c("Accuracy.empirique", "Accuracy.generalization", 
             "Precision.empirique", "Precision.generalization", 
             "Recall.empirique", "Recall.generalization", 
             "F1.empirique", "F1.generalization")

# Vérification de la présence des colonnes métriques dans le dataframe
metrics_in_data <- metrics[metrics %in% colnames(df)]

# Si toutes les métriques nécessaires sont présentes, appliquer l'analyse
if (length(metrics_in_data) > 0) {
  
  # Calcul des moyennes
  method_stats_mean <- df %>%
    group_by(Methods) %>%
    summarise(across(all_of(metrics_in_data), ~mean(.), .names = "{.col}_mean"), .groups = 'drop')
  
  # Calcul des écarts-types
  method_stats_sd <- df %>%
    group_by(Methods) %>%
    summarise(across(all_of(metrics_in_data), ~sd(.), .names = "{.col}_sd"), .groups = 'drop')
  
  # Combinaison des moyennes et écarts-types dans un seul tableau
  method_stats <- left_join(method_stats_mean, method_stats_sd, by = "Methods")
  
  # Afficher les statistiques calculées pour chaque méthode et chaque métrique
  print("Statistiques des méthodes :")
  print(method_stats)
  
  # Transformation des données pour une structure plus facile à analyser
  method_stats_long <- method_stats %>%
    pivot_longer(cols = starts_with("Accuracy") | starts_with("Precision") |
                   starts_with("Recall") | starts_with("F1"),
                 names_to = c("Metric", "Statistic"),
                 names_sep = "_") %>%
    pivot_wider(names_from = Statistic, values_from = value)
  
  # Afficher les statistiques sous forme longue
  print("Données transformées :")
  print(method_stats_long)
  
  # Choisir la meilleure méthode basée sur la métrique 'Accuracy.generalization_mean'
  if ("Accuracy.generalization_mean" %in% colnames(method_stats)) {
    best_method <- method_stats %>%
      arrange(desc(Accuracy.generalization_mean)) %>%
      slice(1) %>%
      select(Methods, Accuracy.generalization_mean)
    
    print(paste("La meilleure méthode est :", best_method$Methods, 
                "avec Accuracy.generalization_mean =", best_method$Accuracy.generalization_mean))
  } else {
    print("La colonne 'Accuracy.generalization_mean' est manquante dans les données.")
  }
  
  # Effectuer des tests ANOVA pour chaque métrique présente
  for (metric in metrics_in_data) {
    if (metric %in% colnames(df)) {
      metric_data <- df %>%
        select(Methods, all_of(metric)) %>%
        drop_na()  # Supprimer les NA si présents
      
      # Appliquer l'ANOVA pour chaque métrique
      anova_result <- aov(as.formula(paste(metric, "~ Methods")), data = metric_data)
      
      # Résultats de l'ANOVA
      print(paste("Résultats de l'ANOVA pour", metric, ":"))
      print(summary(anova_result))
    }
  }
  
} else {
  # Si certaines colonnes sont manquantes, les afficher
  cat("Les colonnes suivantes sont manquantes dans les données : \n")
  print(setdiff(metrics, metrics_in_data))
}


```







```{r warning=FALSE, fig.height=14, fig.width=13, message=FALSE, warning=FALSE, paged.print=FALSE}
# Chargement des librairies nécessaires
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)

# Fusionner les quatre dataframes en ajoutant une colonne pour différencier les datasets
analysis_table_no_balance <- analysis_table_final_complet_no_balance %>%
  mutate(Dataset = "MetaCardis enterotype non-balanced")

analysis_table_balance <- analysis_table_final_complet_balance %>%
  mutate(Dataset = "MetaCardis enterotype balanced")

analysis_table_no_balance_2 <- analysis_table_final_complet_no_balance_2 %>%
  mutate(Dataset = "CRC, Adenoma and Control")

analysis_table_no_balance_3 <- analysis_table_final_complet_no_balance_3 %>%
  mutate(Dataset = "T2D,IGT and Control ")

# Combiner les quatre tables
df_combined <- bind_rows(analysis_table_no_balance, 
                         analysis_table_balance, 
                         analysis_table_no_balance_2, 
                         analysis_table_no_balance_3)

# Imputation des valeurs manquantes pour les colonnes numériques
df_combined <- df_combined %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Renommer "Features" en "k"
df_combined <- df_combined %>%
  rename(k = Features)

# Convertir en format long
df_long <- df_combined %>%
  pivot_longer(
    cols = c("Accuracy.empirique", "Accuracy.generalization"),  # Se limiter aux colonnes de précision
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>%
  mutate(
    Partition = recode(Partition, 'empirique' = 'train', 'generalization' = 'test'),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Predomics Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Predomics Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Predomics Terga1 constrained",
      TRUE ~ "SOTA"
    ),
    Subgroup = case_when(
      str_starts(Methods, "terbeam") ~ "Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Terga1 constrained",
      TRUE ~ "SOTA"
    )
  )

# Regrouper "Predomics Terbeam unconstrained" et "Predomics Terga1 unconstrained"
df_long$Group <- recode(df_long$Group,
                        "Predomics Terbeam unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terga1 unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terbeam constrained" = "Predomics Terbeam & Terga1 constrained", 
                        "Predomics Terga1 constrained" = "Predomics Terbeam & Terga1 constrained")

# Filtrer pour ne garder que la partition "test" et la métrique "Accuracy.generalization"
df_accuracy <- df_long %>% 
  filter(Metric == "Accuracy" & Partition == "test")

# Agréger les données pour le résumé
df_summary <- df_accuracy %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  )

# Supprimer les lignes avec des valeurs manquantes ou vides dans `Metric` ou `Group`
df_summary <- df_summary %>%
  filter(!is.na(Metric) & !is.na(Group) & Metric != "" & Group != "")

# Suppression des préfixes "terbeam", "_terbeam", "terga1", "_terga1" dans l'affichage des méthodes
df_summary <- df_summary %>%
  mutate(Methods = str_remove(Methods, "^_?terbeam_?|^_?terga1_?"))

# Vérification des résultats
print(unique(df_summary$Group))

# Définir une palette de couleurs personnalisée
custom_colors <- c(
  "Predomics Terbeam & Terga1 unconstrained" = "green", 
  "Predomics Terbeam & Terga1 constrained" = "blue", 
  "SOTA" = "red", 
  "Terbeam unconstrained" = "darkgreen", 
  "Terga1 unconstrained" = "darkblue", 
  "Terbeam constrained" = "lightgreen", 
  "Terga1 constrained" = "lightblue"
)

# Définir une palette de formes personnalisée
custom_shapes <- c(
  "Predomics Terbeam & Terga1 unconstrained" = 16, 
  "Predomics Terbeam & Terga1 constrained" = 4,   
  "SOTA" = 19,  
  "Terbeam unconstrained" = 17,  
  "Terga1 unconstrained" = 18,  
  "Terbeam constrained" = 15,  
  "Terga1 constrained" = 8
)

# Tracez le graphique avec faceting pour "Accuracy.generalization" uniquement
df_summary %>%
  ggplot(aes(x = Methods, y = mean_value, color = Subgroup, shape = Subgroup)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value), 
                position = position_dodge(width = 0.5), width = 0.2) +
  facet_grid(Dataset ~ Group, scales = "free") +  # Faceting par Dataset et Group
  ylab("Accuracy") +  # Changez "Value" en "Accuracy" pour plus de clarté
  xlab("Methods") + 
  theme_bw() + 
  scale_colour_manual(values = custom_colors) +  # Appliquer les couleurs personnalisées
  scale_shape_manual(values = custom_shapes) +   # Appliquer les formes personnalisées
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )


```






```{r}
# Chargement des librairies nécessaires
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)

# Fusionner les quatre dataframes en ajoutant une colonne pour différencier les datasets
analysis_table_no_balance <- analysis_table_final_complet_no_balance %>%
  mutate(Dataset = "Enterotype_nobalanced")

analysis_table_balance <- analysis_table_final_complet_balance %>%
  mutate(Dataset = "Enterotype_balanced")

analysis_table_no_balance_2 <- analysis_table_final_complet_no_balance_2 %>%
  mutate(Dataset = "CRC")

analysis_table_no_balance_3 <- analysis_table_final_complet_no_balance_3 %>%
  mutate(Dataset = "T2D ")

# Combiner les quatre tables
df_combined <- bind_rows(analysis_table_no_balance, 
                         analysis_table_balance, 
                         analysis_table_no_balance_2, 
                         analysis_table_no_balance_3)

# Imputation des valeurs manquantes pour les colonnes numériques
df_combined <- df_combined %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Renommer "Features" en "k"
df_combined <- df_combined %>%
  rename(k = Features)

# Convertir en format long
df_long <- df_combined %>%
  pivot_longer(
    cols = c("Accuracy.empirique", "Accuracy.generalization"),  # Se limiter aux colonnes de précision
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>%
  mutate(
    Partition = recode(Partition, 'empirique' = 'train', 'generalization' = 'test'),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Predomics Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Predomics Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Predomics Terga1 constrained",
      TRUE ~ "SOTA"
    ),
    Subgroup = case_when(
      str_starts(Methods, "terbeam") ~ "Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Terga1 constrained",
      TRUE ~ "SOTA"
    )
  )

# Regrouper "Predomics Terbeam unconstrained" et "Predomics Terga1 unconstrained"
df_long$Group <- recode(df_long$Group,
                        "Predomics Terbeam unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terga1 unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terbeam constrained" = "Predomics Terbeam & Terga1 constrained", 
                        "Predomics Terga1 constrained" = "Predomics Terbeam & Terga1 constrained")

# Filtrer pour ne garder que la partition "test" et la métrique "Accuracy.generalization"
df_accuracy <- df_long %>% 
  filter(Metric == "Accuracy" & Partition == "test")

# Agréger les données pour le résumé
df_summary <- df_accuracy %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  )

# Supprimer les lignes avec des valeurs manquantes ou vides dans Metric ou Group
df_summary <- df_summary %>%
  filter(!is.na(Metric) & !is.na(Group) & Metric != "" & Group != "")

# Suppression des préfixes "terbeam", "_terbeam", "terga1", "_terga1" et des suffixes "_no_balance", "_balance" dans l'affichage des méthodes
df_summary <- df_summary %>%
  mutate(Methods = str_remove_all(Methods,"^(terbeam_|_terbeam_|terga1_|_terga1_)|(_no_balance|_balance)$"))


# Vérification des résultats
print(unique(df_summary$Group))

# Définir une palette de couleurs personnalisée
custom_colors <- c(
  "Predomics Terbeam & Terga1 unconstrained" = "green", 
  "Predomics Terbeam & Terga1 constrained" = "blue", 
  "SOTA" = "red", 
  "Terbeam unconstrained" = "darkgreen", 
  "Terga1 unconstrained" = "darkblue", 
  "Terbeam constrained" = "lightgreen", 
  "Terga1 constrained" = "lightblue"
)

# Définir une palette de formes personnalisée
custom_shapes <- c(
  "Predomics Terbeam & Terga1 unconstrained" = 16, 
  "Predomics Terbeam & Terga1 constrained" = 4,   
  "SOTA" = 19,  
  "Terbeam unconstrained" = 17,   
  "Terga1 unconstrained" = 18,   
  "Terbeam constrained" = 15,   
  "Terga1 constrained" = 8
)

# Créer et enregistrer le graphique en format PDF avec disposition paysage
pdf("accuracy_plot.pdf", width = 11, height = 10)  # Format paysage A4

df_summary %>%
  ggplot(aes(x = Methods, y = mean_value, color = Subgroup, shape = Subgroup)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value), 
                position = position_dodge(width = 0.5), width = 0.2) +
  facet_grid(Dataset ~ Group, scales = "free") +  # Faceting par Dataset et Group
  ylab("Accuracy") +  # Changez "Value" en "Accuracy" pour plus de clarté
  xlab("Methods") + 
  theme_bw() + 
  scale_colour_manual(values = custom_colors) +  # Appliquer les couleurs personnalisées
  scale_shape_manual(values = custom_shapes) +   # Appliquer les formes personnalisées
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )

# Fermer le fichier PDF
dev.off()

```




```{r}
# Charger la bibliothèque nécessaire
library(dplyr)
library(stringr)

# Fonction pour remplacer les valeurs dans la colonne Methods
replace_methods <- function(df) {
  df %>%
    mutate(Methods = str_replace_all(Methods, 
      c("_terga1_Predomics_aggregation_ovo_constrained_balance" = "_terga1_predomics_aggregation_ovo_constrained_balance",
        "_terga1_Predomics_aggregation_ovo_unconstrained_balance" = "_terga1_predomics_aggregation_ovo_unconstrained_balance",
        "_terga1_Predomics_aggregation_ova_constrained_balance" = "_terga1_predomics_aggregation_ova_constrained_balance",
        "_terga1_Predomics_aggregation_ova_unconstrained_balance" = "_terga1_predomics_aggregation_ova_unconstrained_balance"))) %>%
    # Remplacer 'P' majuscule par 'p' dans 'Predomics'
    mutate(Methods = str_replace_all(Methods, "Predomics", "predomics"))
}

# Appliquer la fonction à chaque dataframe
analysis_table_final_complet_no_balance <- replace_methods(analysis_table_final_complet_no_balance)
analysis_table_final_complet_balance <- replace_methods(analysis_table_final_complet_balance)
analysis_table_final_complet_no_balance_2 <- replace_methods(analysis_table_final_complet_no_balance_2)
analysis_table_final_complet_no_balance_3 <- replace_methods(analysis_table_final_complet_no_balance_3)

# Vérifier les valeurs uniques après modification
unique(analysis_table_final_complet_no_balance$Methods)
unique(analysis_table_final_complet_balance$Methods)
unique(analysis_table_final_complet_no_balance_2$Methods)
unique(analysis_table_final_complet_no_balance_3$Methods)


```







```{r}
# Chargement des librairies nécessaires
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)

# Fusionner les quatre dataframes en ajoutant une colonne pour différencier les datasets
analysis_table_no_balance <- analysis_table_final_complet_no_balance %>%
  mutate(Dataset = "Enterotype_nobalanced")

analysis_table_balance <- analysis_table_final_complet_balance %>%
  mutate(Dataset = "Enterotype_balanced")

analysis_table_no_balance_2 <- analysis_table_final_complet_no_balance_2 %>%
  mutate(Dataset = "CRC")

analysis_table_no_balance_3 <- analysis_table_final_complet_no_balance_3 %>%
  mutate(Dataset = "T2D ")

# Combiner les quatre tables
df_combined <- bind_rows(analysis_table_no_balance, 
                         analysis_table_balance, 
                         analysis_table_no_balance_2, 
                         analysis_table_no_balance_3)

# Imputation des valeurs manquantes pour les colonnes numériques
df_combined <- df_combined %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Renommer "Features" en "k"
df_combined <- df_combined %>%
  rename(k = Features)

# Convertir en format long
df_long <- df_combined %>%
  pivot_longer(
    cols = c("Accuracy.empirique", "Accuracy.generalization"),  # Se limiter aux colonnes de précision
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>%
  mutate(
    Partition = recode(Partition, 'empirique' = 'train', 'generalization' = 'test'),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Predomics Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Predomics Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Predomics Terga1 constrained",
      TRUE ~ "SOTA"
    ),
    Subgroup = case_when(
      str_starts(Methods, "terbeam") ~ "Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Terga1 constrained",
      TRUE ~ "SOTA"
    )
  )

# Regrouper "Predomics Terbeam unconstrained" et "Predomics Terga1 unconstrained"
df_long$Group <- recode(df_long$Group,
                        "Predomics Terbeam unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terga1 unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terbeam constrained" = "Predomics Terbeam & Terga1 constrained", 
                        "Predomics Terga1 constrained" = "Predomics Terbeam & Terga1 constrained")

# Filtrer pour ne garder que la partition "test" et la métrique "Accuracy.generalization"
df_accuracy <- df_long %>% 
  filter(Metric == "Accuracy" & Partition == "test")

# Agréger les données pour le résumé
df_summary <- df_accuracy %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  )

# Supprimer les lignes avec des valeurs manquantes ou vides dans Metric ou Group
df_summary <- df_summary %>%
  filter(!is.na(Metric) & !is.na(Group) & Metric != "" & Group != "")

# Suppression des préfixes et des suffixes dans l'affichage des méthodes
df_summary <- df_summary %>%
  mutate(Methods = str_remove_all(Methods, "^(terbeam_|_terbeam_|terga1_|_terga1_)|(_no_balance|_balance)$"))

# Supprimer les doublons de méthodes après la suppression des préfixes et des suffixes
df_summary <- df_summary %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(mean_value, na.rm = TRUE),
    sd_value = mean(sd_value, na.rm = TRUE),
    .groups = "drop"
  )

# Définir une palette de couleurs personnalisée
custom_colors <- c(
  "Predomics Terbeam & Terga1 unconstrained" = "green", 
  "Predomics Terbeam & Terga1 constrained" = "blue", 
  "SOTA" = "red", 
  "Terbeam unconstrained" = "darkgreen", 
  "Terga1 unconstrained" = "darkblue", 
  "Terbeam constrained" = "lightgreen", 
  "Terga1 constrained" = "lightblue"
)

# Définir une palette de formes personnalisée
custom_shapes <- c(
  "Predomics Terbeam & Terga1 unconstrained" = 16, 
  "Predomics Terbeam & Terga1 constrained" = 4,   
  "SOTA" = 19,   
  "Terbeam unconstrained" = 17,   
  "Terga1 unconstrained" = 18,   
  "Terbeam constrained" = 15,   
  "Terga1 constrained" = 8
)

# Créer et enregistrer le graphique en format PDF avec disposition paysage
pdf("accuracy_plot.pdf", width = 11, height = 10)

df_summary %>%
  ggplot(aes(x = Methods, y = mean_value, color = Subgroup, shape = Subgroup)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value), 
                position = position_dodge(width = 0.5), width = 0.2) +
  facet_grid(Dataset ~ Group, scales = "free") +
  ylab("Accuracy") +
  xlab("Methods") +
  theme_bw() +
  scale_colour_manual(values = custom_colors) +
  scale_shape_manual(values = custom_shapes) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )

# Fermer le fichier PDF
dev.off()

```







```{r}
# Chargement des librairies nécessaires
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)

# Fusionner les quatre dataframes en ajoutant une colonne pour différencier les datasets
analysis_table_no_balance <- analysis_table_final_complet_no_balance %>%
  mutate(Dataset = "Enterotype no_balanced")

analysis_table_balance <- analysis_table_final_complet_balance %>%
  mutate(Dataset = "Enterotype balanced")

analysis_table_no_balance_2 <- analysis_table_final_complet_no_balance_2 %>%
  mutate(Dataset = "CRC, Adenoma, Control")

analysis_table_no_balance_3 <- analysis_table_final_complet_no_balance_3 %>%
  mutate(Dataset = "T2D, IGT, Control ")

# Combiner les quatre tables
df_combined <- bind_rows(analysis_table_no_balance, 
                         analysis_table_balance, 
                         analysis_table_no_balance_2, 
                         analysis_table_no_balance_3)

# Imputation des valeurs manquantes pour les colonnes numériques
df_combined <- df_combined %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Renommer "Features" en "k"
df_combined <- df_combined %>%
  rename(k = Features)

# Convertir en format long
df_long <- df_combined %>%
  pivot_longer(
    cols = c("Accuracy.empirique", "Accuracy.generalization"),  # Se limiter aux colonnes de précision
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>%
  mutate(
    Partition = recode(Partition, 'empirique' = 'train', 'generalization' = 'test'),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Predomics Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Predomics Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Predomics Terga1 constrained",
      TRUE ~ "SOTA"
    ),
    Subgroup = case_when(
      str_starts(Methods, "terbeam") ~ "Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Terga1 constrained",
      TRUE ~ "SOTA"
    )
  )

# Regrouper "Predomics Terbeam unconstrained" et "Predomics Terga1 unconstrained"
df_long$Group <- recode(df_long$Group,
                        "Predomics Terbeam unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terga1 unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terbeam constrained" = "Predomics Terbeam & Terga1 constrained", 
                        "Predomics Terga1 constrained" = "Predomics Terbeam & Terga1 constrained")

# Filtrer pour ne garder que la partition "test" et la métrique "Accuracy.generalization"
df_accuracy <- df_long %>% 
  filter(Metric == "Accuracy" & Partition == "test")

# Agréger les données pour le résumé
df_summary <- df_accuracy %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  )

# Supprimer les lignes avec des valeurs manquantes ou vides dans Metric ou Group
df_summary <- df_summary %>%
  filter(!is.na(Metric) & !is.na(Group) & Metric != "" & Group != "")

# Suppression des préfixes et des suffixes dans l'affichage des méthodes
df_summary <- df_summary %>%
  mutate(Methods = str_remove_all(Methods, "^(terbeam_|_terbeam_|terga1_|_terga1_)|(_no_balance|_balance)$"))

# Supprimer les doublons de méthodes après la suppression des préfixes et des suffixes
df_summary <- df_summary %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(mean_value, na.rm = TRUE),
    sd_value = mean(sd_value, na.rm = TRUE),
    .groups = "drop"
  )

# Trouver la valeur maximale d'accuracy pour chaque dataset et group
df_summary <- df_summary %>%
  group_by(Dataset, Group) %>%
  mutate(max_accuracy = max(mean_value, na.rm = TRUE)) %>%
  ungroup()

# Ajouter une colonne pour marquer les points avec l'accuracy maximale
df_summary <- df_summary %>%
  mutate(is_max_accuracy = ifelse(mean_value == max_accuracy, TRUE, FALSE))

# Définir une palette de couleurs personnalisée
custom_colors <- c(
  "Predomics Terbeam & Terga1 unconstrained" = "green", 
  "Predomics Terbeam & Terga1 constrained" = "blue", 
  "SOTA" = "black", 
  "Terbeam unconstrained" = "darkgreen", 
  "Terga1 unconstrained" = "darkblue", 
  "Terbeam constrained" = "lightgreen", 
  "Terga1 constrained" = "lightblue"
)

# Définir une palette de formes personnalisée
custom_shapes <- c(
  "Predomics Terbeam & Terga1 unconstrained" = 16, 
  "Predomics Terbeam & Terga1 constrained" = 4,   
  "SOTA" = 19,   
  "Terbeam unconstrained" = 17,   
  "Terga1 unconstrained" = 18,   
  "Terbeam constrained" = 15,   
  "Terga1 constrained" = 8
)

# Créer et enregistrer le graphique en format PDF avec disposition paysage
pdf("accuracy_plot_with_stars.pdf", width = 11, height = 10)

df_summary %>%
  ggplot(aes(x = Methods, y = mean_value, color = Subgroup, shape = Subgroup)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value), 
                position = position_dodge(width = 0.5), width = 0.2) +
  geom_point(data = subset(df_summary, is_max_accuracy == TRUE), 
             aes(x = Methods, y = mean_value), 
             shape = 8, color = "red", size = 5) + # Ajout d'une étoile pour la valeur maximale
  facet_grid(Dataset ~ Group, scales = "free") +
  ylab("Accuracy.generalization") +
  xlab("Methods") +
  theme_bw() +
  scale_colour_manual(values = custom_colors) +
  scale_shape_manual(values = custom_shapes) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )

# Fermer le fichier PDF
dev.off()

```









```{r}
y_T2D <- y
X_T2D <- X


```



```{r}
# Sauvegarder la variable y dans un fichier CSV
write.csv(y_T2D, file = "y_T2D.csv", row.names = FALSE)

# Sauvegarder la variable X dans un fichier CSV
write.csv(X_T2D, file = "X_T2D.csv", row.names = FALSE)


```

```{r}
df_CRC <- data.frame(y = y_repeated, X)
```



```{r warning=FALSE, fig.height=10, fig.width=12, message=FALSE, warning=FALSE, paged.print=FALSE}
# Fusionner les quatre dataframes en ajoutant une colonne pour différencier les datasets
analysis_table_no_balance_ <-analysis_table_final_complet_no_balance_ %>%
  mutate(Dataset = "Enterotype no_balanced")

analysis_table_balance_ <- analysis_table_final_complet_balance_ %>%
  mutate(Dataset = "Enterotype balanced")

analysis_table_no_balance_2_ <- analysis_table_final_complet_no_balance_2_ %>%
  mutate(Dataset = "CRC, Adenoma, Control")

analysis_table_no_balance_3_ <- analysis_table_final_complet_no_balance_3_ %>%
  mutate(Dataset = "T2D, IGT, Control ")

# Combiner les quatre tables
df_combined <- bind_rows(analysis_table_no_balance_, 
                         analysis_table_balance_, 
                         analysis_table_no_balance_2_, 
                         analysis_table_no_balance_3_)

# Imputation des valeurs manquantes pour les colonnes numériques
df_combined <- df_combined %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Renommer "Features" en "k"
df_combined <- df_combined %>%
  rename(k = Features)

# Convertir en format long
df_long <- df_combined %>%
  pivot_longer(
    cols = c("Accuracy.empirique", "Accuracy.generalization"),  # Se limiter aux colonnes de précision
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>%
  mutate(
    Partition = recode(Partition, 'empirique' = 'train', 'generalization' = 'test'),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Predomics Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Predomics Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Predomics Terga1 constrained",
      TRUE ~ "SOTA"
    ),
    Subgroup = case_when(
      str_starts(Methods, "terbeam") ~ "Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Terga1 constrained",
      TRUE ~ "SOTA"
    )
  )

# Regrouper "Predomics Terbeam unconstrained" et "Predomics Terga1 unconstrained"
df_long$Group <- recode(df_long$Group,
                        "Predomics Terbeam unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terga1 unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terbeam constrained" = "Predomics Terbeam & Terga1 constrained", 
                        "Predomics Terga1 constrained" = "Predomics Terbeam & Terga1 constrained")

# Filtrer pour ne garder que la partition "test" et la métrique "Accuracy.generalization"
df_accuracy <- df_long %>% 
  filter(Metric == "Accuracy" & Partition == "test")

# Agréger les données pour le résumé
df_summary <- df_accuracy %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  )

# Supprimer les lignes avec des valeurs manquantes ou vides dans Metric ou Group
df_summary <- df_summary %>%
  filter(!is.na(Metric) & !is.na(Group) & Metric != "" & Group != "")

# Suppression des préfixes et des suffixes dans l'affichage des méthodes
df_summary <- df_summary %>%
  mutate(Methods = str_remove_all(Methods, "^(terbeam_|_terbeam_|terga1_|_terga1_)|(_no_balance|_balance)$"))

# Supprimer les doublons de méthodes après la suppression des préfixes et des suffixes
df_summary <- df_summary %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(mean_value, na.rm = TRUE),
    sd_value = mean(sd_value, na.rm = TRUE),
    .groups = "drop"
  )

# Trouver la valeur maximale d'accuracy pour chaque dataset et group
df_summary <- df_summary %>%
  group_by(Dataset, Group) %>%
  mutate(max_accuracy = max(mean_value, na.rm = TRUE)) %>%
  ungroup()

# Ajouter une colonne pour marquer les points avec l'accuracy maximale
df_summary <- df_summary %>%
  mutate(is_max_accuracy = ifelse(mean_value == max_accuracy, TRUE, FALSE))

# Définir une palette de couleurs personnalisée
custom_colors <- c(
  "Predomics Terbeam & Terga1 unconstrained" = "green", 
  "Predomics Terbeam & Terga1 constrained" = "blue", 
  "SOTA" = "black", 
  "Terbeam unconstrained" = "darkgreen", 
  "Terga1 unconstrained" = "darkblue", 
  "Terbeam constrained" = "lightgreen", 
  "Terga1 constrained" = "lightblue"
)

# Définir une palette de formes personnalisée
custom_shapes <- c(
  "Predomics Terbeam & Terga1 unconstrained" = 16, 
  "Predomics Terbeam & Terga1 constrained" = 4,   
  "SOTA" = 19,   
  "Terbeam unconstrained" = 17,   
  "Terga1 unconstrained" = 18,   
  "Terbeam constrained" = 15,   
  "Terga1 constrained" = 8
)

# Créer et afficher le graphique sans générer un fichier PDF
df_summary %>%
  ggplot(aes(x = Methods, y = mean_value, color = Subgroup, shape = Subgroup)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value), 
                position = position_dodge(width = 0.5), width = 0.2) +
  geom_point(data = subset(df_summary, is_max_accuracy == TRUE), 
             aes(x = Methods, y = mean_value), 
             shape = 8, color = "red", size = 5) + # Ajout d'une étoile pour la valeur maximale
  facet_grid(Dataset ~ Group, scales = "free") +
  ylab("Accuracy.generalization") +
  xlab("Methods") +
  theme_bw() +
  scale_colour_manual(values = custom_colors) +
  scale_shape_manual(values = custom_shapes) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )
```

```{r}
# Fusionner les quatre dataframes en ajoutant une colonne pour différencier les datasets
analysis_table_no_balance <- Metacardis_no_balanced_results %>%
  mutate(Dataset = "Enterotype no_balanced")

analysis_table_balance <- Metacardis_balanced_results %>%
  mutate(Dataset = "Enterotype balanced")

analysis_table_no_balance_2 <- CRC_Adenoma_Control_results %>%
  mutate(Dataset = "CRC, Adenoma, Control")

analysis_table_no_balance_3 <- T2D_IGT_Control_results %>%
  mutate(Dataset = "T2D, IGT, Control ")

# Combiner les quatre tables
df_combined <- bind_rows(analysis_table_no_balance, 
                         analysis_table_balance, 
                         analysis_table_no_balance_2, 
                         analysis_table_no_balance_3)

# Imputation des valeurs manquantes pour les colonnes numériques
df_combined <- df_combined %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Renommer "Features" en "k"
df_combined <- df_combined %>%
  rename(k = Features)

# Convertir en format long
df_long <- df_combined %>%
  pivot_longer(
    cols = c("Accuracy.empirique", "Accuracy.generalization"),  # Se limiter aux colonnes de précision
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>%
  mutate(
    Partition = recode(Partition, 'empirique' = 'train', 'generalization' = 'test'),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Predomics Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Predomics Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Predomics Terga1 constrained",
      TRUE ~ "SOTA"
    ),
    Subgroup = case_when(
      str_starts(Methods, "terbeam") ~ "Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Terga1 constrained",
      TRUE ~ "SOTA"
    )
  )

# Regrouper "Predomics Terbeam unconstrained" et "Predomics Terga1 unconstrained"
df_long$Group <- recode(df_long$Group,
                        "Predomics Terbeam unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terga1 unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terbeam constrained" = "Predomics Terbeam & Terga1 constrained", 
                        "Predomics Terga1 constrained" = "Predomics Terbeam & Terga1 constrained")

# Filtrer pour ne garder que la partition "test" et la métrique "Accuracy.generalization"
df_accuracy <- df_long %>% 
  filter(Metric == "Accuracy" & Partition == "test")

# Agréger les données pour le résumé
df_summary <- df_accuracy %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  )

# Supprimer les lignes avec des valeurs manquantes ou vides dans Metric ou Group
df_summary <- df_summary %>%
  filter(!is.na(Metric) & !is.na(Group) & Metric != "" & Group != "")

# Suppression des préfixes et des suffixes dans l'affichage des méthodes
df_summary <- df_summary %>%
  mutate(Methods = str_remove_all(Methods, "^(terbeam_|_terbeam_|terga1_|_terga1_)|(_no_balance|_balance)$"))

# Supprimer les doublons de méthodes après la suppression des préfixes et des suffixes
df_summary <- df_summary %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(mean_value, na.rm = TRUE),
    sd_value = mean(sd_value, na.rm = TRUE),
    .groups = "drop"
  )

# Trouver la valeur maximale d'accuracy pour chaque dataset et group
df_summary <- df_summary %>%
  group_by(Dataset, Group) %>%
  mutate(max_accuracy = max(mean_value, na.rm = TRUE)) %>%
  ungroup()

# Ajouter une colonne pour marquer les points avec l'accuracy maximale
df_summary <- df_summary %>%
  mutate(is_max_accuracy = ifelse(mean_value == max_accuracy, TRUE, FALSE))

# Définir une palette de couleurs personnalisée
custom_colors <- c(
  "Predomics Terbeam & Terga1 unconstrained" = "green", 
  "Predomics Terbeam & Terga1 constrained" = "blue", 
  "SOTA" = "black", 
  "Terbeam unconstrained" = "darkgreen", 
  "Terga1 unconstrained" = "darkblue", 
  "Terbeam constrained" = "lightgreen", 
  "Terga1 constrained" = "lightblue"
)

# Définir une palette de formes personnalisée
custom_shapes <- c(
  "Predomics Terbeam & Terga1 unconstrained" = 16, 
  "Predomics Terbeam & Terga1 constrained" = 4,   
  "SOTA" = 19,   
  "Terbeam unconstrained" = 17,   
  "Terga1 unconstrained" = 18,   
  "Terbeam constrained" = 15,   
  "Terga1 constrained" = 8
)

# Créer et enregistrer le graphique en format PDF avec disposition paysage
pdf("accuracy_plot_with_stars_.pdf", width = 11, height = 10)

df_summary %>%
  ggplot(aes(x = Methods, y = mean_value, color = Subgroup, shape = Subgroup)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value), 
                position = position_dodge(width = 0.5), width = 0.2) +
  geom_point(data = subset(df_summary, is_max_accuracy == TRUE), 
             aes(x = Methods, y = mean_value), 
             shape = 8, color = "red", size = 5) + # Ajout d'une étoile pour la valeur maximale
  facet_grid(Dataset ~ Group, scales = "free") +
  ylab("Accuracy.generalization") +
  xlab("Methods") +
  theme_bw() +
  scale_colour_manual(values = custom_colors) +
  scale_shape_manual(values = custom_shapes) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )

# Fermer le fichier PDF
dev.off()
```















# Regrouper Predomics


```{r warning=FALSE, fig.height=10, fig.width=12, message=FALSE, warning=FALSE, paged.print=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)

# Fusionner les quatre dataframes en ajoutant une colonne pour différencier les datasets
analysis_table_no_balance_ <- analysis_table_final_complet_no_balance_ %>%
  mutate(Dataset = "Enterotype no_balanced")

analysis_table_balance_ <- analysis_table_final_complet_balance_ %>%
  mutate(Dataset = "Enterotype balanced")

analysis_table_no_balance_2_ <- analysis_table_final_complet_no_balance_2_ %>%
  mutate(Dataset = "CRC, Adenoma, Control")

analysis_table_no_balance_3_ <- analysis_table_final_complet_no_balance_3_ %>%
  mutate(Dataset = "T2D, IGT, Control")

# Combiner les quatre tables
df_combined <- bind_rows(
  analysis_table_no_balance_,
  analysis_table_balance_,
  analysis_table_no_balance_2_,
  analysis_table_no_balance_3_
)

# Imputation des valeurs manquantes pour les colonnes numériques
df_combined <- df_combined %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Renommer "Features" en "k"
df_combined <- df_combined %>%
  rename(k = Features)

# Convertir en format long
df_long <- df_combined %>%
  pivot_longer(
    cols = c("Accuracy.empirique", "Accuracy.generalization"),
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>%
  mutate(
    Partition = recode(Partition, 'empirique' = 'train', 'generalization' = 'test'),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics",
      str_starts(Methods, "_terbeam") ~ "Predomics",
      str_starts(Methods, "terga1") ~ "Predomics",
      str_starts(Methods, "_terga1") ~ "Predomics",
      TRUE ~ "SOTA"
    ),
    Subgroup = case_when(
      str_starts(Methods, "terbeam") ~ "Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Terga1 constrained",
      TRUE ~ "SOTA"
    )
  )

# Filtrer pour ne garder que la partition "test" et la métrique "Accuracy.generalization"
df_accuracy <- df_long %>%
  filter(Metric == "Accuracy" & Partition == "test")

# Agréger les données pour le résumé
df_summary <- df_accuracy %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  )

# Nettoyer les noms des méthodes
df_summary <- df_summary %>%
  mutate(Methods = str_remove_all(Methods, "^(terbeam_|_terbeam_|terga1_|_terga1_)|(_no_balance|_balance)$"))

# Supprimer les doublons de méthodes après nettoyage
df_summary <- df_summary %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(mean_value, na.rm = TRUE),
    sd_value = mean(sd_value, na.rm = TRUE),
    .groups = "drop"
  )

# Trouver la valeur maximale d'accuracy pour chaque dataset et group
df_summary <- df_summary %>%
  group_by(Dataset, Group) %>%
  mutate(max_accuracy = max(mean_value, na.rm = TRUE)) %>%
  ungroup()

# Marquer les points avec l'accuracy maximale
df_summary <- df_summary %>%
  mutate(is_max_accuracy = ifelse(mean_value == max_accuracy, TRUE, FALSE))

# Palette de couleurs personnalisée
custom_colors <- c(
  "Predomics" = "blue",
  "SOTA" = "black",
  "Terbeam unconstrained" = "darkgreen",
  "Terga1 unconstrained" = "darkblue",
  "Terbeam constrained" = "lightgreen",
  "Terga1 constrained" = "lightblue",
  "SOTA" = "black"
)

# Formes personnalisées
custom_shapes <- c(
  "Terbeam unconstrained" = 17,
  "Terga1 unconstrained" = 18,
  "Terbeam constrained" = 15,
  "Terga1 constrained" = 8,
  "SOTA" = 19
)

# Création du graphique
df_summary %>%
  ggplot(aes(x = Methods, y = mean_value, color = Subgroup, shape = Subgroup)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value),
                position = position_dodge(width = 0.5), width = 0.2) +
  geom_point(data = subset(df_summary, is_max_accuracy == TRUE),
             aes(x = Methods, y = mean_value),
             shape = 8, color = "red", size = 5) +  # Étoile rouge pour max
  facet_grid(Dataset ~ Group, scales = "free") +
  ylab("Accuracy.generalization") +
  xlab("Methods") +
  theme_bw() +
  scale_colour_manual(values = custom_colors) +
  scale_shape_manual(values = custom_shapes) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )


```



## PDF



```{r}

library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)

# Fusionner les quatre dataframes en ajoutant une colonne pour différencier les datasets
analysis_table_no_balance_1 <- analysis_table_final_complet_no_balance_Metacardis %>%
  mutate(Dataset = "Metacardis Enterotype\nno balanced")

analysis_table_balance_2 <- analysis_table_final_complet_balance_Metacardis %>%
  mutate(Dataset = "Metacardis Enterotype\nbalanced")

analysis_table_no_balance_3 <- analysis_table_final_complet_no_balance_CRC %>%
  mutate(Dataset = "Study on CRC\nno balanced")

analysis_table_no_balance_4 <- analysis_table_final_complet_no_balance_TD2 %>%
  mutate(Dataset = "Study on T2D\nno balanced")

# Combiner les quatre tables
df_combined <- bind_rows(
  analysis_table_no_balance_1,
  analysis_table_balance_2,
  analysis_table_no_balance_3,
  analysis_table_no_balance_4
)

# Imputation des valeurs manquantes pour les colonnes numériques
df_combined <- df_combined %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Renommer "Features" en "k"
df_combined <- df_combined %>% rename(k = Features)

# Convertir en format long
df_long <- df_combined %>%
  pivot_longer(
    cols = c("Accuracy.empirique", "Accuracy.generalization"),
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>%
  mutate(
    Partition = recode(Partition, 'empirique' = 'train', 'generalization' = 'test'),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics",
      str_starts(Methods, "_terbeam") ~ "Predomics",
      str_starts(Methods, "terga1") ~ "Predomics",
      str_starts(Methods, "_terga1") ~ "Predomics",
      TRUE ~ "SOTA"
    ),
    Subgroup = case_when(
      str_starts(Methods, "terbeam") ~ "Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Terga1 constrained",
      TRUE ~ "SOTA"
    )
  )

# Filtrer pour ne garder que la partition "test" et la métrique "Accuracy"
df_accuracy <- df_long %>%
  filter(Metric == "Accuracy" & Partition == "test")

# Résumé statistique
df_summary <- df_accuracy %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  )

# Nettoyage des noms de méthodes
df_summary <- df_summary %>%
  mutate(Methods = str_remove_all(Methods, "^(terbeam_|_terbeam_|terga1_|_terga1_)|(_no_balance|_balance)$")) %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(mean_value, na.rm = TRUE),
    sd_value = mean(sd_value, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  group_by(Dataset, Group) %>%
  mutate(max_accuracy = max(mean_value, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(is_max_accuracy = ifelse(mean_value == max_accuracy, TRUE, FALSE))

# Palette et formes personnalisées
custom_colors <- c(
  "Predomics" = "blue",
  "SOTA" = "black",
  "Terbeam unconstrained" = "darkgreen",
  "Terga1 unconstrained" = "darkblue",
  "Terbeam constrained" = "lightgreen",
  "Terga1 constrained" = "lightblue",
  "SOTA" = "black"
)

custom_shapes <- c(
  "Terbeam unconstrained" = 17,
  "Terga1 unconstrained" = 18,
  "Terbeam constrained" = 15,
  "Terga1 constrained" = 8,
  "SOTA" = 19
)

# Appliquer un retour à la ligne dans les noms des méthodes uniquement pour le groupe Predomics
df_summary <- df_summary %>%
  mutate(Methods = ifelse(Group == "Predomics" & nchar(Methods) > 20, 
                          str_wrap(Methods, width = 20), 
                          Methods)) %>%
  # Forcer le retour à la ligne avant "unconstrained" et "constrained"
  mutate(Methods = ifelse(Group == "Predomics", 
                          str_replace_all(Methods, 
                                          c("unconstrained" = "\nunconstrained", 
                                            "constrained" = "\nconstrained")), 
                          Methods))

# Création du graphique dans un PDF paysage
pdf("accuracy_generalization_plot.pdf", width = 14, height = 9, paper = "special")

ggplot(df_summary, aes(x = Methods, y = mean_value, color = Subgroup, shape = Subgroup)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value),
                position = position_dodge(width = 0.5), width = 0.2) +
  geom_point(data = subset(df_summary, is_max_accuracy == TRUE),
             aes(x = Methods, y = mean_value),
             shape = 8, color = "red", size = 5) +  # Étoile rouge pour max
  facet_grid(Dataset ~ Group, scales = "free") +
  ylab("accuracy (CV generalization)") +
  xlab("Methods") +
  theme_bw() +
  scale_colour_manual(values = custom_colors) +
  scale_shape_manual(values = custom_shapes) +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
    legend.position = "bottom"
  )

dev.off()


```








```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)

# Fusionner les quatre dataframes en ajoutant une colonne pour différencier les datasets
analysis_table_no_balance_1 <- analysis_table_final_complet_no_balance_Metacardis %>%
  mutate(Dataset = "Metacardis Enterotype\nno balanced")

analysis_table_balance_2 <- analysis_table_final_complet_balance_Metacardis %>%
  mutate(Dataset = "Metacardis Enterotype\nbalanced")

analysis_table_no_balance_3 <- analysis_table_final_complet_no_balance_CRC %>%
  mutate(Dataset = "Study on CRC\nno balanced")

analysis_table_no_balance_4 <- analysis_table_final_complet_no_balance_TD2 %>%
  mutate(Dataset = "Study on T2D\nno balanced")

# Combiner les quatre tables
df_combined <- bind_rows(
  analysis_table_no_balance_1,
  analysis_table_balance_2,
  analysis_table_no_balance_3,
  analysis_table_no_balance_4
)

# Imputation des valeurs manquantes pour les colonnes numériques
df_combined <- df_combined %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Renommer "Features" en "k"
df_combined <- df_combined %>% rename(k = Features)

# Convertir en format long
df_long <- df_combined %>%
  pivot_longer(
    cols = c("Accuracy.empirique", "Accuracy.generalization"),
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>%
  mutate(
    Partition = recode(Partition, 'empirique' = 'train', 'generalization' = 'test'),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics",
      str_starts(Methods, "_terbeam") ~ "Predomics",
      str_starts(Methods, "terga1") ~ "Predomics",
      str_starts(Methods, "_terga1") ~ "Predomics",
      TRUE ~ "SOTA"
    ),
    Subgroup = case_when(
      str_starts(Methods, "terbeam") ~ "Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Terga1 constrained",
      TRUE ~ "SOTA"
    )
  )

# Filtrer pour ne garder que la partition "test" et la métrique "Accuracy"
df_accuracy <- df_long %>%
  filter(Metric == "Accuracy" & Partition == "test")

# Résumé statistique
df_summary <- df_accuracy %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  )

# Nettoyage des noms de méthodes
df_summary <- df_summary %>%
  mutate(Methods = str_remove_all(Methods, "^(terbeam_|_terbeam_|terga1_|_terga1_)|(_no_balance|_balance)$")) %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(mean_value, na.rm = TRUE),
    sd_value = mean(sd_value, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  group_by(Dataset, Group) %>%
  mutate(max_accuracy = max(mean_value, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(is_max_accuracy = ifelse(mean_value == max_accuracy, TRUE, FALSE))

# Ajouter une colonne pour la significativité
df_summary <- df_summary %>%
  mutate(is_significant = ifelse(mean_value > 0.8, "*", NA))  # Exemple avec un seuil de 0.8

# Palette et formes personnalisées
custom_colors <- c(
  "Predomics" = "blue",
  "SOTA" = "black",
  "Terbeam unconstrained" = "darkgreen",
  "Terga1 unconstrained" = "darkblue",
  "Terbeam constrained" = "lightgreen",
  "Terga1 constrained" = "lightblue",
  "SOTA" = "black"
)

custom_shapes <- c(
  "Terbeam unconstrained" = 17,
  "Terga1 unconstrained" = 18,
  "Terbeam constrained" = 15,
  "Terga1 constrained" = 8,
  "SOTA" = 19
)

# Appliquer un retour à la ligne dans les noms des méthodes uniquement pour le groupe Predomics
df_summary <- df_summary %>%
  mutate(Methods = ifelse(Group == "Predomics" & nchar(Methods) > 20, 
                          str_wrap(Methods, width = 20), 
                          Methods)) %>%
  # Forcer le retour à la ligne avant "unconstrained" et "constrained"
  mutate(Methods = ifelse(Group == "Predomics", 
                          str_replace_all(Methods, 
                                          c("unconstrained" = "\nunconstrained", 
                                            "constrained" = "\nconstrained")), 
                          Methods))

# Création du graphique dans un PDF paysage
pdf("accuracy_generalization_plot.pdf", width = 14, height = 9, paper = "special")

ggplot(df_summary, aes(x = Methods, y = mean_value, color = Subgroup, shape = Subgroup)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value),
                position = position_dodge(width = 0.5), width = 0.2) +
  geom_point(data = subset(df_summary, is_max_accuracy == TRUE),
             aes(x = Methods, y = mean_value),
             shape = 8, color = "red", size = 5) +  # Étoile rouge pour max
  geom_text(aes(label = is_significant), vjust = -1, color = "black", size = 5) +  # Ajouter les étoiles
  facet_grid(Dataset ~ Group, scales = "free") +
  ylab("accuracy (CV generalization)") +
  xlab("Methods") +
  theme_bw() +
  scale_colour_manual(values = custom_colors) +
  scale_shape_manual(values = custom_shapes) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1), 
        legend.position = "bottom")

dev.off()



```











