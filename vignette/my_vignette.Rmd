---
title: "my_vignette"
author: "Fabien"
date: "2024-12-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# The mcpredomics package is a supervised learning method for interpretable
# multiclass classification of metagenomic data. This method is derived 
# from one of the genetic algorithm-based heuristics in the predomics package.
# More details: https://github.com/predomics/predomicspkg

# Installation Instructions

## Step 1: Install dependencies
install.packages(c("doSNOW", "foreach", "snow", "doRNG", "gtools",
                   "glmnet", "pROC", "viridis", "kernlab", "randomForest"))

# Ensure BiocManager is installed
if (!requireNamespace("BiocManager", quietly = TRUE)) {
  install.packages("BiocManager")
}

# Install BioQC (a Bioconductor package)
BiocManager::install("BioQC")

# Optionally install additional packages
install.packages(c("testthat", "roxygen2", "predomics", "mcpredomics"))



#Introduction

The mcpredomics package is designed to streamline the development, evaluation, and visualization of machine learning models in a multiclass context. It provides a comprehensive suite of functions to assist users in every stage of the modeling process, from visualizing model characteristics to assessing their performance and managing associated populations. This documentation serves as a guide to introduce the package’s functionality and provide users with the tools they need for effective implementation.

## Visualization Functions

The package includes a variety of general-purpose visualization functions to explore models, their performance metrics, and the underlying populations. These functions allow users to gain valuable insights into their data and models with ease.
The implemented functions include:
printy_mc
printModel_mc
printPopulation_mc
printModelCollection_mc
printExperiment_mc
printClassifier_mc
plotAUC_mc
plotPrevalence_mc
plotAbundanceByClass_mc
mergeMeltImportanceCV_mc
plotFeatureModelCoeffs_mc
makeFeatureAnnot_mc
plotModel_mc
To learn more about the functionality of each visualization tool, use the ?? command followed by the function name. For example, ??printy_mc provides detailed information about the printy_mc function.

##Binary Sub-Model Evaluation

To support the evaluation of binary sub-models, we have implemented several global functions that provide insights into accuracy, AUC, regression metrics, feature importance, and more. These tools are crucial for assessing the quality and relevance of individual sub-models within the multiclass framework.
The following functions are available:
getSign_mc,
evaluateAccuracy_mc,
evaluateAUC_mc,
evaluateAdditionnalMetrics_mc,
evaluateModelRegression_mc,
evaluateFit_mc,
evaluateModel_mc,
evaluatePopulation_mc,
listOfSparseVecToListOfModels_mc,
evaluateFeatureImportanceInPopulation_mc,
generate_combinations_with_factors.
For a detailed description of any of these functions, you can use the ?? command. For instance, ??evaluateAccuracy_mc provides a comprehensive overview of the evaluateAccuracy_mc function.

##Multiclass Predictions and Aggregation Methods

The package supports multiclass modeling using one-versus-all (OVA) and one-versus-one (OVO) approaches. Additionally, it provides multiple methods for aggregating predictions from these approaches, as well as tools for evaluating the aggregated models.
The available functions are:
Prediction: predictModel_ova, predictModel_ovo,
Aggregation: voting, Predomics_aggregation_ovo, weighted, Predomics_aggregation_ova, maximization, ranking
Evaluation: evaluateModel_aggregation, evaluateModels_aggregation.
Each function serves a unique role, whether it involves generating predictions, combining results, or assessing model performance. For more details, use the ?? command, such as ??predictModel_ova for the predictModel_ova function.

##Multiclass Algorithm Implementations

We have extended the terBeam and terga1 algorithms to handle multiclass problems, introducing a set of specialized functions for model generation, feature combination management, and evolution optimization. These tools are designed to create scalable and efficient multiclass models.
The implemented functions include:
terBeam_mc, terBeam_fit_mc
generateAllSingleFeatureModel_mc, generateAllCombinations_mc
countEachFeatureApperance_mc, getFeatures2Keep_mc
terga1_mc, terga1_mc_fit
evolve_mc
You can explore the purpose and functionality of each function using the ?? command. For example, ??terBeam_fit_mc will provide a detailed explanation of the terBeam_fit_mc function.

##Core Functionalities

To ensure flexibility and efficiency in training, testing, and validating multiclass models, we have implemented the following core functions:
fit_mc: Fits models to the provided dataset.
runClassifier_mc: Executes the classifier to generate predictions.
runCrossval_mc: Runs cross-validation to evaluate model performance.
These foundational functions make it easier to integrate multiclass workflows into your projects. For detailed information on any of these functions, use the ?? command, such as ??runClassifier_mc for the runClassifier_mc function.





###Loading libraries

```{r warning=FALSE}
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
library(ggpubr)
library(dplyr)
library(tidyr)
library(tibble)
library(knitr)
library(kableExtra)
library(DT)
library(e1071)
library(glmnet)

```


### Balance the dataset by extracting the same number of samples for all classes.


```{r}
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]

# Filter null values
X_general <- mc.input$X[, colSums(mc.input$X) != 0]
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE)

set.seed(42)
y = as.vector(yvec)
X = X_general

# Determine the number of samples per class
nombre_echantillons_par_classe <- min(table(y))

# Function to balance classes and maintain order
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe, seed = 123) {
  classes <- unique(y)
  indices_equilibres <- integer(0)
  
  for (classe in classes) {
    indices_classe <- which(y == classe)
    set.seed(seed)
    indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
  }

  # Sort balanced indices to maintain original order
  indices_equilibres <- sort(indices_equilibres)
  return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}

# Get balanced data
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X

# Check distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)

# Split balanced data into 80% for training and 20% for testing
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[, indices_division, drop = FALSE]
X.test <- X_equilibre[, -indices_division, drop = FALSE]

table(y)
table(y.test)
dim(X)
dim(X.test)
```

### Original unbalanced dataset

```{r}
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]

# Filter null values
X_general <- mc.input$X[, colSums(mc.input$X) != 0]
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE)

set.seed(42)
y = as.vector(yvec)
X = X_general
# Check distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y, p = 0.8, list = FALSE)

# Split balanced data into 80% for training and 20% for testing
y <- as.vector(y[indices_division])
y.test <- as.vector(y[-indices_division])
X <- X[, indices_division, drop = FALSE]
X.test <- X[, -indices_division, drop = FALSE]

table(y)
table(y.test)
dim(X)
dim(X.test)
```

### Terbeam_mc Multiclass classifier


```{r}
clf <- terBeam_mc(sparsity = c(5,6,7,8,9,10), 
                                   max.nb.features = 1000,
                                   seed = 1,
                                   nCores = 1,
                                   evalToFit = "accuracy_",
                                   objective = "auc",
                                   experiment.id = "terBeam_mc",
                                   experiment.save = "nothing")
  
                      
  printy(clf)  
```


### Terga1_mc Multiclass classifier


```{r}
clf <- terga1_mc(nCores = 1,
              seed = 1,
              plot = TRUE
)
printy(clf)
```

### Running experiment Terga1 Predomics Aggregation one versus all

```{r running experiment, echo=TRUE, fig.width=5, warning=FALSE}
runit = TRUE
if(runit)
{
terbeam_predomics_aggregation_ovo_unconstrained_no_balance <- fit_mc(X = X, y = y, clf = clf,approch="ovo", cross.validate = TRUE,aggregation_ = "Predomics_aggregation_ovo", nfolds= 10, constrained = FALSE); 
save(terbeam_predomics_aggregation_ovo_unconstrained_no_balance , clf, file ="terbeam_predomics_aggregation_ovo_unconstrained_no_balance.rda", compression_level = 9)
}


```




### Exploring the results

```{r load results, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

  load("terga1_Predomics_aggregation_ova_unconstrained_no_balance.rda")

```


### Digesting the results 

```{r digesting results, fig.height=10, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
res_clf.dig <- digestmc(obj = terbeam_predomics_aggregation_ova_unconstrained_no_balance, penalty = 0.75/100, plot = TRUE)
```


### Regenerate object Clf 

```{r}
clf <- regenerate_clf(clf, X, y, approch = "ova")

```

### Visualizing the best model

The best-learned model can be visualized simply by printing it along with a summary of information on performance and model size. Furthermore, we can further explore the model by visualizing it using the barcode plot ?plotModel_mc. Finally, the importance of each feature can also be displayed using the same function, as illustrated in the figure below.

```{r best model, fig.width=7, warning=FALSE}
# get the best model
best.model = res_clf.dig$best$model
# Visualize the model information (if needed)
printy_mc(best.model)

# Generate the plots
plots1 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ova")
plots2 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE, approch = "ova")

# Open a PDF device to save the grid of plots
pdf("model_visualization_plots.pdf", width = 12, height = 8)  # Adjust width and height as needed

# Extract the plots from each list and pass them to `grid.arrange`
grid.arrange(grobs = c(plots1, plots2), ncol = 4)

# Close the PDF device
dev.off()

# Confirmation of the save process
cat("The plots have been saved under the name 'model_visualization_plots.pdf'.\n")

```
### Visualizing model performance AUC

```{r roc analyses, fig.height=7, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
tmp <- plotAUC_mc(scores = best.model$score_, y = y, percent = TRUE, approch = "ova"); rm(tmp)


```

### Family of Best Models (FBM)

A family of best models is defined as the set of models returned by the predomics algorithm, whose accuracy is within a given window of the best model's accuracy. This window is defined by computing a significance threshold assuming that accuracy follows a binomial distribution (p<0.05). 

```{r}
# Convert the model collection into a population of models scrambled by model size
pop <- modelCollectionToPopulation(terbeam_predomics_aggregation_ova_unconstrained_no_balance$classifier$models)
printy_mc(pop)
# Function to create boxplot for a given data frame
create_boxplot <- function(data, title) {
  # Melt the dataframe for ggplot
  data.melt <- melt(data, id.vars = c("accuracy_", "eval.sparsity"))
  
  # Create ggplot
  plot <- ggplot(data = data.melt, aes(y = accuracy_, x = eval.sparsity)) +
    geom_boxplot(notch = FALSE, outlier.shape = NA, position = position_dodge(width = 0.9), alpha = 0.3) +
    geom_point(aes(color = eval.sparsity), position = position_jitterdodge(dodge.width = 0.9), size = 1, alpha = 0.5) +
    ylim(c(0, 1)) +
    xlab("Model Parsimony") +
    ggtitle(title) +
    theme_bw() +
    theme(legend.position = "bottom", legend.direction = "horizontal") +
    guides(colour = "none")
  
  return(plot)
}

# Convert the population to a data frame
pop.df <- populationToDataFrame_mc(pop)

# Plotting for the original population (single figure)
pop.dff <- as.data.frame(pop.df[[1]])  # Convert the first submodel to a data frame
g.before <- create_boxplot(pop.dff, title = "Original Population")
#print(g.before)  # Display the plot

# Select the best population models
fbm <- selectBestPopulation(pop)
#printy_mc(fbm)

# Convert the best population models to a data frame
fbm.df <- populationToDataFrame_mc(fbm)

# Plotting for the selected best models (single figure)
fbm.dff <- as.data.frame(fbm.df[[1]])  # Convert the first submodel to a data frame
g.after <- create_boxplot(fbm.dff, title = "FBM")
print(g.after)  # Display the plot

```

###Analyzing the FBM

The analysis of the family of best models (FBM) can be very insightful for discovering the most important variables in the prediction process. Let's start by examining the usage of variables in the FBM models. For this, we run the ?makeFeatureAnnot_mc function to obtain the feature distribution in the model matrix, which is a list of pop.noz elements. The models in the FBM are ranked by accuracy, and this order will be preserved in the coefficient dataframe. There are 20 features found in the FBM models. The figure below shows that some of these features are very prevalent in the FBM, making them likely important. The abundance and prevalence distribution of these features can be explored with ?plotAbundanceByClass_mc and ?plotPrevalence_mc, respectively. Gray stars on the right side of the graphics indicate significant differences between the prediction groups.

```{r  warning=FALSE, paged.print=FALSE, message=FALSE,fig.height=14, fig.width=16}
fa <- makeFeatureAnnot_mc(pop = fbm, 
                       X = X, 
                       y = y, 
                       clf = clf,
                       approch = "ova")
plot_distribution <- plotFeatureModelCoeffs_mc(feat.model.coeffs = fa, y, approch = "ova")
plots_abundance <- plotAbundanceByClass_mc(features = fa, X, y, approch = "ova")
plots_prevalence <- plotPrevalence_mc(features = fa, X = X, y = y, approch = "ova")
```

### Feature importance

Finally, we can explore the feature importance using mean decrease accuracy information computed during the cross-validation process. It is worth noting that the order of variables based on the prevalence in FBM, is concordant with the order of mean decrease accuracy (i.e. importance) in the cross-validation. But when we look at the variables based on their importance indipendetly on the empirical learning FBM, the picture changes somewhow. There are some important variables that were not selected in the FBM of the whole dataset. It is thus important to explore the cross validation importance.

```{r warning=FALSE, paged.print=FALSE, message=FALSE,fig.height=12, fig.width=15}
  feat1.import <- mergeMeltImportanceCV_mc(list.results = list(terBeam = terbeam_predomics_aggregation_ova_unconstrained_no_balance),
                                        filter.cv.prev = 0,
                                        min.kfold.nb = FALSE,
                                        learner.grep.pattern = "*",
                                        #nb.top.features = 50,
                                        feature.selection = fa,
                                        scaled.importance = TRUE,
                                        make.plot = TRUE,
                                        cv.prevalence = FALSE)


  feat2.import <- mergeMeltImportanceCV_mc(list.results = list(terBeam = terbeam_predomics_aggregation_ova_unconstrained_no_balance),
                                        filter.cv.prev = 0,
                                        min.kfold.nb = FALSE,
                                        learner.grep.pattern = "*",
                                        nb.top.features = 148,
                                        #feature.selection = rownames(fa$pop.noz),
                                        scaled.importance = TRUE,
                                        make.plot = TRUE,
                                        cv.prevalence = FALSE)
```




#############################################################################################################################
############################################################################################################################
## ANNEXES
### Analysys results predomics and sota


The data in the **analysis_table_final1** dataframe comes from various experiments conducted using **Predomics** and state-of-the-art methods. For the **Predomics** approach, two heuristics were used: **Terga1** and **Terbeam**. For each heuristic, 12 experiments were conducted, each involving 6 aggregation methods and 2 variable selection methods. The variables were selected in two ways: (i) a constrained selection, where the same variables are used for all models, and (ii) a model-specific selection, called "unconstrained".

Regarding the aggregation methods, two new approaches were developed: **Predomics_Aggrégation_ova**, adapted to the "one versus all" approach, and **Predomics_Aggrégation_ovo**, adapted to the "one versus one" approach. In parallel, we explored four state-of-the-art methods for each approach ("one versus all" and "one versus one"): **Maximization** and **Ranking** for the "one versus all" approach, and **Voting** and **Weighted** for the "one versus one" approach.

In addition to the **Predomics** heuristics, we also evaluated seven state-of-the-art methods: **Random Forest**, **SVM**, **Logistic Regression**, **KNN**, **Artificial Neural Networks**, **Gradient Boosting**, and **Decision Tree**.

All experiments were conducted using 10-fold cross-validation. The **analysis_table_final1** dataframe contains several columns, including **Fold**, which identifies the folds used, as well as columns for different performance metrics: **Accuracy.empirique**, **Accuracy.generalization**, **Precision.empirique**, **Precision.generalization**, **Recall.empirique**, **Recall.generalization**, **F1.empirique**, **F1.generalization**, and **Methods**, the latter indicating the method used for each evaluation.

To construct this dataframe, the results from the different experiments were merged into a single structure using the **bind_rows()** function, making it easier to analyze and compare the results obtained with the various approaches and methods used.

```{r warning=FALSE, fig.height=10, fig.width=12, message=FALSE, warning=FALSE, paged.print=FALSE}

# Charger les bibliothèques nécessaires
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)  # Pour str_starts

# Imputer les valeurs manquantes avec la moyenne
analysis_table_final <- analysis_table_final2 %>% 
  mutate(across(everything(), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Convertir en format long
df <- analysis_table_final %>% 
  pivot_longer(
    cols = -c(Fold, Methods),
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>% 
  mutate(
    Partition = recode(Partition,
                       'empirique' = 'train',
                       'generalization' = 'test'
    ),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Predomics Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Predomics Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Predomics Terga1 constrained",
      TRUE ~ "SOTA"  # Pour toutes les autres méthodes
    )
  )

# Agréger les données pour le résumé
df_summary <- df %>% 
  group_by(Methods, Partition, Metric) %>% 
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  ) %>% 
  left_join(df %>% select(Methods, Group) %>% distinct(), by = "Methods")  # Assurez-vous que Group est présent

# Vérifiez les groupes présents (optionnel pour le débogage)
#print(unique(df_summary$Group))

# Tracez le graphique
df_summary %>% 
  ggplot(aes(x = Methods, y = mean_value, color = Partition)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +  # Taille des points
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value),
                position = position_dodge(width = 0.5), width = 0.2) +
  facet_grid(Metric ~ Group, scales = "free") +  # Facettage par Metric et Group
  ylab("Value") +
  xlab("Methods") +  # Réglage des labels
  theme_bw() +
  scale_colour_manual(values = c("seagreen", "firebrick3")) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )


```



```{r}
# Charger les bibliothèques nécessaires
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)  # Pour str_starts

# Imputer les valeurs manquantes avec la moyenne
analysis_table_final <- analysis_table_final2 %>% 
  mutate(across(everything(), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Convertir en format long
df <- analysis_table_final %>% 
  pivot_longer(
    cols = -c(Fold, Methods),
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>% 
  mutate(
    Partition = recode(Partition,
                       'empirique' = 'train',
                       'generalization' = 'test'
    ),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Predomics Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Predomics Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Predomics Terga1 constrained",
      TRUE ~ "SOTA"  # Pour toutes les autres méthodes
    )
  )

# Agréger les données pour le résumé
df_summary <- df %>% 
  group_by(Methods, Partition, Metric) %>% 
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  ) %>% 
  left_join(df %>% select(Methods, Group) %>% distinct(), by = "Methods")  # Assurez-vous que Group est présent

# Définir le fichier PDF de sortie
output_pdf <- "analysis_plot.pdf"  # Nom du fichier PDF
pdf(file = output_pdf, width = 12, height = 8)  # Dimensions du graphique

# Générer le graphique
df_summary %>% 
  ggplot(aes(x = Methods, y = mean_value, color = Partition)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +  # Taille des points
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value),
                position = position_dodge(width = 0.5), width = 0.2) +
  facet_grid(Metric ~ Group, scales = "free") +  # Facettage par Metric et Group
  ylab("Value") +
  xlab("Methods") +  # Réglage des labels
  theme_bw() +
  scale_colour_manual(values = c("seagreen", "firebrick3")) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )

# Fermer le fichier PDF
dev.off()

# Message de confirmation
cat("Le fichier PDF a été généré : ", output_pdf, "\n")

```








```{r warning=FALSE}
str(analysis_table_final2)

# Aperçu des premières lignes
analysis_table_final2

```


```{r}
# Charger les bibliothèques nécessaires
library(dplyr)

# Regrouper par méthode et calculer les moyennes et écarts-types
summary_metrics <- analysis_table_final2 %>%
  group_by(Methods) %>%
  summarize(
    accuracy_empirical_mean = mean(Accuracy.empirique, na.rm = TRUE),
    accuracy_empirical_sd = sd(Accuracy.empirique, na.rm = TRUE),
    accuracy_generalization_mean = mean(Accuracy.generalization, na.rm = TRUE),
    accuracy_generalization_sd = sd(Accuracy.generalization, na.rm = TRUE),
    
    precision_empirical_mean = mean(Precision.empirique, na.rm = TRUE),
    precision_empirical_sd = sd(Precision.empirique, na.rm = TRUE),
    precision_generalization_mean = mean(Precision.generalization, na.rm = TRUE),
    precision_generalization_sd = sd(Precision.generalization, na.rm = TRUE),
    
    recall_empirical_mean = mean(Recall.empirique, na.rm = TRUE),
    recall_empirical_sd = sd(Recall.empirique, na.rm = TRUE),
    recall_generalization_mean = mean(Recall.generalization, na.rm = TRUE),
    recall_generalization_sd = sd(Recall.generalization, na.rm = TRUE),
    
    f1_empirical_mean = mean(F1.empirique, na.rm = TRUE),
    f1_empirical_sd = sd(F1.empirique, na.rm = TRUE),
    f1_generalization_mean = mean(F1.generalization, na.rm = TRUE),
    f1_generalization_sd = sd(F1.generalization, na.rm = TRUE)
  )

# Afficher les résultats
print(summary_metrics)
```



```{r warning=FALSE}
# Charger les bibliothèques nécessaires
library(dplyr)
library(tidyr)

# Vérifier les noms de colonnes du dataframe pour identifier la structure
colnames(df)

# Liste des métriques à analyser
metrics <- c("Accuracy.empirique", "Accuracy.generalization", 
             "Precision.empirique", "Precision.generalization", 
             "Recall.empirique", "Recall.generalization", 
             "F1.empirique", "F1.generalization")

# Vérification de la présence des colonnes métriques dans le dataframe
metrics_in_data <- metrics[metrics %in% colnames(df)]

# Si toutes les métriques nécessaires sont présentes, appliquer l'analyse
if (length(metrics_in_data) > 0) {
  
  # Calcul des moyennes
  method_stats_mean <- df %>%
    group_by(Methods) %>%
    summarise(across(all_of(metrics_in_data), ~mean(.), .names = "{.col}_mean"), .groups = 'drop')
  
  # Calcul des écarts-types
  method_stats_sd <- df %>%
    group_by(Methods) %>%
    summarise(across(all_of(metrics_in_data), ~sd(.), .names = "{.col}_sd"), .groups = 'drop')
  
  # Combinaison des moyennes et écarts-types dans un seul tableau
  method_stats <- left_join(method_stats_mean, method_stats_sd, by = "Methods")
  
  # Afficher les statistiques calculées pour chaque méthode et chaque métrique
  print("Statistiques des méthodes :")
  print(method_stats)
  
  # Transformation des données pour une structure plus facile à analyser
  method_stats_long <- method_stats %>%
    pivot_longer(cols = starts_with("Accuracy") | starts_with("Precision") |
                   starts_with("Recall") | starts_with("F1"),
                 names_to = c("Metric", "Statistic"),
                 names_sep = "_") %>%
    pivot_wider(names_from = Statistic, values_from = value)
  
  # Afficher les statistiques sous forme longue
  print("Données transformées :")
  print(method_stats_long)
  
  # Choisir la meilleure méthode basée sur la métrique 'Accuracy.generalization_mean'
  if ("Accuracy.generalization_mean" %in% colnames(method_stats)) {
    best_method <- method_stats %>%
      arrange(desc(Accuracy.generalization_mean)) %>%
      slice(1) %>%
      select(Methods, Accuracy.generalization_mean)
    
    print(paste("La meilleure méthode est :", best_method$Methods, 
                "avec Accuracy.generalization_mean =", best_method$Accuracy.generalization_mean))
  } else {
    print("La colonne 'Accuracy.generalization_mean' est manquante dans les données.")
  }
  
  # Effectuer des tests ANOVA pour chaque métrique présente
  for (metric in metrics_in_data) {
    if (metric %in% colnames(df)) {
      metric_data <- df %>%
        select(Methods, all_of(metric)) %>%
        drop_na()  # Supprimer les NA si présents
      
      # Appliquer l'ANOVA pour chaque métrique
      anova_result <- aov(as.formula(paste(metric, "~ Methods")), data = metric_data)
      
      # Résultats de l'ANOVA
      print(paste("Résultats de l'ANOVA pour", metric, ":"))
      print(summary(anova_result))
    }
  }
  
} else {
  # Si certaines colonnes sont manquantes, les afficher
  cat("Les colonnes suivantes sont manquantes dans les données : \n")
  print(setdiff(metrics, metrics_in_data))
}


```








```{r}
 analysis_table_final2
```











