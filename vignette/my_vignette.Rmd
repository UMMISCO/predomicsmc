---
title: "my_vignette"
author: "Fabien"
date: "2024-12-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# The mcpredomics package is a supervised learning method for interpretable
# multiclass classification of metagenomic data. This method is derived 
# from one of the genetic algorithm-based heuristics in the predomics package.
# More details: https://github.com/predomics/predomicspkg

# Installation Instructions

## Step 1: Install dependencies
install.packages(c("doSNOW", "foreach", "snow", "doRNG", "gtools",
                   "glmnet", "pROC", "viridis", "kernlab", "randomForest"))

# Ensure BiocManager is installed
if (!requireNamespace("BiocManager", quietly = TRUE)) {
  install.packages("BiocManager")
}

# Install BioQC (a Bioconductor package)
BiocManager::install("BioQC")

# Optionally install additional packages
install.packages(c("testthat", "roxygen2", "predomics", "mcpredomics"))



# Introduction

The mcpredomics package is designed to streamline the development, evaluation, and visualization of machine learning models in a multiclass context. It provides a comprehensive suite of functions to assist users in every stage of the modeling process, from visualizing model characteristics to assessing their performance and managing associated populations. This documentation serves as a guide to introduce the packageâ€™s functionality and provide users with the tools they need for effective implementation.

## Visualization Functions

The package includes a variety of general-purpose visualization functions to explore models, their performance metrics, and the underlying populations. These functions allow users to gain valuable insights into their data and models with ease.
The implemented functions include:
printy_mc
printModel_mc
printPopulation_mc
printModelCollection_mc
printExperiment_mc
printClassifier_mc
plotAUC_mc
plotPrevalence_mc
plotAbundanceByClass_mc
mergeMeltImportanceCV_mc
plotFeatureModelCoeffs_mc
makeFeatureAnnot_mc
plotModel_mc
To learn more about the functionality of each visualization tool, use the ?? command followed by the function name. For example, ??printy_mc provides detailed information about the printy_mc function.

## Binary Sub-Model Evaluation

To support the evaluation of binary sub-models, we have implemented several global functions that provide insights into accuracy, AUC, regression metrics, feature importance, and more. These tools are crucial for assessing the quality and relevance of individual sub-models within the multiclass framework.
The following functions are available:
getSign_mc,
evaluateAccuracy_mc,
evaluateAUC_mc,
evaluateAdditionnalMetrics_mc,
evaluateModelRegression_mc,
evaluateFit_mc,
evaluateModel_mc,
evaluatePopulation_mc,
listOfSparseVecToListOfModels_mc,
evaluateFeatureImportanceInPopulation_mc,
generate_combinations_with_factors.
For a detailed description of any of these functions, you can use the ?? command. For instance, ??evaluateAccuracy_mc provides a comprehensive overview of the evaluateAccuracy_mc function.

## Multiclass Predictions and Aggregation Methods

The package supports multiclass modeling using one-versus-all (OVA) and one-versus-one (OVO) approaches. Additionally, it provides multiple methods for aggregating predictions from these approaches, as well as tools for evaluating the aggregated models.
The available functions are:
Prediction: predictModel_ova, predictModel_ovo,
Aggregation: voting, Predomics_aggregation_ovo, weighted, Predomics_aggregation_ova, maximization, ranking
Evaluation: evaluateModel_aggregation, evaluateModels_aggregation.
Each function serves a unique role, whether it involves generating predictions, combining results, or assessing model performance. For more details, use the ?? command, such as ??predictModel_ova for the predictModel_ova function.

## Multiclass Algorithm Implementations

We have extended the terBeam and terga1 algorithms to handle multiclass problems, introducing a set of specialized functions for model generation, feature combination management, and evolution optimization. These tools are designed to create scalable and efficient multiclass models.
The implemented functions include:
terBeam_mc, terBeam_fit_mc
generateAllSingleFeatureModel_mc, generateAllCombinations_mc
countEachFeatureApperance_mc, getFeatures2Keep_mc
terga1_mc, terga1_mc_fit
evolve_mc
You can explore the purpose and functionality of each function using the ?? command. For example, ??terBeam_fit_mc will provide a detailed explanation of the terBeam_fit_mc function.

## Core Functionalities

To ensure flexibility and efficiency in training, testing, and validating multiclass models, we have implemented the following core functions:
fit_mc: Fits models to the provided dataset.
runClassifier_mc: Executes the classifier to generate predictions.
runCrossval_mc: Runs cross-validation to evaluate model performance.
These foundational functions make it easier to integrate multiclass workflows into your projects. For detailed information on any of these functions, use the ?? command, such as ??runClassifier_mc for the runClassifier_mc function.



# Experimental Phase

## Loading libraries

```{r warning=FALSE}
# Package multi class predomics
library(mcpredomics)
# Package predomics
library(predomics)
# Visualization library for creating complex plots
library(ggplot2) 
# Arranges multiple ggplot objects on a single page
library(gridExtra) 
# ROC curve analysis and AUC calculation
library(pROC) 
# Reshaping and melting data frames
library(reshape2) 
# Implementation of the Random Forest algorithm for classification and regression
library(randomForest) 
# Comprehensive library for classification and regression training
library(caret) 
# Various R programming tools and functions, including data manipulation
library(gtools) 
# Adding statistical comparisons and publication-ready visualizations
library(ggpubr) 
# Data manipulation and transformation (part of the tidyverse)
library(dplyr) 
# Tidying messy data by gathering and spreading
library(tidyr) 
# Enhanced data frames with row names as a column (tibble format)
library(tibble) 
# Dynamic report generation and displaying results in tables
library(knitr) 
# Creating aesthetically pleasing and customizable HTML tables
library(kableExtra) 
# Interactive tables for data visualization and exploration
library(DT) 
# Functions for statistical learning, including SVM and Naive Bayes
library(e1071) 
# Lasso and ridge regression via generalized linear models
library(glmnet) 
# Reading data from files (including CSV and text files)
library(readr) 
# String manipulation and regular expression functions
library(stringr) 

```


## Dataset

We experimented with three datasets. The first one was drawn from the MetaCardis project and consists of four enterotype groups: **Bacteroidetes1**, **Bacteroidetes2**, **Prevotella**, and **Ruminococcus**. To do this, we created an initial balanced dataset by randomly extracting the same number of samples from each group. Additionally, we used the original unbalanced dataset. 

Next, we experimented with two other datasets: 
1. A three-class dataset (**Adenoma**, **Colorectal Cancer**, **Control**) from the study by **Qiang Feng et al.**
2. A three-class dataset (**T2D**, **IGT**, and **Control**) from the study by **Fredrik H. Karlsson**.


### Balance the dataset Metacardis.

```{r}
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]

# Filter null values
X_general <- mc.input$X[, colSums(mc.input$X) != 0]
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE)

set.seed(42)
y = as.vector(yvec)
X = X_general

# Determine the number of samples per class
nombre_echantillons_par_classe <- min(table(y))

# Function to balance classes and maintain order
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe, seed = 123) {
  classes <- unique(y)
  indices_equilibres <- integer(0)
  
  for (classe in classes) {
    indices_classe <- which(y == classe)
    set.seed(seed)
    indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
  }

  # Sort balanced indices to maintain original order
  indices_equilibres <- sort(indices_equilibres)
  return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}

# Get balanced data
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X

# Check distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)

# Split balanced data into 80% for training and 20% for testing
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[, indices_division, drop = FALSE]
X.test <- X_equilibre[, -indices_division, drop = FALSE]

table(y)
dim(X)

```


### Original unbalanced dataset Metacardis

```{r}
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]

# Filter null values
X_general <- mc.input$X[, colSums(mc.input$X) != 0]
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE)

set.seed(42)
y = as.vector(yvec)
X = X_general
# Check distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y, p = 0.8, list = FALSE)

# Split balanced data into 80% for training and 20% for testing
y <- as.vector(y[indices_division])
y.test <- as.vector(y[-indices_division])
X <- X[, indices_division, drop = FALSE]
X.test <- X[, -indices_division, drop = FALSE]

table(y)
dim(X)
```

Load Other Datasets

```{r}
# Load the file
load("../data/predomics.inputs.ExperimentHubMulticlass.Rda")
lapply(predomics.inputs, function(x){summary(colSums(x[["X"]]))})
```


### Dataset Colorectal cancer


```{r}
# Extract data
data1 <- predomics.inputs$FengQ_2015
df <- data1$y.df
y <- df$study_condition
X <- data1$X
table(y)
dim(X)

```
### T2D

```{r}
data1 <- predomics.inputs$KarlssonFH_2013
df <- data1$y.df
y <- df$study_condition
X <- data1$X
table(y)
dim(X)
```



## Heuristics

Heuristics: We have integrated the possibility of calling multiclass functions while incorporating two heuristics from Predomics: TerGa1, based on genetic algorithms, and TerBeam, which performs beam search. This led to the development of the multiclass versions TerGa1 Multiclass and TerBeam Multiclass, applied in our package.  

### terbeam_mc

```{r}
clf <- terBeam_mc(sparsity = c(10), 
                                   max.nb.features = 1000,
                                   seed = 1,
                                   nCores = 1,
                                   evalToFit = "accuracy_",
                                   objective = "auc",
                                   experiment.id = "terBeam_mc",
                                   experiment.save = "nothing")
  
                      
  printy(clf)  
```


### Terga1_mc


```{r}
clf <- terga1_mc(nCores = 1,
              seed = 1,
              plot = TRUE
)
printy(clf)
```



## Running experiment

Launching the experiment using the TerBeam_MC heuristic, with the Predomics Aggregation method "One Versus All", following the One Versus All approach. This experiment is launched by setting the parameter `constrained` to `false`, meaning that each binary submodel will use its own variables for model construction.



```{r running experiment, echo=TRUE, fig.width=5, warning=FALSE}
runit = TRUE
if(runit)
{
terbeam_maximization_CRC_unconstrained_no_balance <- fit_mc(X = X, y = y, clf = clf,approch="ova", cross.validate = TRUE,aggregation_ = "maximization", nfolds= 10, constrained = FALSE); 
save(terbeam_maximization_CRC_unconstrained_no_balance, clf, file ="terbeam_maximization_CRC_unconstrained_no_balance.rda", compression_level = 9)
}


```




## Exploring the results

```{r load results, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

  load("res_clf_mc.rda")

```


```{r warning=FALSE}
res_clf.dig <- digest(obj = resclf2 , penalty = 0.75/100, plot = TRUE)
```





### Regenerate object Clf 

```{r}
clf <- regenerate_clf(clf, X, y, approch = "ova")
```


### Family of Best Models (FBM)

A family of best models is defined as the set of models returned by the predomics algorithm, whose accuracy is within a given window of the best model's accuracy. This window is defined by computing a significance threshold assuming that accuracy follows a binomial distribution (p<0.05). 


```{r}
# Convert the model collection into a population of models scrambled by model size
pop <- modelCollectionToPopulation(terbeam_Majority_Voting_with_Tie_Breaking_metacardis_unconstrained_balance$classifier$models)
printy_mc(pop)
# Function to create boxplot for a given data frame
create_boxplot <- function(data, title) {
  # Melt the dataframe for ggplot
  data.melt <- melt(data, id.vars = c("accuracy_", "eval.sparsity"))
  
  # Create ggplot
  plot <- ggplot(data = data.melt, aes(y = accuracy_, x = eval.sparsity)) +
    geom_boxplot(notch = FALSE, outlier.shape = NA, position = position_dodge(width = 0.9), alpha = 0.3) +
    geom_point(aes(color = eval.sparsity), position = position_jitterdodge(dodge.width = 0.9), size = 1, alpha = 0.5) +
    ylim(c(0, 1)) +
    xlab("Model Parsimony") +
    ggtitle(title) +
    theme_bw() +
    theme(legend.position = "bottom", legend.direction = "horizontal") +
    guides(colour = "none")
  
  return(plot)
}

# Convert the population to a data frame
pop.df <- populationToDataFrame_mc(pop)

# Plotting for the original population (single figure)
pop.dff <- as.data.frame(pop.df[[1]])  # Convert the first submodel to a data frame
g.before <- create_boxplot(pop.dff, title = "Original Population")
#print(g.before)  # Display the plot

# Select the best population models
fbm <- selectBestPopulation(pop)
#printy_mc(fbm)

# Convert the best population models to a data frame
fbm.df <- populationToDataFrame_mc(fbm)

# Plotting for the selected best models (single figure)
fbm.dff <- as.data.frame(fbm.df[[1]])  # Convert the first submodel to a data frame
g.after <- create_boxplot(fbm.dff, title = "FBM")
print(g.after)  # Display the plot
```

###Analyzing the FBM

The analysis of the family of best models (FBM) can be very insightful for discovering the most important variables in the prediction process. Let's start by examining the usage of variables in the FBM models. For this, we run the ?makeFeatureAnnot_mc function to obtain the feature distribution in the model matrix, which is a list of pop.noz elements. The models in the FBM are ranked by accuracy, and this order will be preserved in the coefficient dataframe. There are 20 features found in the FBM models. The figure below shows that some of these features are very prevalent in the FBM, making them likely important. The abundance and prevalence distribution of these features can be explored with ?plotAbundanceByClass_mc and ?plotPrevalence_mc, respectively. Gray stars on the right side of the graphics indicate significant differences between the prediction groups.


```{r warning=FALSE, paged.print=FALSE, message=FALSE,fig.height=9, fig.width=25}
fa <- makeFeatureAnnot_mc(pop = fbm, 
                       X = X, 
                       y = y, 
                       clf = clf,
                       approch = "ovo")
plot_distribution <- plotFeatureModelCoeffs_mc(feat.model.coeffs = fa, y, approch = "ovo")
plots_abundance <- plotAbundanceByClass_mc(features = fa, X, y, approch = "ovo")
plots_prevalence <- plotPrevalence_mc(features = fa, X = X, y = y, approch = "ovo")
```


### Feature importance

Finally, we can explore the feature importance using mean decrease accuracy information computed during the cross-validation process. It is worth noting that the order of variables based on the prevalence in FBM, is concordant with the order of mean decrease accuracy (i.e. importance) in the cross-validation. But when we look at the variables based on their importance indipendetly on the empirical learning FBM, the picture changes somewhow. There are some important variables that were not selected in the FBM of the whole dataset. It is thus important to explore the cross validation importance.

```{r warning=FALSE, paged.print=FALSE, message=FALSE,fig.height=9, fig.width=25}
  feat1.import <- mergeMeltImportanceCV_mc(list.results = list(terBeam = terbeam_Majority_Voting_with_Tie_Breaking_metacardis_unconstrained_balance ),
                                        filter.cv.prev = 0,
                                        min.kfold.nb = FALSE,
                                        learner.grep.pattern = "*",
                                        #nb.top.features = 50,
                                        feature.selection = fa,
                                        scaled.importance = TRUE,
                                        make.plot = TRUE,
                                        cv.prevalence = FALSE)


  feat2.import <- mergeMeltImportanceCV_mc(list.results = list(terBeam = terbeam_Majority_Voting_with_Tie_Breaking_metacardis_unconstrained_balance),
                                        filter.cv.prev = 0,
                                        min.kfold.nb = FALSE,
                                        learner.grep.pattern = "*",
                                        nb.top.features = 148,
                                        #feature.selection = rownames(fa$pop.noz),
                                        scaled.importance = TRUE,
                                        make.plot = TRUE,
                                        cv.prevalence = FALSE)
```



### Visualizing the best model

The best-learned model can be visualized simply by printing it along with a summary of information on performance and model size. Furthermore, we can further explore the model by visualizing it using the barcode plot ?plotModel_mc. Finally, the importance of each feature can also be displayed using the same function, as illustrated in the figure below.

```{r best model, fig.width=12, warning=FALSE}
# get the best model
best.model = fbm[[1]]
# Visualize the model information (if needed)
printy_mc(best.model)

# Generate the plots
plots1 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ovo")
plots2 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE, approch = "ovo")

# Open a PDF device to save the grid of plots
pdf("model_visualization_plots2.pdf", width = 15, height = 10)  # Adjust width and height as needed

# Extract the plots from each list and pass them to `grid.arrange`
grid.arrange(grobs = c(plots1, plots2), ncol = 6)

# Close the PDF device
dev.off()

# Confirmation of the save process
cat("The plots have been saved under the name 'model_visualization_plots2.pdf'.\n")

```
### Visualizing model performance AUC

```{r roc analyses, fig.height=7, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALSE}
tmp <- plotAUC_mc(scores = best.model$score_, y = y, percent = TRUE, approch = "ovo"); rm(tmp)


```










##########################################################################################################
############################################################################################################################






















## ANNEXES
### Analysys results predomics and sota


The data in the **analysis_table_final1** dataframe comes from various experiments conducted using **Predomics** and state-of-the-art methods. For the **Predomics** approach, two heuristics were used: **Terga1** and **Terbeam**. For each heuristic, 12 experiments were conducted, each involving 6 aggregation methods and 2 variable selection methods. The variables were selected in two ways: (i) a constrained selection, where the same variables are used for all models, and (ii) a model-specific selection, called "unconstrained".

Regarding the aggregation methods, two new approaches were developed: **Predomics_AggrÃ©gation_ova**, adapted to the "one versus all" approach, and **Predomics_AggrÃ©gation_ovo**, adapted to the "one versus one" approach. In parallel, we explored four state-of-the-art methods for each approach ("one versus all" and "one versus one"): **Maximization** and **Ranking** for the "one versus all" approach, and **Voting** and **Weighted** for the "one versus one" approach.

In addition to the **Predomics** heuristics, we also evaluated seven state-of-the-art methods: **Random Forest**, **SVM**, **Logistic Regression**, **KNN**, **Artificial Neural Networks**, **Gradient Boosting**, and **Decision Tree**.

All experiments were conducted using 10-fold cross-validation. The **analysis_table_final1** dataframe contains several columns, including **Fold**, which identifies the folds used, as well as columns for different performance metrics: **Accuracy.empirique**, **Accuracy.generalization**, **Precision.empirique**, **Precision.generalization**, **Recall.empirique**, **Recall.generalization**, **F1.empirique**, **F1.generalization**, and **Methods**, the latter indicating the method used for each evaluation.

To construct this dataframe, the results from the different experiments were merged into a single structure using the **bind_rows()** function, making it easier to analyze and compare the results obtained with the various approaches and methods used.

```{r warning=FALSE, fig.height=10, fig.width=12, message=FALSE, warning=FALSE, paged.print=FALSE}

# Charger les bibliothÃ¨ques nÃ©cessaires
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)  # Pour str_starts

# Imputer les valeurs manquantes avec la moyenne
analysis_table_final <- analysis_table_final2_no_balance %>% 
  mutate(across(everything(), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Convertir en format long
df <- analysis_table_final %>% 
  pivot_longer(
    cols = -c(Fold, Methods),
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>% 
  mutate(
    Partition = recode(Partition,
                       'empirique' = 'train',
                       'generalization' = 'test'
    ),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Predomics Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Predomics Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Predomics Terga1 constrained",
      TRUE ~ "SOTA"  # Pour toutes les autres mÃ©thodes
    )
  )

# AgrÃ©ger les donnÃ©es pour le rÃ©sumÃ©
df_summary <- df %>% 
  group_by(Methods, Partition, Metric) %>% 
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  ) %>% 
  left_join(df %>% select(Methods, Group) %>% distinct(), by = "Methods")  # Assurez-vous que Group est prÃ©sent

# VÃ©rifiez les groupes prÃ©sents (optionnel pour le dÃ©bogage)
#print(unique(df_summary$Group))

# Tracez le graphique
df_summary %>% 
  ggplot(aes(x = Methods, y = mean_value, color = Partition)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +  # Taille des points
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value),
                position = position_dodge(width = 0.5), width = 0.2) +
  facet_grid(Metric ~ Group, scales = "free") +  # Facettage par Metric et Group
  ylab("Value") +
  xlab("Methods") +  # RÃ©glage des labels
  theme_bw() +
  scale_colour_manual(values = c("seagreen", "firebrick3")) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )


```



```{r}
# Charger les bibliothÃ¨ques nÃ©cessaires
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)  # Pour str_starts

# Imputer les valeurs manquantes avec la moyenne
analysis_table_final <- analysis_table_final2 %>% 
  mutate(across(everything(), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Convertir en format long
df <- analysis_table_final %>% 
  pivot_longer(
    cols = -c(Fold, Methods),
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>% 
  mutate(
    Partition = recode(Partition,
                       'empirique' = 'train',
                       'generalization' = 'test'
    ),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Predomics Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Predomics Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Predomics Terga1 constrained",
      TRUE ~ "SOTA"  # Pour toutes les autres mÃ©thodes
    )
  )

# AgrÃ©ger les donnÃ©es pour le rÃ©sumÃ©
df_summary <- df %>% 
  group_by(Methods, Partition, Metric) %>% 
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  ) %>% 
  left_join(df %>% select(Methods, Group) %>% distinct(), by = "Methods")  # Assurez-vous que Group est prÃ©sent

# DÃ©finir le fichier PDF de sortie
output_pdf <- "analysis_plot2.pdf"  # Nom du fichier PDF
pdf(file = output_pdf, width = 12, height = 10)  # Dimensions du graphique

# GÃ©nÃ©rer le graphique
df_summary %>% 
  ggplot(aes(x = Methods, y = mean_value, color = Partition)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +  # Taille des points
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value),
                position = position_dodge(width = 0.5), width = 0.2) +
  facet_grid(Metric ~ Group, scales = "free") +  # Facettage par Metric et Group
  ylab("Value") +
  xlab("Methods") +  # RÃ©glage des labels
  theme_bw() +
  scale_colour_manual(values = c("seagreen", "firebrick3")) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )

# Fermer le fichier PDF
dev.off()

# Message de confirmation
cat("Le fichier PDF a Ã©tÃ© gÃ©nÃ©rÃ© : ", output_pdf, "\n")

```








```{r warning=FALSE}
str(analysis_table_final2)

# AperÃ§u des premiÃ¨res lignes
analysis_table_final2

```


```{r}
# Charger les bibliothÃ¨ques nÃ©cessaires
library(dplyr)

# Regrouper par mÃ©thode et calculer les moyennes et Ã©carts-types
summary_metrics <- analysis_table_final2 %>%
  group_by(Methods) %>%
  summarize(
    accuracy_empirical_mean = mean(Accuracy.empirique, na.rm = TRUE),
    accuracy_empirical_sd = sd(Accuracy.empirique, na.rm = TRUE),
    accuracy_generalization_mean = mean(Accuracy.generalization, na.rm = TRUE),
    accuracy_generalization_sd = sd(Accuracy.generalization, na.rm = TRUE),
    
    precision_empirical_mean = mean(Precision.empirique, na.rm = TRUE),
    precision_empirical_sd = sd(Precision.empirique, na.rm = TRUE),
    precision_generalization_mean = mean(Precision.generalization, na.rm = TRUE),
    precision_generalization_sd = sd(Precision.generalization, na.rm = TRUE),
    
    recall_empirical_mean = mean(Recall.empirique, na.rm = TRUE),
    recall_empirical_sd = sd(Recall.empirique, na.rm = TRUE),
    recall_generalization_mean = mean(Recall.generalization, na.rm = TRUE),
    recall_generalization_sd = sd(Recall.generalization, na.rm = TRUE),
    
    f1_empirical_mean = mean(F1.empirique, na.rm = TRUE),
    f1_empirical_sd = sd(F1.empirique, na.rm = TRUE),
    f1_generalization_mean = mean(F1.generalization, na.rm = TRUE),
    f1_generalization_sd = sd(F1.generalization, na.rm = TRUE)
  )

# Afficher les rÃ©sultats
print(summary_metrics)
```



```{r warning=FALSE}
# Charger les bibliothÃ¨ques nÃ©cessaires
library(dplyr)
library(tidyr)

# VÃ©rifier les noms de colonnes du dataframe pour identifier la structure
colnames(df)

# Liste des mÃ©triques Ã  analyser
metrics <- c("Accuracy.empirique", "Accuracy.generalization", 
             "Precision.empirique", "Precision.generalization", 
             "Recall.empirique", "Recall.generalization", 
             "F1.empirique", "F1.generalization")

# VÃ©rification de la prÃ©sence des colonnes mÃ©triques dans le dataframe
metrics_in_data <- metrics[metrics %in% colnames(df)]

# Si toutes les mÃ©triques nÃ©cessaires sont prÃ©sentes, appliquer l'analyse
if (length(metrics_in_data) > 0) {
  
  # Calcul des moyennes
  method_stats_mean <- df %>%
    group_by(Methods) %>%
    summarise(across(all_of(metrics_in_data), ~mean(.), .names = "{.col}_mean"), .groups = 'drop')
  
  # Calcul des Ã©carts-types
  method_stats_sd <- df %>%
    group_by(Methods) %>%
    summarise(across(all_of(metrics_in_data), ~sd(.), .names = "{.col}_sd"), .groups = 'drop')
  
  # Combinaison des moyennes et Ã©carts-types dans un seul tableau
  method_stats <- left_join(method_stats_mean, method_stats_sd, by = "Methods")
  
  # Afficher les statistiques calculÃ©es pour chaque mÃ©thode et chaque mÃ©trique
  print("Statistiques des mÃ©thodes :")
  print(method_stats)
  
  # Transformation des donnÃ©es pour une structure plus facile Ã  analyser
  method_stats_long <- method_stats %>%
    pivot_longer(cols = starts_with("Accuracy") | starts_with("Precision") |
                   starts_with("Recall") | starts_with("F1"),
                 names_to = c("Metric", "Statistic"),
                 names_sep = "_") %>%
    pivot_wider(names_from = Statistic, values_from = value)
  
  # Afficher les statistiques sous forme longue
  print("DonnÃ©es transformÃ©es :")
  print(method_stats_long)
  
  # Choisir la meilleure mÃ©thode basÃ©e sur la mÃ©trique 'Accuracy.generalization_mean'
  if ("Accuracy.generalization_mean" %in% colnames(method_stats)) {
    best_method <- method_stats %>%
      arrange(desc(Accuracy.generalization_mean)) %>%
      slice(1) %>%
      select(Methods, Accuracy.generalization_mean)
    
    print(paste("La meilleure mÃ©thode est :", best_method$Methods, 
                "avec Accuracy.generalization_mean =", best_method$Accuracy.generalization_mean))
  } else {
    print("La colonne 'Accuracy.generalization_mean' est manquante dans les donnÃ©es.")
  }
  
  # Effectuer des tests ANOVA pour chaque mÃ©trique prÃ©sente
  for (metric in metrics_in_data) {
    if (metric %in% colnames(df)) {
      metric_data <- df %>%
        select(Methods, all_of(metric)) %>%
        drop_na()  # Supprimer les NA si prÃ©sents
      
      # Appliquer l'ANOVA pour chaque mÃ©trique
      anova_result <- aov(as.formula(paste(metric, "~ Methods")), data = metric_data)
      
      # RÃ©sultats de l'ANOVA
      print(paste("RÃ©sultats de l'ANOVA pour", metric, ":"))
      print(summary(anova_result))
    }
  }
  
} else {
  # Si certaines colonnes sont manquantes, les afficher
  cat("Les colonnes suivantes sont manquantes dans les donnÃ©es : \n")
  print(setdiff(metrics, metrics_in_data))
}


```







```{r warning=FALSE, fig.height=14, fig.width=13, message=FALSE, warning=FALSE, paged.print=FALSE}
# Chargement des librairies nÃ©cessaires
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)

# Fusionner les quatre dataframes en ajoutant une colonne pour diffÃ©rencier les datasets
analysis_table_no_balance <- analysis_table_final_complet_no_balance %>%
  mutate(Dataset = "MetaCardis enterotype non-balanced")

analysis_table_balance <- analysis_table_final_complet_balance %>%
  mutate(Dataset = "MetaCardis enterotype balanced")

analysis_table_no_balance_2 <- analysis_table_final_complet_no_balance_2 %>%
  mutate(Dataset = "CRC, Adenoma and Control")

analysis_table_no_balance_3 <- analysis_table_final_complet_no_balance_3 %>%
  mutate(Dataset = "T2D,IGT and Control ")

# Combiner les quatre tables
df_combined <- bind_rows(analysis_table_no_balance, 
                         analysis_table_balance, 
                         analysis_table_no_balance_2, 
                         analysis_table_no_balance_3)

# Imputation des valeurs manquantes pour les colonnes numÃ©riques
df_combined <- df_combined %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Renommer "Features" en "k"
df_combined <- df_combined %>%
  rename(k = Features)

# Convertir en format long
df_long <- df_combined %>%
  pivot_longer(
    cols = c("Accuracy.empirique", "Accuracy.generalization"),  # Se limiter aux colonnes de prÃ©cision
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>%
  mutate(
    Partition = recode(Partition, 'empirique' = 'train', 'generalization' = 'test'),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Predomics Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Predomics Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Predomics Terga1 constrained",
      TRUE ~ "SOTA"
    ),
    Subgroup = case_when(
      str_starts(Methods, "terbeam") ~ "Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Terga1 constrained",
      TRUE ~ "SOTA"
    )
  )

# Regrouper "Predomics Terbeam unconstrained" et "Predomics Terga1 unconstrained"
df_long$Group <- recode(df_long$Group,
                        "Predomics Terbeam unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terga1 unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terbeam constrained" = "Predomics Terbeam & Terga1 constrained", 
                        "Predomics Terga1 constrained" = "Predomics Terbeam & Terga1 constrained")

# Filtrer pour ne garder que la partition "test" et la mÃ©trique "Accuracy.generalization"
df_accuracy <- df_long %>% 
  filter(Metric == "Accuracy" & Partition == "test")

# AgrÃ©ger les donnÃ©es pour le rÃ©sumÃ©
df_summary <- df_accuracy %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  )

# Supprimer les lignes avec des valeurs manquantes ou vides dans `Metric` ou `Group`
df_summary <- df_summary %>%
  filter(!is.na(Metric) & !is.na(Group) & Metric != "" & Group != "")

# Suppression des prÃ©fixes "terbeam", "_terbeam", "terga1", "_terga1" dans l'affichage des mÃ©thodes
df_summary <- df_summary %>%
  mutate(Methods = str_remove(Methods, "^_?terbeam_?|^_?terga1_?"))

# VÃ©rification des rÃ©sultats
print(unique(df_summary$Group))

# DÃ©finir une palette de couleurs personnalisÃ©e
custom_colors <- c(
  "Predomics Terbeam & Terga1 unconstrained" = "green", 
  "Predomics Terbeam & Terga1 constrained" = "blue", 
  "SOTA" = "red", 
  "Terbeam unconstrained" = "darkgreen", 
  "Terga1 unconstrained" = "darkblue", 
  "Terbeam constrained" = "lightgreen", 
  "Terga1 constrained" = "lightblue"
)

# DÃ©finir une palette de formes personnalisÃ©e
custom_shapes <- c(
  "Predomics Terbeam & Terga1 unconstrained" = 16, 
  "Predomics Terbeam & Terga1 constrained" = 4,   
  "SOTA" = 19,  
  "Terbeam unconstrained" = 17,  
  "Terga1 unconstrained" = 18,  
  "Terbeam constrained" = 15,  
  "Terga1 constrained" = 8
)

# Tracez le graphique avec faceting pour "Accuracy.generalization" uniquement
df_summary %>%
  ggplot(aes(x = Methods, y = mean_value, color = Subgroup, shape = Subgroup)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value), 
                position = position_dodge(width = 0.5), width = 0.2) +
  facet_grid(Dataset ~ Group, scales = "free") +  # Faceting par Dataset et Group
  ylab("Accuracy") +  # Changez "Value" en "Accuracy" pour plus de clartÃ©
  xlab("Methods") + 
  theme_bw() + 
  scale_colour_manual(values = custom_colors) +  # Appliquer les couleurs personnalisÃ©es
  scale_shape_manual(values = custom_shapes) +   # Appliquer les formes personnalisÃ©es
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )


```






```{r}
# Chargement des librairies nÃ©cessaires
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)

# Fusionner les quatre dataframes en ajoutant une colonne pour diffÃ©rencier les datasets
analysis_table_no_balance <- analysis_table_final_complet_no_balance %>%
  mutate(Dataset = "Enterotype_nobalanced")

analysis_table_balance <- analysis_table_final_complet_balance %>%
  mutate(Dataset = "Enterotype_balanced")

analysis_table_no_balance_2 <- analysis_table_final_complet_no_balance_2 %>%
  mutate(Dataset = "CRC")

analysis_table_no_balance_3 <- analysis_table_final_complet_no_balance_3 %>%
  mutate(Dataset = "T2D ")

# Combiner les quatre tables
df_combined <- bind_rows(analysis_table_no_balance, 
                         analysis_table_balance, 
                         analysis_table_no_balance_2, 
                         analysis_table_no_balance_3)

# Imputation des valeurs manquantes pour les colonnes numÃ©riques
df_combined <- df_combined %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Renommer "Features" en "k"
df_combined <- df_combined %>%
  rename(k = Features)

# Convertir en format long
df_long <- df_combined %>%
  pivot_longer(
    cols = c("Accuracy.empirique", "Accuracy.generalization"),  # Se limiter aux colonnes de prÃ©cision
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>%
  mutate(
    Partition = recode(Partition, 'empirique' = 'train', 'generalization' = 'test'),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Predomics Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Predomics Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Predomics Terga1 constrained",
      TRUE ~ "SOTA"
    ),
    Subgroup = case_when(
      str_starts(Methods, "terbeam") ~ "Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Terga1 constrained",
      TRUE ~ "SOTA"
    )
  )

# Regrouper "Predomics Terbeam unconstrained" et "Predomics Terga1 unconstrained"
df_long$Group <- recode(df_long$Group,
                        "Predomics Terbeam unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terga1 unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terbeam constrained" = "Predomics Terbeam & Terga1 constrained", 
                        "Predomics Terga1 constrained" = "Predomics Terbeam & Terga1 constrained")

# Filtrer pour ne garder que la partition "test" et la mÃ©trique "Accuracy.generalization"
df_accuracy <- df_long %>% 
  filter(Metric == "Accuracy" & Partition == "test")

# AgrÃ©ger les donnÃ©es pour le rÃ©sumÃ©
df_summary <- df_accuracy %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  )

# Supprimer les lignes avec des valeurs manquantes ou vides dans Metric ou Group
df_summary <- df_summary %>%
  filter(!is.na(Metric) & !is.na(Group) & Metric != "" & Group != "")

# Suppression des prÃ©fixes "terbeam", "_terbeam", "terga1", "_terga1" et des suffixes "_no_balance", "_balance" dans l'affichage des mÃ©thodes
df_summary <- df_summary %>%
  mutate(Methods = str_remove_all(Methods,"^(terbeam_|_terbeam_|terga1_|_terga1_)|(_no_balance|_balance)$"))


# VÃ©rification des rÃ©sultats
print(unique(df_summary$Group))

# DÃ©finir une palette de couleurs personnalisÃ©e
custom_colors <- c(
  "Predomics Terbeam & Terga1 unconstrained" = "green", 
  "Predomics Terbeam & Terga1 constrained" = "blue", 
  "SOTA" = "red", 
  "Terbeam unconstrained" = "darkgreen", 
  "Terga1 unconstrained" = "darkblue", 
  "Terbeam constrained" = "lightgreen", 
  "Terga1 constrained" = "lightblue"
)

# DÃ©finir une palette de formes personnalisÃ©e
custom_shapes <- c(
  "Predomics Terbeam & Terga1 unconstrained" = 16, 
  "Predomics Terbeam & Terga1 constrained" = 4,   
  "SOTA" = 19,  
  "Terbeam unconstrained" = 17,   
  "Terga1 unconstrained" = 18,   
  "Terbeam constrained" = 15,   
  "Terga1 constrained" = 8
)

# CrÃ©er et enregistrer le graphique en format PDF avec disposition paysage
pdf("accuracy_plot.pdf", width = 11, height = 10)  # Format paysage A4

df_summary %>%
  ggplot(aes(x = Methods, y = mean_value, color = Subgroup, shape = Subgroup)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value), 
                position = position_dodge(width = 0.5), width = 0.2) +
  facet_grid(Dataset ~ Group, scales = "free") +  # Faceting par Dataset et Group
  ylab("Accuracy") +  # Changez "Value" en "Accuracy" pour plus de clartÃ©
  xlab("Methods") + 
  theme_bw() + 
  scale_colour_manual(values = custom_colors) +  # Appliquer les couleurs personnalisÃ©es
  scale_shape_manual(values = custom_shapes) +   # Appliquer les formes personnalisÃ©es
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )

# Fermer le fichier PDF
dev.off()

```




```{r}
# Charger la bibliothÃ¨que nÃ©cessaire
library(dplyr)
library(stringr)

# Fonction pour remplacer les valeurs dans la colonne Methods
replace_methods <- function(df) {
  df %>%
    mutate(Methods = str_replace_all(Methods, 
      c("_terga1_Predomics_aggregation_ovo_constrained_balance" = "_terga1_predomics_aggregation_ovo_constrained_balance",
        "_terga1_Predomics_aggregation_ovo_unconstrained_balance" = "_terga1_predomics_aggregation_ovo_unconstrained_balance",
        "_terga1_Predomics_aggregation_ova_constrained_balance" = "_terga1_predomics_aggregation_ova_constrained_balance",
        "_terga1_Predomics_aggregation_ova_unconstrained_balance" = "_terga1_predomics_aggregation_ova_unconstrained_balance"))) %>%
    # Remplacer 'P' majuscule par 'p' dans 'Predomics'
    mutate(Methods = str_replace_all(Methods, "Predomics", "predomics"))
}

# Appliquer la fonction Ã  chaque dataframe
analysis_table_final_complet_no_balance <- replace_methods(analysis_table_final_complet_no_balance)
analysis_table_final_complet_balance <- replace_methods(analysis_table_final_complet_balance)
analysis_table_final_complet_no_balance_2 <- replace_methods(analysis_table_final_complet_no_balance_2)
analysis_table_final_complet_no_balance_3 <- replace_methods(analysis_table_final_complet_no_balance_3)

# VÃ©rifier les valeurs uniques aprÃ¨s modification
unique(analysis_table_final_complet_no_balance$Methods)
unique(analysis_table_final_complet_balance$Methods)
unique(analysis_table_final_complet_no_balance_2$Methods)
unique(analysis_table_final_complet_no_balance_3$Methods)


```







```{r}
# Chargement des librairies nÃ©cessaires
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)

# Fusionner les quatre dataframes en ajoutant une colonne pour diffÃ©rencier les datasets
analysis_table_no_balance <- analysis_table_final_complet_no_balance %>%
  mutate(Dataset = "Enterotype_nobalanced")

analysis_table_balance <- analysis_table_final_complet_balance %>%
  mutate(Dataset = "Enterotype_balanced")

analysis_table_no_balance_2 <- analysis_table_final_complet_no_balance_2 %>%
  mutate(Dataset = "CRC")

analysis_table_no_balance_3 <- analysis_table_final_complet_no_balance_3 %>%
  mutate(Dataset = "T2D ")

# Combiner les quatre tables
df_combined <- bind_rows(analysis_table_no_balance, 
                         analysis_table_balance, 
                         analysis_table_no_balance_2, 
                         analysis_table_no_balance_3)

# Imputation des valeurs manquantes pour les colonnes numÃ©riques
df_combined <- df_combined %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Renommer "Features" en "k"
df_combined <- df_combined %>%
  rename(k = Features)

# Convertir en format long
df_long <- df_combined %>%
  pivot_longer(
    cols = c("Accuracy.empirique", "Accuracy.generalization"),  # Se limiter aux colonnes de prÃ©cision
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>%
  mutate(
    Partition = recode(Partition, 'empirique' = 'train', 'generalization' = 'test'),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Predomics Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Predomics Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Predomics Terga1 constrained",
      TRUE ~ "SOTA"
    ),
    Subgroup = case_when(
      str_starts(Methods, "terbeam") ~ "Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Terga1 constrained",
      TRUE ~ "SOTA"
    )
  )

# Regrouper "Predomics Terbeam unconstrained" et "Predomics Terga1 unconstrained"
df_long$Group <- recode(df_long$Group,
                        "Predomics Terbeam unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terga1 unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terbeam constrained" = "Predomics Terbeam & Terga1 constrained", 
                        "Predomics Terga1 constrained" = "Predomics Terbeam & Terga1 constrained")

# Filtrer pour ne garder que la partition "test" et la mÃ©trique "Accuracy.generalization"
df_accuracy <- df_long %>% 
  filter(Metric == "Accuracy" & Partition == "test")

# AgrÃ©ger les donnÃ©es pour le rÃ©sumÃ©
df_summary <- df_accuracy %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  )

# Supprimer les lignes avec des valeurs manquantes ou vides dans Metric ou Group
df_summary <- df_summary %>%
  filter(!is.na(Metric) & !is.na(Group) & Metric != "" & Group != "")

# Suppression des prÃ©fixes et des suffixes dans l'affichage des mÃ©thodes
df_summary <- df_summary %>%
  mutate(Methods = str_remove_all(Methods, "^(terbeam_|_terbeam_|terga1_|_terga1_)|(_no_balance|_balance)$"))

# Supprimer les doublons de mÃ©thodes aprÃ¨s la suppression des prÃ©fixes et des suffixes
df_summary <- df_summary %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(mean_value, na.rm = TRUE),
    sd_value = mean(sd_value, na.rm = TRUE),
    .groups = "drop"
  )

# DÃ©finir une palette de couleurs personnalisÃ©e
custom_colors <- c(
  "Predomics Terbeam & Terga1 unconstrained" = "green", 
  "Predomics Terbeam & Terga1 constrained" = "blue", 
  "SOTA" = "red", 
  "Terbeam unconstrained" = "darkgreen", 
  "Terga1 unconstrained" = "darkblue", 
  "Terbeam constrained" = "lightgreen", 
  "Terga1 constrained" = "lightblue"
)

# DÃ©finir une palette de formes personnalisÃ©e
custom_shapes <- c(
  "Predomics Terbeam & Terga1 unconstrained" = 16, 
  "Predomics Terbeam & Terga1 constrained" = 4,   
  "SOTA" = 19,   
  "Terbeam unconstrained" = 17,   
  "Terga1 unconstrained" = 18,   
  "Terbeam constrained" = 15,   
  "Terga1 constrained" = 8
)

# CrÃ©er et enregistrer le graphique en format PDF avec disposition paysage
pdf("accuracy_plot.pdf", width = 11, height = 10)

df_summary %>%
  ggplot(aes(x = Methods, y = mean_value, color = Subgroup, shape = Subgroup)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value), 
                position = position_dodge(width = 0.5), width = 0.2) +
  facet_grid(Dataset ~ Group, scales = "free") +
  ylab("Accuracy") +
  xlab("Methods") +
  theme_bw() +
  scale_colour_manual(values = custom_colors) +
  scale_shape_manual(values = custom_shapes) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )

# Fermer le fichier PDF
dev.off()

```







```{r}
# Chargement des librairies nÃ©cessaires
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)

# Fusionner les quatre dataframes en ajoutant une colonne pour diffÃ©rencier les datasets
analysis_table_no_balance <- analysis_table_final_complet_no_balance %>%
  mutate(Dataset = "Enterotype no_balanced")

analysis_table_balance <- analysis_table_final_complet_balance %>%
  mutate(Dataset = "Enterotype balanced")

analysis_table_no_balance_2 <- analysis_table_final_complet_no_balance_2 %>%
  mutate(Dataset = "CRC, Adenoma, Control")

analysis_table_no_balance_3 <- analysis_table_final_complet_no_balance_3 %>%
  mutate(Dataset = "T2D, IGT, Control ")

# Combiner les quatre tables
df_combined <- bind_rows(analysis_table_no_balance, 
                         analysis_table_balance, 
                         analysis_table_no_balance_2, 
                         analysis_table_no_balance_3)

# Imputation des valeurs manquantes pour les colonnes numÃ©riques
df_combined <- df_combined %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Renommer "Features" en "k"
df_combined <- df_combined %>%
  rename(k = Features)

# Convertir en format long
df_long <- df_combined %>%
  pivot_longer(
    cols = c("Accuracy.empirique", "Accuracy.generalization"),  # Se limiter aux colonnes de prÃ©cision
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>%
  mutate(
    Partition = recode(Partition, 'empirique' = 'train', 'generalization' = 'test'),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Predomics Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Predomics Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Predomics Terga1 constrained",
      TRUE ~ "SOTA"
    ),
    Subgroup = case_when(
      str_starts(Methods, "terbeam") ~ "Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Terga1 constrained",
      TRUE ~ "SOTA"
    )
  )

# Regrouper "Predomics Terbeam unconstrained" et "Predomics Terga1 unconstrained"
df_long$Group <- recode(df_long$Group,
                        "Predomics Terbeam unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terga1 unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terbeam constrained" = "Predomics Terbeam & Terga1 constrained", 
                        "Predomics Terga1 constrained" = "Predomics Terbeam & Terga1 constrained")

# Filtrer pour ne garder que la partition "test" et la mÃ©trique "Accuracy.generalization"
df_accuracy <- df_long %>% 
  filter(Metric == "Accuracy" & Partition == "test")

# AgrÃ©ger les donnÃ©es pour le rÃ©sumÃ©
df_summary <- df_accuracy %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  )

# Supprimer les lignes avec des valeurs manquantes ou vides dans Metric ou Group
df_summary <- df_summary %>%
  filter(!is.na(Metric) & !is.na(Group) & Metric != "" & Group != "")

# Suppression des prÃ©fixes et des suffixes dans l'affichage des mÃ©thodes
df_summary <- df_summary %>%
  mutate(Methods = str_remove_all(Methods, "^(terbeam_|_terbeam_|terga1_|_terga1_)|(_no_balance|_balance)$"))

# Supprimer les doublons de mÃ©thodes aprÃ¨s la suppression des prÃ©fixes et des suffixes
df_summary <- df_summary %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(mean_value, na.rm = TRUE),
    sd_value = mean(sd_value, na.rm = TRUE),
    .groups = "drop"
  )

# Trouver la valeur maximale d'accuracy pour chaque dataset et group
df_summary <- df_summary %>%
  group_by(Dataset, Group) %>%
  mutate(max_accuracy = max(mean_value, na.rm = TRUE)) %>%
  ungroup()

# Ajouter une colonne pour marquer les points avec l'accuracy maximale
df_summary <- df_summary %>%
  mutate(is_max_accuracy = ifelse(mean_value == max_accuracy, TRUE, FALSE))

# DÃ©finir une palette de couleurs personnalisÃ©e
custom_colors <- c(
  "Predomics Terbeam & Terga1 unconstrained" = "green", 
  "Predomics Terbeam & Terga1 constrained" = "blue", 
  "SOTA" = "black", 
  "Terbeam unconstrained" = "darkgreen", 
  "Terga1 unconstrained" = "darkblue", 
  "Terbeam constrained" = "lightgreen", 
  "Terga1 constrained" = "lightblue"
)

# DÃ©finir une palette de formes personnalisÃ©e
custom_shapes <- c(
  "Predomics Terbeam & Terga1 unconstrained" = 16, 
  "Predomics Terbeam & Terga1 constrained" = 4,   
  "SOTA" = 19,   
  "Terbeam unconstrained" = 17,   
  "Terga1 unconstrained" = 18,   
  "Terbeam constrained" = 15,   
  "Terga1 constrained" = 8
)

# CrÃ©er et enregistrer le graphique en format PDF avec disposition paysage
pdf("accuracy_plot_with_stars.pdf", width = 11, height = 10)

df_summary %>%
  ggplot(aes(x = Methods, y = mean_value, color = Subgroup, shape = Subgroup)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value), 
                position = position_dodge(width = 0.5), width = 0.2) +
  geom_point(data = subset(df_summary, is_max_accuracy == TRUE), 
             aes(x = Methods, y = mean_value), 
             shape = 8, color = "red", size = 5) + # Ajout d'une Ã©toile pour la valeur maximale
  facet_grid(Dataset ~ Group, scales = "free") +
  ylab("Accuracy.generalization") +
  xlab("Methods") +
  theme_bw() +
  scale_colour_manual(values = custom_colors) +
  scale_shape_manual(values = custom_shapes) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )

# Fermer le fichier PDF
dev.off()

```









```{r}
y_T2D <- y
X_T2D <- X


```



```{r}
# Sauvegarder la variable y dans un fichier CSV
write.csv(y_T2D, file = "y_T2D.csv", row.names = FALSE)

# Sauvegarder la variable X dans un fichier CSV
write.csv(X_T2D, file = "X_T2D.csv", row.names = FALSE)


```

```{r}
df_CRC <- data.frame(y = y_repeated, X)
```



```{r warning=FALSE, fig.height=10, fig.width=12, message=FALSE, warning=FALSE, paged.print=FALSE}
# Fusionner les quatre dataframes en ajoutant une colonne pour diffÃ©rencier les datasets
analysis_table_no_balance_ <-analysis_table_final_complet_no_balance_ %>%
  mutate(Dataset = "Enterotype no_balanced")

analysis_table_balance_ <- analysis_table_final_complet_balance_ %>%
  mutate(Dataset = "Enterotype balanced")

analysis_table_no_balance_2_ <- analysis_table_final_complet_no_balance_2_ %>%
  mutate(Dataset = "CRC, Adenoma, Control")

analysis_table_no_balance_3_ <- analysis_table_final_complet_no_balance_3_ %>%
  mutate(Dataset = "T2D, IGT, Control ")

# Combiner les quatre tables
df_combined <- bind_rows(analysis_table_no_balance_, 
                         analysis_table_balance_, 
                         analysis_table_no_balance_2_, 
                         analysis_table_no_balance_3_)

# Imputation des valeurs manquantes pour les colonnes numÃ©riques
df_combined <- df_combined %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Renommer "Features" en "k"
df_combined <- df_combined %>%
  rename(k = Features)

# Convertir en format long
df_long <- df_combined %>%
  pivot_longer(
    cols = c("Accuracy.empirique", "Accuracy.generalization"),  # Se limiter aux colonnes de prÃ©cision
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>%
  mutate(
    Partition = recode(Partition, 'empirique' = 'train', 'generalization' = 'test'),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Predomics Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Predomics Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Predomics Terga1 constrained",
      TRUE ~ "SOTA"
    ),
    Subgroup = case_when(
      str_starts(Methods, "terbeam") ~ "Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Terga1 constrained",
      TRUE ~ "SOTA"
    )
  )

# Regrouper "Predomics Terbeam unconstrained" et "Predomics Terga1 unconstrained"
df_long$Group <- recode(df_long$Group,
                        "Predomics Terbeam unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terga1 unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terbeam constrained" = "Predomics Terbeam & Terga1 constrained", 
                        "Predomics Terga1 constrained" = "Predomics Terbeam & Terga1 constrained")

# Filtrer pour ne garder que la partition "test" et la mÃ©trique "Accuracy.generalization"
df_accuracy <- df_long %>% 
  filter(Metric == "Accuracy" & Partition == "test")

# AgrÃ©ger les donnÃ©es pour le rÃ©sumÃ©
df_summary <- df_accuracy %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  )

# Supprimer les lignes avec des valeurs manquantes ou vides dans Metric ou Group
df_summary <- df_summary %>%
  filter(!is.na(Metric) & !is.na(Group) & Metric != "" & Group != "")

# Suppression des prÃ©fixes et des suffixes dans l'affichage des mÃ©thodes
df_summary <- df_summary %>%
  mutate(Methods = str_remove_all(Methods, "^(terbeam_|_terbeam_|terga1_|_terga1_)|(_no_balance|_balance)$"))

# Supprimer les doublons de mÃ©thodes aprÃ¨s la suppression des prÃ©fixes et des suffixes
df_summary <- df_summary %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(mean_value, na.rm = TRUE),
    sd_value = mean(sd_value, na.rm = TRUE),
    .groups = "drop"
  )

# Trouver la valeur maximale d'accuracy pour chaque dataset et group
df_summary <- df_summary %>%
  group_by(Dataset, Group) %>%
  mutate(max_accuracy = max(mean_value, na.rm = TRUE)) %>%
  ungroup()

# Ajouter une colonne pour marquer les points avec l'accuracy maximale
df_summary <- df_summary %>%
  mutate(is_max_accuracy = ifelse(mean_value == max_accuracy, TRUE, FALSE))

# DÃ©finir une palette de couleurs personnalisÃ©e
custom_colors <- c(
  "Predomics Terbeam & Terga1 unconstrained" = "green", 
  "Predomics Terbeam & Terga1 constrained" = "blue", 
  "SOTA" = "black", 
  "Terbeam unconstrained" = "darkgreen", 
  "Terga1 unconstrained" = "darkblue", 
  "Terbeam constrained" = "lightgreen", 
  "Terga1 constrained" = "lightblue"
)

# DÃ©finir une palette de formes personnalisÃ©e
custom_shapes <- c(
  "Predomics Terbeam & Terga1 unconstrained" = 16, 
  "Predomics Terbeam & Terga1 constrained" = 4,   
  "SOTA" = 19,   
  "Terbeam unconstrained" = 17,   
  "Terga1 unconstrained" = 18,   
  "Terbeam constrained" = 15,   
  "Terga1 constrained" = 8
)

# CrÃ©er et afficher le graphique sans gÃ©nÃ©rer un fichier PDF
df_summary %>%
  ggplot(aes(x = Methods, y = mean_value, color = Subgroup, shape = Subgroup)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value), 
                position = position_dodge(width = 0.5), width = 0.2) +
  geom_point(data = subset(df_summary, is_max_accuracy == TRUE), 
             aes(x = Methods, y = mean_value), 
             shape = 8, color = "red", size = 5) + # Ajout d'une Ã©toile pour la valeur maximale
  facet_grid(Dataset ~ Group, scales = "free") +
  ylab("Accuracy.generalization") +
  xlab("Methods") +
  theme_bw() +
  scale_colour_manual(values = custom_colors) +
  scale_shape_manual(values = custom_shapes) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )
```

```{r}
# Fusionner les quatre dataframes en ajoutant une colonne pour diffÃ©rencier les datasets
analysis_table_no_balance <- Metacardis_no_balanced_results %>%
  mutate(Dataset = "Enterotype no_balanced")

analysis_table_balance <- Metacardis_balanced_results %>%
  mutate(Dataset = "Enterotype balanced")

analysis_table_no_balance_2 <- CRC_Adenoma_Control_results %>%
  mutate(Dataset = "CRC, Adenoma, Control")

analysis_table_no_balance_3 <- T2D_IGT_Control_results %>%
  mutate(Dataset = "T2D, IGT, Control ")

# Combiner les quatre tables
df_combined <- bind_rows(analysis_table_no_balance, 
                         analysis_table_balance, 
                         analysis_table_no_balance_2, 
                         analysis_table_no_balance_3)

# Imputation des valeurs manquantes pour les colonnes numÃ©riques
df_combined <- df_combined %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Renommer "Features" en "k"
df_combined <- df_combined %>%
  rename(k = Features)

# Convertir en format long
df_long <- df_combined %>%
  pivot_longer(
    cols = c("Accuracy.empirique", "Accuracy.generalization"),  # Se limiter aux colonnes de prÃ©cision
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>%
  mutate(
    Partition = recode(Partition, 'empirique' = 'train', 'generalization' = 'test'),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Predomics Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Predomics Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Predomics Terga1 constrained",
      TRUE ~ "SOTA"
    ),
    Subgroup = case_when(
      str_starts(Methods, "terbeam") ~ "Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Terga1 constrained",
      TRUE ~ "SOTA"
    )
  )

# Regrouper "Predomics Terbeam unconstrained" et "Predomics Terga1 unconstrained"
df_long$Group <- recode(df_long$Group,
                        "Predomics Terbeam unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terga1 unconstrained" = "Predomics Terbeam & Terga1 unconstrained", 
                        "Predomics Terbeam constrained" = "Predomics Terbeam & Terga1 constrained", 
                        "Predomics Terga1 constrained" = "Predomics Terbeam & Terga1 constrained")

# Filtrer pour ne garder que la partition "test" et la mÃ©trique "Accuracy.generalization"
df_accuracy <- df_long %>% 
  filter(Metric == "Accuracy" & Partition == "test")

# AgrÃ©ger les donnÃ©es pour le rÃ©sumÃ©
df_summary <- df_accuracy %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  )

# Supprimer les lignes avec des valeurs manquantes ou vides dans Metric ou Group
df_summary <- df_summary %>%
  filter(!is.na(Metric) & !is.na(Group) & Metric != "" & Group != "")

# Suppression des prÃ©fixes et des suffixes dans l'affichage des mÃ©thodes
df_summary <- df_summary %>%
  mutate(Methods = str_remove_all(Methods, "^(terbeam_|_terbeam_|terga1_|_terga1_)|(_no_balance|_balance)$"))

# Supprimer les doublons de mÃ©thodes aprÃ¨s la suppression des prÃ©fixes et des suffixes
df_summary <- df_summary %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(mean_value, na.rm = TRUE),
    sd_value = mean(sd_value, na.rm = TRUE),
    .groups = "drop"
  )

# Trouver la valeur maximale d'accuracy pour chaque dataset et group
df_summary <- df_summary %>%
  group_by(Dataset, Group) %>%
  mutate(max_accuracy = max(mean_value, na.rm = TRUE)) %>%
  ungroup()

# Ajouter une colonne pour marquer les points avec l'accuracy maximale
df_summary <- df_summary %>%
  mutate(is_max_accuracy = ifelse(mean_value == max_accuracy, TRUE, FALSE))

# DÃ©finir une palette de couleurs personnalisÃ©e
custom_colors <- c(
  "Predomics Terbeam & Terga1 unconstrained" = "green", 
  "Predomics Terbeam & Terga1 constrained" = "blue", 
  "SOTA" = "black", 
  "Terbeam unconstrained" = "darkgreen", 
  "Terga1 unconstrained" = "darkblue", 
  "Terbeam constrained" = "lightgreen", 
  "Terga1 constrained" = "lightblue"
)

# DÃ©finir une palette de formes personnalisÃ©e
custom_shapes <- c(
  "Predomics Terbeam & Terga1 unconstrained" = 16, 
  "Predomics Terbeam & Terga1 constrained" = 4,   
  "SOTA" = 19,   
  "Terbeam unconstrained" = 17,   
  "Terga1 unconstrained" = 18,   
  "Terbeam constrained" = 15,   
  "Terga1 constrained" = 8
)

# CrÃ©er et enregistrer le graphique en format PDF avec disposition paysage
pdf("accuracy_plot_with_stars_.pdf", width = 11, height = 10)

df_summary %>%
  ggplot(aes(x = Methods, y = mean_value, color = Subgroup, shape = Subgroup)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value), 
                position = position_dodge(width = 0.5), width = 0.2) +
  geom_point(data = subset(df_summary, is_max_accuracy == TRUE), 
             aes(x = Methods, y = mean_value), 
             shape = 8, color = "red", size = 5) + # Ajout d'une Ã©toile pour la valeur maximale
  facet_grid(Dataset ~ Group, scales = "free") +
  ylab("Accuracy.generalization") +
  xlab("Methods") +
  theme_bw() +
  scale_colour_manual(values = custom_colors) +
  scale_shape_manual(values = custom_shapes) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )

# Fermer le fichier PDF
dev.off()
```















# Regrouper Predomics


```{r warning=FALSE, fig.height=10, fig.width=12, message=FALSE, warning=FALSE, paged.print=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)

# Fusionner les quatre dataframes en ajoutant une colonne pour diffÃ©rencier les datasets
analysis_table_no_balance_ <- analysis_table_final_complet_no_balance_ %>%
  mutate(Dataset = "Enterotype no_balanced")

analysis_table_balance_ <- analysis_table_final_complet_balance_ %>%
  mutate(Dataset = "Enterotype balanced")

analysis_table_no_balance_2_ <- analysis_table_final_complet_no_balance_2_ %>%
  mutate(Dataset = "CRC, Adenoma, Control")

analysis_table_no_balance_3_ <- analysis_table_final_complet_no_balance_3_ %>%
  mutate(Dataset = "T2D, IGT, Control")

# Combiner les quatre tables
df_combined <- bind_rows(
  analysis_table_no_balance_,
  analysis_table_balance_,
  analysis_table_no_balance_2_,
  analysis_table_no_balance_3_
)

# Imputation des valeurs manquantes pour les colonnes numÃ©riques
df_combined <- df_combined %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Renommer "Features" en "k"
df_combined <- df_combined %>%
  rename(k = Features)

# Convertir en format long
df_long <- df_combined %>%
  pivot_longer(
    cols = c("Accuracy.empirique", "Accuracy.generalization"),
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>%
  mutate(
    Partition = recode(Partition, 'empirique' = 'train', 'generalization' = 'test'),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics",
      str_starts(Methods, "_terbeam") ~ "Predomics",
      str_starts(Methods, "terga1") ~ "Predomics",
      str_starts(Methods, "_terga1") ~ "Predomics",
      TRUE ~ "SOTA"
    ),
    Subgroup = case_when(
      str_starts(Methods, "terbeam") ~ "Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Terga1 constrained",
      TRUE ~ "SOTA"
    )
  )

# Filtrer pour ne garder que la partition "test" et la mÃ©trique "Accuracy.generalization"
df_accuracy <- df_long %>%
  filter(Metric == "Accuracy" & Partition == "test")

# AgrÃ©ger les donnÃ©es pour le rÃ©sumÃ©
df_summary <- df_accuracy %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  )

# Nettoyer les noms des mÃ©thodes
df_summary <- df_summary %>%
  mutate(Methods = str_remove_all(Methods, "^(terbeam_|_terbeam_|terga1_|_terga1_)|(_no_balance|_balance)$"))

# Supprimer les doublons de mÃ©thodes aprÃ¨s nettoyage
df_summary <- df_summary %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(mean_value, na.rm = TRUE),
    sd_value = mean(sd_value, na.rm = TRUE),
    .groups = "drop"
  )

# Trouver la valeur maximale d'accuracy pour chaque dataset et group
df_summary <- df_summary %>%
  group_by(Dataset, Group) %>%
  mutate(max_accuracy = max(mean_value, na.rm = TRUE)) %>%
  ungroup()

# Marquer les points avec l'accuracy maximale
df_summary <- df_summary %>%
  mutate(is_max_accuracy = ifelse(mean_value == max_accuracy, TRUE, FALSE))

# Palette de couleurs personnalisÃ©e
custom_colors <- c(
  "Predomics" = "blue",
  "SOTA" = "black",
  "Terbeam unconstrained" = "darkgreen",
  "Terga1 unconstrained" = "darkblue",
  "Terbeam constrained" = "lightgreen",
  "Terga1 constrained" = "lightblue",
  "SOTA" = "black"
)

# Formes personnalisÃ©es
custom_shapes <- c(
  "Terbeam unconstrained" = 17,
  "Terga1 unconstrained" = 18,
  "Terbeam constrained" = 15,
  "Terga1 constrained" = 8,
  "SOTA" = 19
)

# CrÃ©ation du graphique
df_summary %>%
  ggplot(aes(x = Methods, y = mean_value, color = Subgroup, shape = Subgroup)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value),
                position = position_dodge(width = 0.5), width = 0.2) +
  geom_point(data = subset(df_summary, is_max_accuracy == TRUE),
             aes(x = Methods, y = mean_value),
             shape = 8, color = "red", size = 5) +  # Ã‰toile rouge pour max
  facet_grid(Dataset ~ Group, scales = "free") +
  ylab("Accuracy.generalization") +
  xlab("Methods") +
  theme_bw() +
  scale_colour_manual(values = custom_colors) +
  scale_shape_manual(values = custom_shapes) +
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
    legend.position = "bottom"
  )


```



## PDF



```{r}

library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)

# Fusionner les quatre dataframes en ajoutant une colonne pour diffÃ©rencier les datasets
analysis_table_no_balance_1 <- analysis_table_final_complet_no_balance_Metacardis %>%
  mutate(Dataset = "Metacardis Enterotype\nno balanced")

analysis_table_balance_2 <- analysis_table_final_complet_balance_Metacardis %>%
  mutate(Dataset = "Metacardis Enterotype\nbalanced")

analysis_table_no_balance_3 <- analysis_table_final_complet_no_balance_CRC %>%
  mutate(Dataset = "Study on CRC\nno balanced")

analysis_table_no_balance_4 <- analysis_table_final_complet_no_balance_TD2 %>%
  mutate(Dataset = "Study on T2D\nno balanced")

# Combiner les quatre tables
df_combined <- bind_rows(
  analysis_table_no_balance_1,
  analysis_table_balance_2,
  analysis_table_no_balance_3,
  analysis_table_no_balance_4
)

# Imputation des valeurs manquantes pour les colonnes numÃ©riques
df_combined <- df_combined %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Renommer "Features" en "k"
df_combined <- df_combined %>% rename(k = Features)

# Convertir en format long
df_long <- df_combined %>%
  pivot_longer(
    cols = c("Accuracy.empirique", "Accuracy.generalization"),
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>%
  mutate(
    Partition = recode(Partition, 'empirique' = 'train', 'generalization' = 'test'),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics",
      str_starts(Methods, "_terbeam") ~ "Predomics",
      str_starts(Methods, "terga1") ~ "Predomics",
      str_starts(Methods, "_terga1") ~ "Predomics",
      TRUE ~ "SOTA"
    ),
    Subgroup = case_when(
      str_starts(Methods, "terbeam") ~ "Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Terga1 constrained",
      TRUE ~ "SOTA"
    )
  )

# Filtrer pour ne garder que la partition "test" et la mÃ©trique "Accuracy"
df_accuracy <- df_long %>%
  filter(Metric == "Accuracy" & Partition == "test")

# RÃ©sumÃ© statistique
df_summary <- df_accuracy %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  )

# Nettoyage des noms de mÃ©thodes
df_summary <- df_summary %>%
  mutate(Methods = str_remove_all(Methods, "^(terbeam_|_terbeam_|terga1_|_terga1_)|(_no_balance|_balance)$")) %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(mean_value, na.rm = TRUE),
    sd_value = mean(sd_value, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  group_by(Dataset, Group) %>%
  mutate(max_accuracy = max(mean_value, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(is_max_accuracy = ifelse(mean_value == max_accuracy, TRUE, FALSE))

# Palette et formes personnalisÃ©es
custom_colors <- c(
  "Predomics" = "blue",
  "SOTA" = "black",
  "Terbeam unconstrained" = "darkgreen",
  "Terga1 unconstrained" = "darkblue",
  "Terbeam constrained" = "lightgreen",
  "Terga1 constrained" = "lightblue",
  "SOTA" = "black"
)

custom_shapes <- c(
  "Terbeam unconstrained" = 17,
  "Terga1 unconstrained" = 18,
  "Terbeam constrained" = 15,
  "Terga1 constrained" = 8,
  "SOTA" = 19
)

# Appliquer un retour Ã  la ligne dans les noms des mÃ©thodes uniquement pour le groupe Predomics
df_summary <- df_summary %>%
  mutate(Methods = ifelse(Group == "Predomics" & nchar(Methods) > 20, 
                          str_wrap(Methods, width = 20), 
                          Methods)) %>%
  # Forcer le retour Ã  la ligne avant "unconstrained" et "constrained"
  mutate(Methods = ifelse(Group == "Predomics", 
                          str_replace_all(Methods, 
                                          c("unconstrained" = "\nunconstrained", 
                                            "constrained" = "\nconstrained")), 
                          Methods))

# CrÃ©ation du graphique dans un PDF paysage
pdf("accuracy_generalization_plot.pdf", width = 14, height = 9, paper = "special")

ggplot(df_summary, aes(x = Methods, y = mean_value, color = Subgroup, shape = Subgroup)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value),
                position = position_dodge(width = 0.5), width = 0.2) +
  geom_point(data = subset(df_summary, is_max_accuracy == TRUE),
             aes(x = Methods, y = mean_value),
             shape = 8, color = "red", size = 5) +  # Ã‰toile rouge pour max
  facet_grid(Dataset ~ Group, scales = "free") +
  ylab("accuracy (CV generalization)") +
  xlab("Methods") +
  theme_bw() +
  scale_colour_manual(values = custom_colors) +
  scale_shape_manual(values = custom_shapes) +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
    legend.position = "bottom"
  )

dev.off()


```








```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)

# Fusionner les quatre dataframes en ajoutant une colonne pour diffÃ©rencier les datasets
analysis_table_no_balance_1 <- analysis_table_final_complet_no_balance_Metacardis %>%
  mutate(Dataset = "Metacardis Enterotype\nno balanced")

analysis_table_balance_2 <- analysis_table_final_complet_balance_Metacardis %>%
  mutate(Dataset = "Metacardis Enterotype\nbalanced")

analysis_table_no_balance_3 <- analysis_table_final_complet_no_balance_CRC %>%
  mutate(Dataset = "Study on CRC\nno balanced")

analysis_table_no_balance_4 <- analysis_table_final_complet_no_balance_TD2 %>%
  mutate(Dataset = "Study on T2D\nno balanced")

# Combiner les quatre tables
df_combined <- bind_rows(
  analysis_table_no_balance_1,
  analysis_table_balance_2,
  analysis_table_no_balance_3,
  analysis_table_no_balance_4
)

# Imputation des valeurs manquantes pour les colonnes numÃ©riques
df_combined <- df_combined %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

# Renommer "Features" en "k"
df_combined <- df_combined %>% rename(k = Features)

# Convertir en format long
df_long <- df_combined %>%
  pivot_longer(
    cols = c("Accuracy.empirique", "Accuracy.generalization"),
    names_to = c("Metric", "Partition"),
    names_sep = "\\.",
    values_to = "Value"
  ) %>%
  mutate(
    Partition = recode(Partition, 'empirique' = 'train', 'generalization' = 'test'),
    Partition = factor(Partition, levels = c("train", "test")),
    Value = as.numeric(Value),
    Group = case_when(
      str_starts(Methods, "terbeam") ~ "Predomics",
      str_starts(Methods, "_terbeam") ~ "Predomics",
      str_starts(Methods, "terga1") ~ "Predomics",
      str_starts(Methods, "_terga1") ~ "Predomics",
      TRUE ~ "SOTA"
    ),
    Subgroup = case_when(
      str_starts(Methods, "terbeam") ~ "Terbeam unconstrained",
      str_starts(Methods, "_terbeam") ~ "Terbeam constrained",
      str_starts(Methods, "terga1") ~ "Terga1 unconstrained",
      str_starts(Methods, "_terga1") ~ "Terga1 constrained",
      TRUE ~ "SOTA"
    )
  )

# Filtrer pour ne garder que la partition "test" et la mÃ©trique "Accuracy"
df_accuracy <- df_long %>%
  filter(Metric == "Accuracy" & Partition == "test")

# RÃ©sumÃ© statistique
df_summary <- df_accuracy %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(Value, na.rm = TRUE),
    sd_value = sd(Value, na.rm = TRUE),
    .groups = "drop"
  )

# Nettoyage des noms de mÃ©thodes
df_summary <- df_summary %>%
  mutate(Methods = str_remove_all(Methods, "^(terbeam_|_terbeam_|terga1_|_terga1_)|(_no_balance|_balance)$")) %>%
  group_by(Methods, Partition, Metric, k, Subgroup, Group, Dataset) %>%
  summarize(
    mean_value = mean(mean_value, na.rm = TRUE),
    sd_value = mean(sd_value, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  group_by(Dataset, Group) %>%
  mutate(max_accuracy = max(mean_value, na.rm = TRUE)) %>%
  ungroup() %>%
  mutate(is_max_accuracy = ifelse(mean_value == max_accuracy, TRUE, FALSE))

# Ajouter une colonne pour la significativitÃ©
df_summary <- df_summary %>%
  mutate(is_significant = ifelse(mean_value > 0.8, "*", NA))  # Exemple avec un seuil de 0.8

# Palette et formes personnalisÃ©es
custom_colors <- c(
  "Predomics" = "blue",
  "SOTA" = "black",
  "Terbeam unconstrained" = "darkgreen",
  "Terga1 unconstrained" = "darkblue",
  "Terbeam constrained" = "lightgreen",
  "Terga1 constrained" = "lightblue",
  "SOTA" = "black"
)

custom_shapes <- c(
  "Terbeam unconstrained" = 17,
  "Terga1 unconstrained" = 18,
  "Terbeam constrained" = 15,
  "Terga1 constrained" = 8,
  "SOTA" = 19
)

# Appliquer un retour Ã  la ligne dans les noms des mÃ©thodes uniquement pour le groupe Predomics
df_summary <- df_summary %>%
  mutate(Methods = ifelse(Group == "Predomics" & nchar(Methods) > 20, 
                          str_wrap(Methods, width = 20), 
                          Methods)) %>%
  # Forcer le retour Ã  la ligne avant "unconstrained" et "constrained"
  mutate(Methods = ifelse(Group == "Predomics", 
                          str_replace_all(Methods, 
                                          c("unconstrained" = "\nunconstrained", 
                                            "constrained" = "\nconstrained")), 
                          Methods))

# CrÃ©ation du graphique dans un PDF paysage
pdf("accuracy_generalization_plot.pdf", width = 14, height = 9, paper = "special")

ggplot(df_summary, aes(x = Methods, y = mean_value, color = Subgroup, shape = Subgroup)) +
  geom_point(position = position_dodge(width = 0.5), size = 4) +
  geom_errorbar(aes(ymin = mean_value - sd_value, ymax = mean_value + sd_value),
                position = position_dodge(width = 0.5), width = 0.2) +
  geom_point(data = subset(df_summary, is_max_accuracy == TRUE),
             aes(x = Methods, y = mean_value),
             shape = 8, color = "red", size = 5) +  # Ã‰toile rouge pour max
  geom_text(aes(label = is_significant), vjust = -1, color = "black", size = 5) +  # Ajouter les Ã©toiles
  facet_grid(Dataset ~ Group, scales = "free") +
  ylab("accuracy (CV generalization)") +
  xlab("Methods") +
  theme_bw() +
  scale_colour_manual(values = custom_colors) +
  scale_shape_manual(values = custom_shapes) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1), 
        legend.position = "bottom")

dev.off()



```











