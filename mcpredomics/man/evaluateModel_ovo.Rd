% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/globale_ovo.R, R/ovoExample.R
\name{evaluateModel_ovo}
\alias{evaluateModel_ovo}
\title{Evaluates the fitting score of a model object}
\usage{
evaluateModel_ovo(
  mod,
  X,
  y,
  clf,
  eval.all = FALSE,
  force.re.evaluation = FALSE,
  estim.feat.importance = FALSE,
  mode = "train"
)

evaluateModel_ovo(
  mod,
  X,
  y,
  clf,
  eval.all = FALSE,
  force.re.evaluation = FALSE,
  estim.feat.importance = FALSE,
  mode = "train"
)
}
\arguments{
\item{mod:}{a model object ovo}

\item{X:}{the data matrix with variables in the rows and observations in the columns}

\item{y:}{the response vector}

\item{clf:}{the classifier parameter object}

\item{eval.all:}{should the function evaluate all the scores (default:FALSE)}

\item{force.re.evaluation:}{re-evaluate all the scores even if they exist (default:FALSE)}

\item{estim.feat.importance:}{evaluate the importance in the model object (default:FALSE)}

\item{mode:}{A choice from c("train", "test") indicates wether we wish to learn the threthold
of the model (default:"train") or not "test" for the c("terinter","bininter","ratio") languages}
}
\value{
a list model object with the fitting scores evaluated

a model object with the fitting scores evaluated
}
\description{
Evaluates the fitting score of a model object one versus one.

Evaluates the fitting score of a model object ovo.
}
