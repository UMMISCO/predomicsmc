---
title: "testee"
author: "Fabien"
date: "2023-06-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Multiclass predomics

This document explains the method used to do multiclass classification with predomics.
To do the classification with the N-class instance dataset, I generated the N-binary classifier models using the one against all method and then merged them to form a single model at the output. For example, for a dataset of instances of classes 3 (A, B and C); I created three classifiers represented by the following equations:
classifier 1 = A + B - C - β > 0.
classifier 2 = A + C - B - β > 0.
classifier 3 = B + C - A - β > 0.

For the final multi-class classifier, I calculated the average of the metrics of all the N-binary classifiers.
final classifier = average(classifier 1, classifier 2, classifier 3).



```{r load the packages, message=FALSE, warning=FALSE}
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
```




```{r loading filtering the data, message=FALSE, warning=FALSE, paged.print=FALSE}
setwd("~/Documents/multiclasse_predomics/mcpredomics/data")
load(file = "mc.input.rda")
mcinput <- mc.input
str(mcinput, max.level = 1)
# Filter the non informative variables
X <- mcinput$X; y <- mcinput$y # set global variables
X <- X[rowSums(X)!=0,]; dim(X) # filter out variables with only zero values
X <- filterNoSignal(X = X, side = 1, threshold = "auto", verbose = FALSE); dim(X) 

```

```{r setting the classifier}
clf <- terga1_ovo(nCores = 1, 
              seed = 1, 
              plot = TRUE
)
printy(clf) # print the object for more information
isClf(clf)  # test whether the object is a classifier
class(clf)  # the class of the classifier object

```






```{r setting the classifier}
# Terga13
clf <- terga1(nCores = 1, 
              seed = 1, 
              plot = TRUE
)
printy(clf) # print the object for more information
isClf(clf)  # test whether the object is a classifier
class(clf)  # the class of the classifier object
```

# Preparing a dataset with 3 classes

```{r}
y <-  y[, 1]

y <- as.vector(y)
```



```{r}
  nClasse <- unique(y)
 clf$data          <- list()
  list_y <- list()
  list_X <- list()
  list_features <- list()
  k <- 1
  y <- as.vector(y)

  for (i in 1:(length(nClasse)-1)) {
    for (j in (i+1):length(nClasse)) {
      class_i <- nClasse[i]
      class_j <- nClasse[j]
      indices <- which(y == class_i | y == class_j)
      y_pair <- y[indices]
      X_pair <- X[,indices]
      list_y[[k]] <- y_pair
      list_X[[k]] <- X_pair
      list_features[[k]] <- as.character(rownames(X_pair))
      k <- k + 1
    }
  }
```

```{r}
y <- y[-yi]
y
```
```{r}
X <- X[,-yi]
length(X)
```


#Executing the fit function (fit_mc) with a 3-class dataset

```{r running experiment, echo=TRUE, fig.width=5,warning=FALSE}
runit = TRUE
if(runit)
{
res.clf <- fit_ova(X, y, clf, cross.validate = FALSE, nfolds = 1)
save(res.clf, clf, file = "fite_terga1ova.rda", compression_level = 9)
}

# ... Database X is not a matrix! Converting ...
# ... Classification mode, computing factor(y) for speedup and robustness
# ... Loading feature correlation for speedup
# ... Correlation file loaded
# ... Storing data in the classifier object for speedup
# ... Computing ternary coefficients for speedup
# ... One seed found, setting by default
# ... Running the classifier terga1 with a single CPU
# ... Second and faster version of terga fitting based on Genetic Algorithm heuristics ...
# ... Cross validation mode
# ... Starting cross validation not in parallel
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ... Learning process is finished succesfuly
# ... Thank you for using Predomics. Don't forget to digest the results now.
# 
# [1] "experiment" "predomics" 

```
# Display of multiclass model result for dataset with 3 classes

```{r load results, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}

```






# 4 class dataset preparation


```{r}
#ggggggg
y <-  y[, 1]
y <- as.vector(y)
```






```{r  loading filtering the data, message=FALSE, warning=FALSE, paged.print=FALSE}
setwd("~/Documents/multiclasse_predomics/mcpredomics/data")
load(file = "mc.input.rda")
mcinput <- mc.input
str(mcinput, max.level = 1)
# Filter the non informative variables
X <- mcinput$X; y <- mcinput$y # set global variables
X <- X[rowSums(X)!=0,]; dim(X) # filter out variables with only zero values
X <- filterNoSignal(X = X, side = 1, threshold = "auto", verbose = FALSE); dim(X) 
```












```{r}
runit = TRUE
if(runit)
{
res.clf <- fit_OVO(X, y, clf, cross.validate = FALSE, nfolds = 1)

}
```





```{r digesting results, fig.height=10, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALS}

res_clf.dig <- digest(obj = res.clf[[4]], penalty = 0.75/100, plot = TRUE)
```
```{r best model, fig.width=7, warning=FALSE}
# get the best model
best.model <- res_clf.dig$best$model
printy(best.model)
grid.arrange(plotModel(best.model, X, y, sort.features = FALSE, feature.name = TRUE),
             plotModel(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE),
             ncol=2)
```

```{r digesting results, fig.height=10, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALS}
res_clf.dig <- digest(obj = res.clf[[1]], penalty = 0.75/100, plot = TRUE)
```
```{r best model, fig.width=7, warning=FALSE}
# get the best model
best.model <- res_clf.dig$best$model
printy(best.model)
grid.arrange(plotModel(best.model, X, y, sort.features = FALSE, feature.name = TRUE),
             plotModel(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE),
             ncol=2)
```
```{r digesting results, fig.height=10, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALS}}
res_clf.dig <- digest(obj = res.clf[[2]], penalty = 0.75/100, plot = TRUE)
```
```{r best model, fig.width=7, warning=FALSE}
# get the best model
best.model <- res_clf.dig$best$model
printy(best.model)
grid.arrange(plotModel(best.model, X, y, sort.features = FALSE, feature.name = TRUE),
             plotModel(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE),
             ncol=2)
```
```{r digesting results, fig.height=10, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALS}
res_clf.dig <- digest(obj = res.clf[[3]], penalty = 0.75/100, plot = TRUE)
```

```{r best model, fig.width=7, warning=FALSE}
# get the best model
best.model <- res_clf.dig$best$model
printy(best.model)
grid.arrange(plotModel(best.model, X, y, sort.features = FALSE, feature.name = TRUE),
             plotModel(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE),
             ncol=2)
```
```{r digesting results, fig.height=10, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALS}
res_clf.dig <- digest(obj = res.clf[[5]], penalty = 0.75/100, plot = TRUE)
```

```{r best model, fig.width=7, warning=FALSE}
# get the best model
best.model <- res_clf.dig$best$model
printy(best.model)
grid.arrange(plotModel(best.model, X, y, sort.features = FALSE, feature.name = TRUE),
             plotModel(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE),
             ncol=2)
```

```{r digesting results, fig.height=10, fig.width=7, message=FALSE, warning=FALSE, paged.print=FALS}
res_clf.dig <- digest(obj = res.clf[[6]], penalty = 0.75/100, plot = TRUE)
```


```{r best model, fig.width=7, warning=FALSE}
# get the best model
best.model <- res_clf.dig$best$model
printy(best.model)
grid.arrange(plotModel(best.model, X, y, sort.features = FALSE, feature.name = TRUE),
             plotModel(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE),
             ncol=2)
```







```{r}

res <- evaluatePopulation(X=list_X[[i]], y=list_y[[i]], clf, pop, eval.all = TRUE,
                                     force.re.evaluation = TRUE)
```

