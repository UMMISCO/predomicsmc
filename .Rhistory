model$predictions <- predictions_list
model$pos_score_ <- lapply(mod, function(x) x$pos_score_)
model$neg_score_ <- lapply(mod, function(x) x$neg_score_)
model$confusionMatrix_ <- lapply(mod, function(x) x$confusionMatrix_)
mod <- list()
mod <- model
# Return mod predicted class labels for each combination, the corresponding score vectors, and the distances
return(mod)
}
# Function to aggregate one-versus-one predictions using majority voting
#' @title votingAggregation
#' @description Function to aggregate one-versus-one predictions using majority voting.
#' @param mod: The model object containing predictions and scores.
#' @return The function returns the model object with aggregated predictions.
#' @export
votingAggregation <- function(mod) {
predictions_list <- list()
predictions_list = mod$predictions
num_predictions <- length(predictions_list)  # Number of prediction vectors
num_samples <- length(predictions_list[[1]])  # Number of samples in one vector
aggregated_vector <- character(num_samples)  # Initialize aggregated vector
# Iterate over each sample
for (i in 1:num_samples) {
# Count votes for position i
votes <- table(sapply(predictions_list, `[`, i))
# Aggregate prediction for position i
aggregated_vector[i] <- names(sort(votes, decreasing = TRUE)[1])
}
mod$predictions_aggre <- aggregated_vector
return(mod)
}
#' This function performs weighted aggregation of predictions.
#' @title weightedAggregation
#' @description Function to aggregate one-versus-one predictions using weighted voting.
#' @param mod: The model object containing predictions and scores.
#' @return The function returns the model object with aggregated predictions.
#' @export
#'
weightedAggregation <- function(mod) {
predictions_list <- list()
scores_list <- list()
predictions_list <- mod$predictions
scores_list <- mod$scores_normalises_
num_predictions <- length(predictions_list)
num_samples <- length(predictions_list[[1]])
aggregated_vector <- character(num_samples)
# Iterate over each sample
for (i in 1:num_samples) {
# Extract scores for the current sample
class_scores <- sapply(scores_list, function(score_vector) score_vector[i])
# Extract predictions for the current sample
prediction_ <- sapply(predictions_list, function(prediction_vector) prediction_vector[i])
# Get unique classes in the predictions
classes <- unique(prediction_)
class_scores_sum <- numeric(length(classes))
# Calculate the sum of scores for each unique class
for (j in 1:length(classes)) {
class_ <- classes[j]
class_scores_sum[j] <- sum(scores_list[[j]][prediction_ == class_])
}
# Find the index of the class with the highest sum of scores
best_class_index <- which.max(class_scores_sum)
best_class <- classes[best_class_index]
# Assign the predicted class with the highest sum of scores to the aggregated vector
aggregated_vector[i] <- best_class
}
# Store the aggregated predictions in the model object
mod$predictions_aggre <- aggregated_vector
return(mod)
}
#' Aggregation function of one-versus-all predictions using search and score max.
#' @title Predomics_aggregation_ova
#' @description  Function to aggregate one-versus-all predictions using New approach.
#' @param mod: The model object containing predictions and scores.
#' @param y: true class.
#' @return The function returns the model object with aggregated predictions.
#' @export
Predomics_aggregation_ova <- function(mod, y) {
# Initialize the vector to hold the aggregated predictions with the appropriate length
classes_list <- mod$predictions
score_list <- mod$scores_normalises_
aggregated_predictions <- character(length = length(classes_list[[1]]))
y = as.vector(y)
# Create a character vector of the unique classes from y
names_class <- unique(y)
names_class <- as.character(names_class)
# Iterate over each position in the aggregated predictions
for (i in seq_along(aggregated_predictions)) {
# Extract the classes and scores for the current position from each list
current_classes <- sapply(classes_list, function(class_vector) class_vector[i])
scores <- sapply(score_list, function(score_vector) score_vector[i])
# Exclude 'ALL' and retrieve corresponding scores
valid_classes_indices <- which(current_classes != "ALL")
valid_classes <- current_classes[valid_classes_indices]
valid_scores <- scores[valid_classes_indices]
# If all scores are zero, randomly choose a class from the list of unique classes
if (all(scores == 0)) {
predicted_class <- sample(names_class, 1)
} else if (length(valid_classes) == 1) {
# If there is only one valid class, predict that class
predicted_class <- valid_classes
} else if (length(valid_classes) > 1) {
# If there are multiple valid classes, choose the class with the highest score
max_score_index <- which.max(valid_scores)
predicted_class <- valid_classes[max_score_index]
} else {
# If there are no valid classes (all were 'ALL'), choose the class from 'ALL' with the highest score
max_score <- max(scores)  # first, find the max score from the original scores
max_score_indices <- which(scores == max_score)  # then find all the indices with max score
predicted_class <- names_class[max_score_indices[1]]  # choose the class for the first index with max score
}
# Fill the aggregation vector at position i with the predicted class
aggregated_predictions[i] <- predicted_class
}
mod$predictions_aggre <- aggregated_predictions
# Return the final vector of aggregated predictions
return(mod)
}
#' Aggregation function of one-versus-all predictions using maximum score.
#' @title maximizationAggregation
#' @description  Function to aggregate one-versus-all predictions using maximization Aggregation.
#' @param mod: The model object containing predictions and scores.
#' @param y: True class.
#' @return  The function returns the model object with aggregated predictions.
#' @export
maximizationAggregation <- function(mod, y) {
# Initialize the vector to hold the aggregated predictions with the appropriate length
classes_list <- list()
score_list <- list()
classes_list <- mod$predictions
score_list <- mod$scores_normalises_
aggregated_predictions <- character(length = length(classes_list[[1]]))
y = as.vector(y)
current_classes <- unique(y)
# Iterate over each position in the aggregated predictions
for (i in seq_along(aggregated_predictions)) {
scores <- sapply(score_list, function(score_vector) score_vector[i])
# Find the index of the maximum score
max_score_index <- which.max(scores)
# Fill the aggregation vector at position i with the predicted class with the maximum score
aggregated_predictions[i] <- current_classes[max_score_index]
}
mod$predictions_aggre <- aggregated_predictions
return(mod)
}
#' Aggregation function of one-versus-all predictions using ranking score.
#' @title rankingAggregation
#' @description Function to aggregate one-versus-all predictions using ranking Aggregation.
#' @param mod: The model object containing predictions and scores.
#' @param y: True class.
#' @return  The function returns the model object with aggregated predictions.
#' @export
rankingAggregation <- function(mod, y) {
# Initialize the vector to hold the aggregated predictions with the appropriate length
classes_list <- list()
score_list <- list()
classes_list <- mod$predictions
score_list <- mod$scores_normalises_
aggregated_predictions <- character(length = length(classes_list[[1]]))
y = as.vector(y)
current_classes <- unique(y)
# Iterating over each position in the aggregated predictions
for (i in seq_along(aggregated_predictions)) {
# Extract the scores for the current position from each list
scores <- sapply(score_list, function(score_vector) score_vector[i])
# Ranking the scores and assigning the corresponding rank to each score
rank_scores <- rank(-scores, ties.method = "min")
# Selecting the class corresponding to the highest score
best_class_index <- which(rank_scores == 1)
aggregated_predictions[i] <- current_classes[best_class_index]
}
mod$predictions_aggre <- aggregated_predictions
return(mod)
}
#' This function evaluates the aggregated model's performance.
#' @title evaluateModel_aggregation
#' @param mod: The model object containing aggregated predictions and true labels.
#' @param y: The true class labels.
#' @return The function returns the model object with evaluation metrics.
#' @export
#'
evaluateModel_aggregation <- function(mod, y) {
# Calculation of the confusion matrix
confusion_matrix <- table(mod$predictions_aggre, y)
# Calculation of the overall accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# Initialization of variables for precision, recall, and F1 score
class_precision <- numeric(length = nrow(confusion_matrix))
class_recall <- numeric(length = nrow(confusion_matrix))
# Calculation of precision, recall, and F1 score for each class
for (i in 1:nrow(confusion_matrix)) {
tp <- confusion_matrix[i, i] # True Positives
fp <- sum(confusion_matrix[i, ]) - tp # False Positives
fn <- sum(confusion_matrix[, i]) - tp # False Negatives
class_precision[i] <- ifelse(tp + fp > 0, tp / (tp + fp), 0)
class_recall[i] <- ifelse(tp + fn > 0, tp / (tp + fn), 0)
}
# Calculation of averages for precision, recall, and F1 score
mean_precision <- mean(class_precision, na.rm = TRUE)
mean_recall <- mean(class_recall, na.rm = TRUE)
mean_f1 <- 2 * (mean_precision * mean_recall) / (mean_precision + mean_recall)
# Creation of the model object containing all metrics
mod$fit_ <- accuracy
mod$unpenalized_fit_ <- accuracy
mod$accuracy_ <- accuracy
auc <- mean(unlist(mod$auc_))
mod$auc_ = auc
mod$precision_ <- mean_precision
mod$recall_ <- mean_recall
mod$f1_ <-  mean_f1
mod$confusionMatrix_mc <- confusion_matrix
return(mod)
}
#' This function evaluates the aggregated models' performance on a population level.
#' @title evaluatePopulation_aggregation
#' @param pop: A population of model objects.
#' @param y: The true class labels.
#' @param X: The feature matrix of the data to be predicted.
#' @param force.re.evaluation: Boolean to force re-evaluation of the model even if it is already evaluated.
#' @param clf: Object clf.
#' @param aggregation: Type of aggregation method to be used ("votingAggregation" or "weightedAggregation").
#' @return The function returns a list containing evaluated models with their respective metrics.
#' @export
#'
evaluatePopulation_aggregation <- function(pop, y, X, force.re.evaluation = TRUE, clf,  aggregation_ = "votingAggregation") {
# Initializing an empty list to store the overall evaluation of each model.
pop_overall_full <- list()
# Looping through each model in the population.
for (j in 1:length(pop)) {
popu <- list()
popu <- pop[[j]]
pop_overall <- list()
for (i in 1:length(popu)) {
if (aggregation_ == "votingAggregation") {
predict_ <- predictModel_ovo(mod = popu[[i]], y = y, X = X, clf, force.re.evaluation = force.re.evaluation)
voting <- votingAggregation(mod = predict_)
modevaluate <- evaluateModel_aggregation(mod = voting, y = y)
}
else if (aggregation_ == "weightedAggregation") {
predict_ <- predictModel_ovo(mod = popu[[i]], y = y, X = X, clf, force.re.evaluation = force.re.evaluation)
weighted <- weightedAggregation(mod = predict_)
modevaluate <- evaluateModel_aggregation(mod = weighted, y = y)
}
else if (aggregation_ == "NewApproach") {
predict_ <- predictModel_ova(mod = popu[[i]], y = y, X = X, clf, force.re.evaluation = force.re.evaluation)
newapproach <-  NewApproach(mod = predict_, y = y)
modevaluate <- evaluateModel_aggregation(mod = newapproach, y = y)
}
else if (aggregation_ == "maximizationAggregation") {
predict_ <- predictModel_ova(mod = popu[[i]], y = y, X = X, clf, force.re.evaluation = force.re.evaluation)
maximization <-  maximizationAggregation(mod = predict_, y = y)
modevaluate <- evaluateModel_aggregation(mod = maximization, y = y)
}
else if (aggregation_ == "rankingAggregation") {
predict_ <- predictModel_ova(mod = popu[[i]], y = y, X = X, clf, force.re.evaluation = force.re.evaluation)
ranking <-  rankingAggregation(mod = predict_, y = y)
modevaluate <- evaluateModel_aggregation(mod = ranking, y = y)
}
else {
return(NULL)
}
pop_overall[[i]] <- modevaluate
}
# Sorting evaluated models based on accuracy
accuracys <- sapply(pop_overall, function(model) model$accuracy_)
indices_tri <- order(accuracys, decreasing = TRUE)
pop_overall_tri <- pop_overall[indices_tri]
pop_overall_full[[j]] <- pop_overall_tri
}
# Return the list containing evaluated models with their respective metrics.
return(pop_overall_full)
}
#' This function evaluates the aggregated models' performance.
#' @title evaluateModel_aggregation
#' @param mod models objects.
#' @param y The true class labels.
#' @param X The feature matrix of the data to be predicted.
#' @param force.re.evaluation Boolean to force re-evaluation of the model even if it is already evaluated.
#' @param clf Object clf.
#' @param approach Type of approach to be used ("ovo" or "ova").
#' @param aggregation Type of aggregation method to be used ("votingAggregation", "weightedAggregation", "Predomics_aggregation_ova", "maximizationAggregation", "rankingAggregation").
#' @return The function returns a list containing evaluated models with their respective metrics.
#' @export
evaluateModels_aggregation <- function(mod, y, X, force.re.evaluation = TRUE, clf, approch = "ovo", aggregation_ = "voting") {
# Initializing an empty list to store the overall evaluation of each model.
mod_evaluate <- NULL
if (approch == "ovo") {
if (aggregation_ == "votingAggregation") {
mod_predict <- predictModel_ovo(mod = mod, y = y, X = X, clf = clf, force.re.evaluation = TRUE)
mod_voting <- votingAggregation(mod = mod_predict)
mod_evaluate <- evaluateModel_aggregation(mod = mod_voting, y = y)
} else if (aggregation_ == "weightedAggregation") {
mod_predict <- predictModel_ovo(mod = mod, y = y, X = X, clf = clf, force.re.evaluation = TRUE)
mod_weighted <- weightedAggregation(mod = mod_predict)
mod_evaluate <- evaluateModel_aggregation(mod = mod_weighted, y = y)
}
} else if (approch == "ova") {
if (aggregation_ == "Predomics_aggregation_ova") {
mod_predict <- predictModel_ova(mod = mod, y = y, X = X, clf = clf, force.re.evaluation = TRUE)
mod_Predomics <- Predomics_aggregation_ova(mod = mod_predict, y = y)
mod_evaluate <- evaluateModel_aggregation(mod = mod_Predomics, y = y)
} else if (aggregation_ == "maximizationAggregation") {
mod_predict <- predictModel_ova(mod = mod, y = y, X = X, clf = clf, force.re.evaluation = TRUE)
mod_maximization <- maximizationAggregation(mod = mod_predict, y = y)
mod_evaluate <- evaluateModel_aggregation(mod = mod_maximization, y = y)
} else if (aggregation_ == "rankingAggregation") {
mod_predict <- predictModel_ova(mod = mod, y = y, X = X, clf = clf, force.re.evaluation = TRUE)
mod_ranking <- rankingAggregation(mod = mod_predict, y = y)
mod_evaluate <- evaluateModel_aggregation(mod = mod_ranking, y = y)
}
}
# Return the list containing evaluated models with their respective metrics.
return(mod_evaluate)
}
best.model$list_intercept_
knitr::opts_chunk$set(echo = TRUE)
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Create an index vector for data partitioning
X_general <- X_general[rowSums(X_general)!=0,]; dim(X_general) # filter out variables with only zero values
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE); dim(X_general)
set.seed(42)
y = as.vector(yvec_trie)
X = X_general
# Number of desired samples in each class
nombre_echantillons_par_classe <- min(table(y))
# Function to balance the classes
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe,seed =123) {
classes <- unique(y)
indices_equilibres <- integer(0)
for (classe in classes) {
indices_classe <- which(y == classe)
set.seed(seed)
indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
}
return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X
# Verify the distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)
# Split yvec_trie into 80% train and 20% test
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[,indices_division]
X.test <- X_equilibre[,-indices_division]
table(y)
table(y.test)
dim(X)
dim(X.test)
clf <- terBeam_mc(sparsity = c(4),
max.nb.features = 1000,
seed = 1,
nCores = 1,
evalToFit = "accuracy_",
objective = "auc",
experiment.id = "terBeam_mc",
experiment.save = "nothing")
printy(clf)
runit = TRUE
if(runit)
{
test1 <- fit_mc(X = X, y = y, clf = clf,approch="ovo", aggregation_ = "votingAggregation", cross.validate = TRUE, nfolds= 1); # class(res_clf_ovo)
# save results
##save(res_clf_ovo_weightedAggregation , clf, file = "res_clf_ovo_weightedAggregation.rda", compression_level = 9)
}
# ... Database X is not a matrix! Converting ...
# ... Classification mode, computing factor(y) for speedup and robustness
# ... Loading feature correlation for speedup
# ... Correlation file loaded
# ... Storing data in the classifier object for speedup
# ... Computing ternary coefficients for speedup
# ... One seed found, setting by default
# ... Running the classifier terga2 with a single CPU
# ... Second and faster version of terga fitting based on Genetic Algorithm heuristics ...
# ... Cross validation mode
# ... Starting cross validation not in parallel
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ... Learning process is finished succesfuly
# ... Thank you for using Predomics. Don't forget to digest the results now.
#
# [1] "experiment" "predomics"
test1$classifier$coeffs_
test1$classifier$feature.cor
clf = mael
clf$params$k_penalty
clf$params$sparsity
cat("... No cross validation mode\n")
res.clf               <- list()
res.clf$classifier    <- runClassifier_mc(X, y, clf, approch = approch, aggregation_ = aggregation_)
res.clf$classifier
pop
pop                 <- sortPopulation(pop, evalToOrder = "fit_")
tom = pop
# Sample the best and veryBest population
best                <- pop[1:min(clf$params$nbBest,length(pop))]
veryBest            <- pop[1:min(clf$params$nbVeryBest,length(pop))]
# features.to.keep
### Evaluate the apperance of every features in the best and veryBest models
featuresApperance   <- countEachFeatureApperance_mc(clf, allFeatures, pop, best, veryBest,approch=approch)
features.to.keep    <- getFeatures2Keep_mc(clf, featuresApperance,approch=approch)
listf <- list()
listf <- features.to.keep[[1]]
features.to.keep <- list()
features.to.keep <-  listf
if(clf$params$verbose)
{
if(isModel(pop[[1]]))
{
try(printModel_mc(mod = pop[[1]], method = clf$params$print_ind_method, score = "fit_"), silent = TRUE)
}
}
#EP: keep only the verybest
#fullPop[(length(fullPop) +1):(length(fullPop) + length(pop))] <- pop
minsize <- min(length(pop), clf$params$nbVeryBest)
fullPop[(length(fullPop) +1):(length(fullPop) + minsize)] <- pop[1:minsize]
# save populatio in a file
if(!(clf$params$popSaveFile=="NULL"))
{
savePopulation(fullPop, paste("resulstsForSparsity", k, clf$params$popSaveFile, sep = "_"))
}
# stopping testing
if((length(features.to.keep) < k + 2) & (k != 1)) # If we exhausted all the combinations
{
break
} #
length(pop)
if(clf$params$verbose) print(paste("... ... models are created"))
return.perc       <- clf$params$final.pop.perc
if(return.perc > 100)
{
return.perc = 100 # upper bound
warning("terBeam_fit: clf$params$final.pop.perc can not be greater than 100")
}
#fullPop           <- sortPopulation(fullPop, evalToOrder = "fit_")
fullPop           <- unique(fullPop) # keep only unique models
if(return.perc == 100)
{
# transform the population onto a model collection
res.mod.coll    <- listOfModels2ModelCollection(pop = fullPop)
} else # if smaller percentage
{
#if(clf$params$final.pop.perc>100)
nBest           <- round(return.perc * clf$params$nbVeryBest / 100)
res.mod.coll    <- listOfModels2ModelCollection(pop = fullPop, nBest = nBest)
}
length(res.mod.coll$k_4)
length(res.mod.coll)
saf = res.mod.coll$k_4
doc = res.mod.coll
x_test = NULL
y_test = NULL
if(clf$params$verbose) cat("... Entering runClassifier\n")
# test the classifier object
if(!isClf(clf))
{
stop("fit: please provide a valid classifier object!")
}
if(clf$params$verbose) cat("... Storing data in classifier object for speedup\n")
# set the epsion
if(!is.null(clf$params$epsilon))
{
if(clf$params$epsilon=="NULL")
{
clf$params$epsilon        <- 0
}
}
# when not in crossval this will be null so we need to initiate this
if(is.null(clf$params$current_seed))
{
clf$params$current_seed <- clf$params$seed[1]
}
if(clf$params$debug) cat("=> DBG: before fit\n")
startingTime <- Sys.time()
res = doc
if(isModelCollection(res))
{
clf$models <- res
}else
{
clf$models <- NULL
warning("runClassifier: major issue - no models produced ... stoping")
}
isModelCollection(res)
isModelCollection(res$k_4)
res$k_4
