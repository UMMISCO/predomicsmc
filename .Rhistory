Method = rep(c("K_2", "K_3", "K_4", "K_5"), times = 4)
)
colors <- c("Accuracy" = "#1f77b4", "Precision" = "#ff7f0e", "Recall" = "#2ca02c", "F1" = "#d62728")
ggplot(data, aes(x = Method, y = Value, fill = Metric)) +
geom_boxplot(color = "black", outlier.color = "black") +
scale_fill_manual(values = colors) +  # Utiliser la palette de couleurs définie
facet_wrap(~Metric, scales = "free_y") +
labs(x = "Metrics", y = "Value", title = "Metrics by sparsity") +
ylim(0, 100) +  # Définir la limite de l'axe y de 0 à 100
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA),
legend.title = element_blank(),
legend.position = "bottom")
best_model_ranking_aggregation = Population_ranking_Aggregation[[3]][[1]]
best_model_ranking_aggregation$accuracy_
best_model_ranking_aggregation$confusionMatrix_
best_model_ranking_aggregation$coeffs_
best_model_ranking_aggregation$predictions_aggre
library(caret)
library(randomForest)
library(e1071)
X_train = t(X)
y_train = y
X_test = t(X.test)
y_test =y.test
y_train <- as.factor(y_train)
y_test <- as.factor(y_test)
# Define 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)
# Train the multiclass random forest model with 10-fold cross-validation
rf_model <- train(x = X_train, y = y_train, method = "rf", trControl = train_control)
# Prediction on the training data
train_pred <- predict(rf_model, X_train)
# Calculation of metrics for the training data
train_metrics <- confusionMatrix(train_pred, y_train)
# Calculation of the AUC for training
roc_train <- multiclass.roc(y_train, as.numeric(train_pred))
auc_train <- auc(roc_train)
# Displaying metrics for training
print(train_metrics)
print(auc_train)
# Prediction on the test data
test_pred <- predict(rf_model, X_test)
# Calculation of metrics for the test data
test_metrics <- confusionMatrix(test_pred, y_test)
# Calculation of the AUC for the test data.
roc_test <- multiclass.roc(y_test, as.numeric(test_pred))
auc_test <- auc(roc_test)
# Displaying metrics for the test data
print(test_metrics)
print(auc_test)
# Prediction on the training data
train_pred <- predict(rf_model, X_train)
# Calculation of metrics for the training data
train_metrics <- confusionMatrix(train_pred, y_train)
# Calculation of the AUC for training
roc_train <- multiclass.roc(y_train, as.numeric(train_pred))
auc_train <- auc(roc_train)
# Displaying metrics for training
print(train_metrics)
print(auc_train)
# Prediction on the test data
test_pred <- predict(rf_model, X_test)
# Calculation of metrics for the test data
test_metrics <- confusionMatrix(test_pred, y_test)
# Calculation of the AUC for the test data.
roc_test <- multiclass.roc(y_test, as.numeric(test_pred))
auc_test <- auc(roc_test)
# Displaying metrics for the test data
print(test_metrics)
print(auc_test)
# Calculating metrics for the training data
train_accuracy <- train_metrics$overall["Accuracy"]
train_precision <- mean(train_metrics$byClass[,"Precision"], na.rm = TRUE)
train_recall <- mean(train_metrics$byClass[,"Recall"], na.rm = TRUE)
train_auc <- auc_train
# Calculer le score F1 pour les données d'entraînement
f1_train <- 2 * (train_precision * train_recall) / (train_precision + train_recall)
# Afficher le score F1 pour les données d'entraînement
print(paste("Train F1 Score:", f1_train))
# Displaying metrics for the training data
print(paste("Train Accuracy:", train_accuracy))
print(paste("Train Precision:", train_precision))
print(paste("Train Recall:", train_recall))
print(paste("Train AUC:", train_auc))
# Calculating metrics for the test data
test_metrics <- confusionMatrix(test_pred, y_test)
test_accuracy <- test_metrics$overall["Accuracy"]
test_precision <- mean(test_metrics$byClass[,"Precision"], na.rm = TRUE)
test_recall <- mean(test_metrics$byClass[,"Recall"], na.rm = TRUE)
# Calculating the AUC for the test data
roc_test <- multiclass.roc(y_test, as.numeric(test_pred))
test_auc <- auc(roc_test)
# Calculer et afficher le score F1 pour les données de test
test_precision <- mean(test_metrics$byClass[,"Precision"], na.rm = TRUE)
test_recall <- mean(test_metrics$byClass[,"Recall"], na.rm = TRUE)
f1_test <- 2 * (test_precision * test_recall) / (test_precision + test_recall)
print(paste("Test F1 Score:", f1_test))
# Displaying metrics for the test data
print(paste("Test Accuracy:", test_accuracy))
print(paste("Test Precision:", test_precision))
print(paste("Test Recall:", test_recall))
print(paste("Test AUC:", test_auc))
library(ggplot2)
# Example of metrics for the training data
accuracy_train <- train_accuracy
precision_train <- train_precision
AUC_train <- train_auc
recall_train <- train_recall
f1_train <- f1_train
# Example of metrics for the test data
accuracy_test <- test_accuracy
precision_test <- test_precision
AUC_test <- test_auc
recall_test <- test_recall
f1_test <- f1_test
# Create a data frame for the training metrics
train_metrics <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_train, precision_train, f1_train, recall_train),
Dataset = "Training"
)
# Create a data frame for the test metrics
test_metrics <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_test, precision_test, f1_test, recall_test),
Dataset = "Test"
)
# Combine the training and test metrics
all_metrics <- rbind(train_metrics, test_metrics)
# Create the graph
plot <- ggplot(all_metrics, aes(x = Metric, y = Value, fill = Dataset)) +
geom_bar(stat = "identity", position = "dodge") +
geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
labs(title = "Comparison of test and training metrics Sota RF",
x = "Metrics", y = "Values") +
theme_minimal()
# Display the graph
print(plot)
library(nnet)
# Create a data.frame from matrix X
library(glmnet)
# Utilize cv.glmnet with 10-fold cross-validation
df <- as.data.frame(t(X))
df$y <- factor(y)
cv_model_rl <- cv.glmnet(as.matrix(df[, -ncol(df)]), df$y, family = "multinomial", alpha = 1, nfolds = 10)
# Get the best model based on cross-validation
best_model_rl <- cv_model_rl$glmnet.fit
# Obtain predictions directly from cross-validation
predictions_rl <- predict(cv_model_rl, newx = as.matrix(df[, -ncol(df)]), s = "lambda.min", type = "class")
predictions_rl <- factor(predictions_rl)
# Check the dimensions and create the confusion matrix
if(length(predictions_rl) == length(df$y)) {
confusionMatrix(predictions_rl, df$y)
} else {
print("The dimensions of the predictions do not match those of the ground truth")
}
conf_matrix_rl <- confusionMatrix(predictions_rl, df$y)
# Calcul de l'exactitude
accuracy_train_rl <- conf_matrix_rl$overall[["Accuracy"]]
# Calcul de la précision moyenne de chaque classe
precision_train_rl <- mean(conf_matrix_rl$byClass[ , "Precision"])
# Calcul de l'AUC en utilisant la précision équilibrée moyenne (Balanced Accuracy)
AUC_train_rl <- mean(conf_matrix_rl$byClass[ , "Balanced Accuracy"])
# Calcul du rappel moyen (Sensibilité) de chaque classe
recall_train_rl <- mean(conf_matrix_rl$byClass[ , "Sensitivity"])
# Calcul du score F1 pour les données d'entraînement
F1_train_rl <- 2 * (precision_train_rl * recall_train_rl) / (precision_train_rl + recall_train_rl)
# Affichage des métriques pour les données d'entraînement
print(paste("Train Accuracy:", accuracy_train_rl))
print(paste("Train Precision:", precision_train_rl))
print(paste("Train Recall (Sensitivity):", recall_train_rl))
print(paste("Train AUC (Balanced Accuracy):", AUC_train_rl))
print(paste("Train F1 Score:", F1_train_rl))
predictions_test1_rl <- predict(cv_model_rl, newx = as.matrix(X_test), type = "class")
library(caret)
predictions_test1_rl <- as.factor(predictions_test1_rl)
# Create the confusion matrix for predictions on the test dataset
conf_matrix_test_rl <- confusionMatrix(predictions_test1_rl, y_test)
# Calculate accuracy, precision, AUC, and recall for the test dataset
# Calculate accuracy, precision, AUC, and recall for the test dataset
accuracy_test1_rl <- conf_matrix_test_rl$overall[["Accuracy"]]
precision_test1_rl <- mean(conf_matrix_test_rl$byClass[ , "Precision"], na.rm = TRUE)
AUC_test1_rl <- mean(conf_matrix_test_rl$byClass[ , "Balanced Accuracy"], na.rm = TRUE)
recall_test1_rl <- mean(conf_matrix_test_rl$byClass[ ,"Sensitivity"], na.rm = TRUE)
# Calcul du score F1 pour les données d'entraînement
F1_test1_rl <- 2 * (precision_test1_rl * recall_test1_rl) / (precision_test1_rl + recall_test1_rl)
library(ggplot2)
# Create a data frame for the training metrics
train_metrics_rl <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_train_rl, precision_train_rl, F1_train_rl, recall_train_rl),
Dataset = "Training"
)
# Create a data frame for the test metrics
test_metrics_rl <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_test1_rl, precision_test1_rl, F1_test1_rl, recall_test1_rl),
Dataset = "Test"
)
# Combine the training and test metrics
all_metrics_rl <- rbind(test_metrics_rl,train_metrics_rl)
# Create the graph
plot <- ggplot(all_metrics_rl, aes(x = Metric, y = Value, fill = Dataset)) +
geom_bar(stat = "identity", position = "dodge") +
geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
labs(title = "Comparison of test and training metrics Sota LR",
x = "Metrics", y = "Values") +
theme_minimal()
# Display the graph
print(plot)
library(caret)
library(pROC)
# Train an SVM model with radial kernel and 10-fold cross-validation
svm_model <- train(x = X_train, y = y_train, method = "svmRadial", trControl = trainControl(method = "cv", number = 10))
# Make predictions with the SVM model
svm_predictions_train <- predict(svm_model, X_train)
svm_predictions_test <- predict(svm_model, X_test)
# Calculate the accuracy for the training and test sets
accuracy_train_svm <- confusionMatrix(data = svm_predictions_train, reference = y_train)$overall['Accuracy'][1]
accuracy_test_svm <- confusionMatrix(data = svm_predictions_test, reference = y_test)$overall['Accuracy'][1]
# Calculate precision, recall, and AUC for the training and test sets
precision_train_svm <- confusionMatrix(data = svm_predictions_train, reference = y_train)$byClass[, 'Pos Pred Value']
precision_test_svm <- confusionMatrix(data = svm_predictions_test, reference = y_test)$byClass[, 'Pos Pred Value']
recall_train_svm <- confusionMatrix(data = svm_predictions_train, reference = y_train)$byClass[, 'Sensitivity']
recall_test_svm <- confusionMatrix(data = svm_predictions_test, reference = y_test)$byClass[, 'Sensitivity']
F1_train_svm <- 2 * (precision_train_svm * recall_train_svm) / (precision_train_svm + recall_train_svm)
F1_train = mean(F1_train_svm)
auc_train_svm <- roc(y_train, as.numeric(svm_predictions_train))$auc
auc_test_svm <- roc(y_test, as.numeric(svm_predictions_test))$auc
precision_train_svm = mean(precision_train_svm)
precision_test_svm = mean(precision_test_svm)
recall_train_svm = mean(recall_train_svm)
recall_test_svm = mean(recall_test_svm)
F1_test_svm <- 2 * (precision_test_svm * recall_test_svm) / (precision_test_svm + recall_test_svm)
F1_test_svm <- mean(F1_test_svm)
library(ggplot2)
# Create a data frame for the training metrics
train_metrics_svm <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_train_svm, precision_train_svm, F1_train, recall_train_svm),
Dataset = "Training"
)
# Create a data frame for the test metrics
test_metrics_svm <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_test_svm, precision_test_svm, F1_test_svm, recall_test_svm),
Dataset = "Test"
)
# Combine the training and test metrics
all_metrics_svm <- rbind(test_metrics_svm,train_metrics_svm)
# Create the graph
plot <- ggplot(all_metrics_svm, aes(x = Metric, y = Value, fill = Dataset)) +
geom_bar(stat = "identity", position = "dodge") +
geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
labs(title = "Comparison of test and training metrics Sota SVM",
x = "Metrics", y = "Values") +
theme_minimal()
# Display the graph
print(plot)
accuracy_val = best_model_voting_aggregation$accuracy_
precision_val = best_model_voting_aggregation$precision_
recall_val = best_model_voting_aggregation$recall_
f1_val <- best_model_voting_aggregation$fit_
# Création du dataframe sans la matrice de confusion
df_voting <- data.frame(
Algorithme = "Predomics_voting",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_voting
accuracy_val = best_model_voting_aggregation$accuracy_
precision_val = best_model_voting_aggregation$precision_
recall_val = best_model_voting_aggregation$recall_
f1_val <- best_model_voting_aggregation$fit_
# Création du dataframe sans la matrice de confusion
df_voting <- data.frame(
Algorithme = "Voting Aggregation",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_voting
accuracy_val = best_model_weighted_aggregation$accuracy_
precision_val = best_model_weighted_aggregation$precision_
recall_val = best_model_weighted_aggregation$recall_
f1_val <- best_model_weighted_aggregation$fit_
# Création du dataframe sans la matrice de confusion
df_weighted <- data.frame(
Algorithme = "weighted Aggregation",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_weighted
#' This function evaluates the aggregated model's performance.
#' @title evaluateModel_aggregation
#' @param mod: The model object containing aggregated predictions and true labels.
#' @param y: The true class labels.
#' @return The function returns the model object with evaluation metrics.
#' @export
#'
evaluateModel_aggregation <- function(mod, y) {
# Calculation of the confusion matrix
confusion_matrix <- table(mod$predictions_aggre, y)
# Calculation of the overall accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# Initialization of variables for precision, recall, and F1 score
class_precision <- numeric(length = nrow(confusion_matrix))
class_recall <- numeric(length = nrow(confusion_matrix))
# Calculation of precision, recall, and F1 score for each class
for (i in 1:nrow(confusion_matrix)) {
tp <- confusion_matrix[i, i] # True Positives
fp <- sum(confusion_matrix[i, ]) - tp # False Positives
fn <- sum(confusion_matrix[, i]) - tp # False Negatives
class_precision[i] <- ifelse(tp + fp > 0, tp / (tp + fp), 0)
class_recall[i] <- ifelse(tp + fn > 0, tp / (tp + fn), 0)
}
# Calculation of averages for precision, recall, and F1 score
mean_precision <- mean(class_precision, na.rm = TRUE)
mean_recall <- mean(class_recall, na.rm = TRUE)
mean_f1 <- 2 * (mean_precision * mean_recall) / (mean_precision + mean_recall)
# Creation of the model object containing all metrics
mod$fit_ <- accuracy
mod$unpenalized_fit_ <- accuracy
mod$accuracy_ <- accuracy
mod$intercept_ <- mod$list_intercept_mc
mod$list_intercept_mc <- NULL
mod$auc_ <- NULL
mod$precision_ <- mean_precision
mod$recall_ <- mean_recall
mod$f1_ <-  mean_f1
mod$confusionMatrix_ <- confusion_matrix
return(mod)
}
#' This function evaluates the aggregated models' performance on a population level.
#' @title evaluatePopulation_aggregation
#' @param pop: A population of model objects.
#' @param y: The true class labels.
#' @param X: The feature matrix of the data to be predicted.
#' @param force.re.evaluation: Boolean to force re-evaluation of the model even if it is already evaluated.
#' @param clf: Object clf.
#' @param aggregation: Type of aggregation method to be used ("votingAggregation" or "weightedAggregation").
#' @return The function returns a list containing evaluated models with their respective metrics.
#' @export
#'
evaluatePopulation_aggregation <- function(pop, y, X, force.re.evaluation = TRUE, clf,  aggregation = "votingAggregation") {
# Initializing an empty list to store the overall evaluation of each model.
pop_overall_full <- list()
# Looping through each model in the population.
for (j in 1:length(pop)) {
popu <- list()
popu <- pop[[j]]
pop_overall <- list()
for (i in 1:length(popu)) {
if (aggregation == "votingAggregation") {
predict_ <- predictModel_ovo(mod = popu[[i]], y = y, X = X, clf, force.re.evaluation = force.re.evaluation)
voting <- votingAggregation(mod = predict_)
modevaluate <- evaluateModel_aggregation(mod = voting, y = y)
}
else if (aggregation == "weightedAggregation") {
predict_ <- predictModel_ovo(mod = popu[[i]], y = y, X = X, clf, force.re.evaluation = force.re.evaluation)
weighted <- weightedAggregation(mod = predict_)
modevaluate <- evaluateModel_aggregation(mod = weighted, y = y)
}
else if (aggregation == "NewApproach") {
predict_ <- predictModel_ova(mod = popu[[i]], y = y, X = X, clf, force.re.evaluation = force.re.evaluation)
newapproach <-  NewApproach(mod = predict_, y = y)
modevaluate <- evaluateModel_aggregation(mod = newapproach, y = y)
}
else if (aggregation == "maximizationAggregation") {
predict_ <- predictModel_ova(mod = popu[[i]], y = y, X = X, clf, force.re.evaluation = force.re.evaluation)
maximization <-  maximizationAggregation(mod = predict_, y = y)
modevaluate <- evaluateModel_aggregation(mod = maximization, y = y)
}
else if (aggregation == "rankingAggregation") {
predict_ <- predictModel_ova(mod = popu[[i]], y = y, X = X, clf, force.re.evaluation = force.re.evaluation)
ranking <-  rankingAggregation(mod = predict_, y = y)
modevaluate <- evaluateModel_aggregation(mod = ranking, y = y)
}
else {
return(NULL)
}
pop_overall[[i]] <- modevaluate
}
# Sorting evaluated models based on accuracy
accuracys <- sapply(pop_overall, function(model) model$accuracy_)
indices_tri <- order(accuracys, decreasing = TRUE)
pop_overall_tri <- pop_overall[indices_tri]
pop_overall_full[[j]] <- pop_overall_tri
}
# Return the list containing evaluated models with their respective metrics.
return(pop_overall_full)
}
best_model_voting_aggregation$f1_
accuracy_val = best_model_voting_aggregation$accuracy_
precision_val = best_model_voting_aggregation$precision_
recall_val = best_model_voting_aggregation$recall_
f1_val <- best_model_voting_aggregation$f1_
# Création du dataframe sans la matrice de confusion
df_voting <- data.frame(
Algorithme = "Voting Aggregation",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_voting
accuracy_val = best_model_weighted_aggregation$accuracy_
precision_val = best_model_weighted_aggregation$precision_
recall_val = best_model_weighted_aggregation$recall_
f1_val <- best_model_weighted_aggregation$f1_
# Création du dataframe sans la matrice de confusion
df_weighted <- data.frame(
Algorithme = "weighted Aggregation",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_weighted
accuracy_val = best_model_newapproach_aggregation$accuracy_
precision_val = best_model_newapproach_aggregation$precision_
recall_val = best_model_newapproach_aggregation$recall_
f1_val <- best_model_newapproach_aggregation$f1_
# Création du dataframe sans la matrice de confusion
df_newapproach <- data.frame(
Algorithme = "New Approach",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_newapproach
accuracy_val = best_model_maximization_aggregation$accuracy_
precision_val = best_model_maximization_aggregation$precision_
recall_val = best_model_maximization_aggregation$recall_
f1_val <- best_model_maximization_aggregation$f1_
# Création du dataframe sans la matrice de confusion
df_maximization <- data.frame(
Algorithme = "maximization aggregation",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_maximization
accuracy_val = best_model_ranking_aggregation$accuracy_
precision_val =best_model_ranking_aggregation$precision_
recall_val = best_model_ranking_aggregation$recall_
f1_val <-best_model_ranking_aggregation$f1_
# Création du dataframe sans la matrice de confusion
df_ranking <- data.frame(
Algorithme = "ranking aggregation",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_ranking
accuracy_val <-  test_accuracy
precision_val <- test_precision
recall_val <- test_recall
f1_val <- f1_test
# Création du dataframe sans la matrice de confusion
df_rf <- data.frame(
Algorithme = "Sota_RF",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_rf
accuracy_val <-  accuracy_test1_rl
precision_val <- precision_test1_rl
recall_val <- recall_test1_rl
f1_val <- F1_test1_rl
# Création du dataframe sans la matrice de confusion
df_LR <- data.frame(
Algorithme = "Sota_LR",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_LR
accuracy_val <-  accuracy_test_svm
precision_val <- precision_test_svm
recall_val <- recall_test_svm
f1_val <- F1_test_svm
# Création du dataframe sans la matrice de confusion
df_SVM <- data.frame(
Algorithme = "Sota_SVM",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_SVM
# Combinez les dataframes en un seul dataframe
df_all <- rbind(df_ranking, df_maximization,df_newapproach, df_voting,df_weighted, df_rf,df_LR, df_SVM)
# Créer une liste des métriques à comparer
metrics_list <- c("Accuracy", "Precision", "Recall", "F1")
# Parcourir chaque métrique pour créer un plot unique
for (metric in metrics_list) {
# Filtrer les données par métrique
df_metric <- df_all[df_all$Metrics == metric, ]
# Créer le plot
plot <- ggplot(df_metric, aes(x = Algorithme, y = Values, fill = Algorithme)) +
geom_bar(stat = "identity") +
labs(title = paste("Comparison of", metric, "between the models"), y = metric, x = "Algorithms") +
theme_minimal() +
geom_text(aes(label = round(Values, 2), y = Values), vjust = -0.3, size = 3)
# Afficher le plot
print(plot)
}
library(mcpredomics)
knitr::opts_chunk$set(echo = TRUE)
best_model_voting_aggregation
