res.crossval$scores$generalization.auc[k_sparse.name,i]       <- mod.test$auc_
res.crossval$scores$generalization.acc[k_sparse.name,i]       <- mod.test$accuracy_
res.crossval$scores$generalization.rec[k_sparse.name,i]       <- mod.test$recall_
res.crossval$scores$generalization.prc[k_sparse.name,i]       <- mod.test$precision_
res.crossval$scores$generalization.f1s[k_sparse.name,i]       <- mod.test$f1_
res.crossval$scores$generalization.cor[k_sparse.name,i]       <- mod.test$cor_
# store by k
# AUC
res.crossval$k$auc[k_sparse.name,"empirical"]                 <- mod.train$auc_
res.crossval$k$auc[k_sparse.name,"generalization"]            <- mod.test$auc_
# Accuracy
res.crossval$k$acc[k_sparse.name,"empirical"]                 <- mod.train$accuracy_
res.crossval$k$acc[k_sparse.name,"generalization"]            <- mod.test$accuracy_
# Recall
res.crossval$k$rec[k_sparse.name,"empirical"]                 <- mod.train$recall_
res.crossval$k$rec[k_sparse.name,"generalization"]            <- mod.test$recall_
# Precision
res.crossval$k$prc[k_sparse.name,"empirical"]                 <- mod.train$precision_
res.crossval$k$prc[k_sparse.name,"generalization"]            <- mod.test$precision_
# F1-Score
res.crossval$k$f1s[k_sparse.name,"empirical"]                 <- mod.train$f1_
res.crossval$k$f1s[k_sparse.name,"generalization"]            <- mod.test$f1_
# Regression
res.crossval$k$cor[k_sparse.name,"empirical"]                 <- mod.train$cor_
res.crossval$k$cor[k_sparse.name,"generalization"]            <- mod.test$cor_
} # if training and testing results exist
} # end of k_sparse loop
# if saving move one level up
if(!(clf$params$popSaveFile=="NULL"))
{
setwd("..")
}
} # end if null digest
# Compute FEATURE IMPORTANCE for each classifier in GENERALIZATION
if(clf$params$compute.importance & !isLearnerSota(clf))
{
# we compute the feature importance and BTR languages and algorithms
if (isClf(res.all[[i]])) {
for (jo in 1:length(res.all[[1]]$fip)) {
# Check if results are valid
if (!is.null(res.all[[i]]$fip[[jo]])) {
# Check if object exists
if (!is.null(res.all[[i]]$fip[[jo]]$mda)) {
mda.all[res.all[[i]]$fip[[jo]]$feat.catalogue, i] <- res.all[[i]]$fip[[jo]]$mda
}
# Check if object exists
if (!is.null(res.all[[i]]$fip[[jo]]$sda)) {
sda.all[res.all[[i]]$fip[[jo]]$feat.catalogue, i] <- res.all[[i]]$fip[[jo]]$sda
}
# Check if object exists
if (!is.null(res.all[[i]]$fip[[jo]]$pda)) {
pda.all[res.all[[i]]$fip[[jo]]$feat.catalogue, i] <- res.all[[i]]$fip[[jo]]$pda
}
}
}
}
else
{
# print out information (might be errors)
print(res.all[[i]])
next
}
} # end importance
# Do we need to return everything back ?
if(return.all)
{
res.crossval$nfold[[i]]             <- list(results = res_train,
resultsDigest = res_train.digest)
}
} # end for (dispatching results)
if(clf$params$verbose) cat("... All results from cross validation are dispatched\n")
# reorder results function
reorderByRownamesNumeric <- function(mat)
{
ind <- as.numeric(gsub("k_","",rownames(mat)))
mat <- mat[order(ind),]
}
# reorder results
res.crossval$scores$empirical.auc       <- reorderByRownamesNumeric(res.crossval$scores$empirical.auc)
res.crossval$scores$empirical.acc       <- reorderByRownamesNumeric(res.crossval$scores$empirical.acc)
res.crossval$scores$empirical.rec       <- reorderByRownamesNumeric(res.crossval$scores$empirical.rec)
res.crossval$scores$empirical.prc       <- reorderByRownamesNumeric(res.crossval$scores$empirical.prc)
res.crossval$scores$empirical.f1s       <- reorderByRownamesNumeric(res.crossval$scores$empirical.f1s)
res.crossval$scores$empirical.cor       <- reorderByRownamesNumeric(res.crossval$scores$empirical.cor)
res.crossval$scores$generalization.auc  <- reorderByRownamesNumeric(res.crossval$scores$generalization.auc)
res.crossval$scores$generalization.acc  <- reorderByRownamesNumeric(res.crossval$scores$generalization.acc)
res.crossval$scores$generalization.rec  <- reorderByRownamesNumeric(res.crossval$scores$generalization.rec)
res.crossval$scores$generalization.prc  <- reorderByRownamesNumeric(res.crossval$scores$generalization.prc)
res.crossval$scores$generalization.f1s  <- reorderByRownamesNumeric(res.crossval$scores$generalization.f1s)
res.crossval$scores$generalization.cor  <- reorderByRownamesNumeric(res.crossval$scores$generalization.cor)
# auc
if(!is.null(dim(res.crossval$scores$empirical.auc)))
{
res.crossval$scores$mean.auc            <- data.frame(cbind(rowMeans(res.crossval$scores$empirical.auc, na.rm = TRUE),
rowMeans(res.crossval$scores$generalization.auc, na.rm = TRUE)))
colnames(res.crossval$scores$mean.auc)  <- c("empirical","generalization")
}
# accuracy
if(!is.null(dim(res.crossval$scores$empirical.acc)))
{
res.crossval$scores$mean.acc            <- data.frame(cbind(rowMeans(res.crossval$scores$empirical.acc, na.rm = TRUE),
rowMeans(res.crossval$scores$generalization.acc, na.rm = TRUE)))
colnames(res.crossval$scores$mean.acc)  <- c("empirical","generalization")
}
# recall
if(!is.null(dim(res.crossval$scores$empirical.rec)))
{
res.crossval$scores$mean.rec            <- data.frame(cbind(rowMeans(res.crossval$scores$empirical.rec, na.rm = TRUE),
rowMeans(res.crossval$scores$generalization.rec, na.rm = TRUE)))
colnames(res.crossval$scores$mean.rec)  <- c("empirical","generalization")
}
# precision
if(!is.null(dim(res.crossval$scores$empirical.prc)))
{
res.crossval$scores$mean.prc            <- data.frame(cbind(rowMeans(res.crossval$scores$empirical.prc, na.rm = TRUE),
rowMeans(res.crossval$scores$generalization.prc, na.rm = TRUE)))
colnames(res.crossval$scores$mean.prc)  <- c("empirical","generalization")
}
# f1-score
if(!is.null(dim(res.crossval$scores$empirical.f1s)))
{
res.crossval$scores$mean.f1s            <- data.frame(cbind(rowMeans(res.crossval$scores$empirical.f1s, na.rm = TRUE),
rowMeans(res.crossval$scores$generalization.f1s, na.rm = TRUE)))
colnames(res.crossval$scores$mean.f1s)  <- c("empirical","generalization")
}
# correlation
if(!is.null(dim(res.crossval$scores$empirical.cor)))
{
res.crossval$scores$mean.cor            <- data.frame(cbind(rowMeans(res.crossval$scores$empirical.cor, na.rm = TRUE),
rowMeans(res.crossval$scores$generalization.cor, na.rm = TRUE)))
colnames(res.crossval$scores$mean.cor)  <- c("empirical","generalization")
}
# adding results for feature importance
if(clf$params$compute.importance & !isLearnerSota(clf))
{
res.crossval$fip <- list(mda = mda.all,
sda = sda.all,
pda = pda.all,
fpf = rowSums(!is.na(mda.all)) # feature prevalenc in folds
)
}
length(res.crossval$fip)
res.crossval$fip$mda[[1]]
res.clf               <- list()
res.clf$classifier = res.crossval
res.clf$classifier <- list()
res.clf$lfolds        <- lfolds
res.clf$crossVal = res.crossval
res.clf$classifier    <- res.clf$crossVal$whole
res.clf$crossVal      <- res.clf$crossVal[-match("whole", names(res.clf$crossVal))]
res.clf$lfolds        <- lfolds
length(res.clf$crossVal$fip$mda)
length(res.clf$crossVal$fip$mda[[1]])
length(res.clf$crossVal$fip$mda[[2]])
length(res.clf$crossVal$fip$mda[[3]])
length(res.clf$crossVal$fip$mda[[4]])
length(res.clf$crossVal$fip$fpf)
ce.cv <- res.clf$crossVal$fip$fpf / ncol(res.clf$crossVal$fip$mda[[1]])
ce.cv
to = ncol(res.clf$crossVal$fip$mda[[1]]
)
to
res.clf$crossVal$fip$mda[[1]]
mda.all
feature.importance.cvd <- rowMeans(res.clf$crossVal$fip$mda [[1]], na.rm = TRUE)
feature.importance.cvd <- rowMeans(res.clf$crossVal$fip[[1]]$mda, na.rm = TRUE)
res.clf$crossVal$fip$mda
res.clf$crossVal$fip$mda
res.clf$crossVal$fip$fpf
feature.importance.cvd <- rowMeans(res.clf$crossVal$fip$mda, na.rm = TRUE)
feature.importance.cvd
feature.importance.cvd <- rowMeans(res.clf$crossVal$fip$mda[[1]], na.rm = TRUE)
feature.importance.cv <- rowMeans(res.clf$crossVal$fip$mda, na.rm = TRUE)
feature.prevalence.cv <- res.clf$crossVal$fip$fpf / ncol(res.clf$crossVal$fip$mda)
feature.prevalence.cv
feature.importance  <- res.clf$classifier$fip$mda
feature.importance
feature.importance  <- rep(NA, length(clf$data$features))
feature.importance
names(feature.importance) <- names(clf$data$features)
pop <- modelCollectionToPopulation(res.clf$classifier$models)
length(pop[[1]]$names_[[1]])
length(pop[[2]]$names_[[1]])
length(pop[[3]]$names_[[1]])
pop[[3]]$names_[[1]]
pop[[3]]$names_
length(pop)
length(pop[[3]])
pop[[102]]$names_
length(pop[[102]]$names_)
mda.cv_ <- list() # Initialisation de la liste des résultats de MDA
ind.features <- list() # Initialisation de la liste des indices des caractéristiques
for (i in 1:length(pop)) {
# Généralisation de MDA
# Initialisation de mda.cv_ pour chaque modèle de la population
pop[[i]]$mda.cv_ <- rep(0, length(pop[[i]]$names_[[1]]))
# Boucle sur les noms des caractéristiques pour chaque modèle
for (j in 1:length(pop[[1]]$names_)) {
# Attribution des noms des caractéristiques à mda.cv_
names(pop[[i]]$mda.cv_[[j]]) <- pop[[i]]$names_[[j]]
# Intersection des noms des caractéristiques avec les noms de l'importance des caractéristiques
ind.features[[j]] <- intersect(pop[[i]]$names_[[j]], names(feature.importance.cv))
}
# Attribution des résultats d'importance des caractéristiques à mda.cv_
pop[[i]]$mda.cv_[ind.features] <- feature.importance.cv[ind.features]
}
mda.cv_ <- list() # Initialisation de la liste des résultats de MDA
ind.features <- list() # Initialisation de la liste des indices des caractéristiques
for (i in 1:length(pop)) {
# Généralisation de MDA
# Initialisation de mda.cv_ pour chaque modèle de la population
pop[[i]]$mda.cv_ <- list()
# Boucle sur les noms des caractéristiques pour chaque modèle
for (j in 1:length(pop[[1]]$names_)) {
# Attribution des noms des caractéristiques à mda.cv_
pop[[i]]$mda.cv_[[j]] <- rep(0, length(pop[[i]]$names_[[j]]))
names(pop[[i]]$mda.cv_[[j]]) <- pop[[i]]$names_[[j]]
# Intersection des noms des caractéristiques avec les noms de l'importance des caractéristiques
ind.features[[j]] <- intersect(pop[[i]]$names_[[j]], names(feature.importance.cv))
}
# Attribution des résultats d'importance des caractéristiques à mda.cv_
for (j in 1:length(ind.features)) {
pop[[i]]$mda.cv_[[j]][ind.features[[j]]] <- feature.importance.cv[ind.features[[j]]]
}
}
pop[[1]]$mda.cv_
pop[[102]]$mda.cv_
pop[[302]]$mda.cv_
mda.cv_ <- list() # Initializing the list of MDA results
ind.features <- list() # Initializing the list of feature indices
prev.cv_ <- list()
mda_ <- list()
for (i in 1:length(pop)) {
# Generalizing MDA
# Initializing mda.cv_ for each model in the population
pop[[i]]$mda.cv_ <- list()
# Looping over feature names for each model
for (j in 1:length(pop[[1]]$names_)) {
# Assigning feature names to mda.cv_
pop[[i]]$mda.cv_[[j]] <- rep(0, length(pop[[i]]$names_[[j]]))
names(pop[[i]]$mda.cv_[[j]]) <- pop[[i]]$names_[[j]]
# Intersection of feature names with the names of feature importance
ind.features[[j]] <- intersect(pop[[i]]$names_[[j]], names(feature.importance.cv))
}
# Assigning feature importance results to mda.cv_
for (j in 1:length(ind.features)) {
pop[[i]]$mda.cv_[[j]][ind.features[[j]]] <- feature.importance.cv[ind.features[[j]]]
}
# Initializing mda.cv_ for each model in the population
pop[[i]]$prev.cv_ <- list()
# Looping over feature names for each model
for (j in 1:length(pop[[1]]$names_)) {
# prevalence in top models in the folds
pop[[i]]$prev.cv_[[j]] <- rep(0, length(pop[[i]]$names_[[j]]))
names(pop[[i]]$prev.cv_[[j]]) <- pop[[i]]$names_[[j]]
ind.features[[j]] <-intersect(pop[[i]]$names_[[j]], names(feature.prevalence.cv))
}
for (j in 1:length(ind.features)) {
pop[[i]]$prev.cv_[[j]][ind.features[[j]]] <-feature.prevalence.cv[ind.features[[j]]]
}
# MDA empirical
pop[[i]]$mda_ <- list()
for (j in 1:length(pop[[1]]$names_)) {
pop[[i]]$mda_[[j]] <- rep(0, length(pop[[i]]$names_[[j]]))
names(pop[[i]]$mda_[[j]]) <- pop[[i]]$names_[[j]]
ind.features[[j]] <-intersect(pop[[i]]$names_[[j]], names(feature.importance))
}
for (j in 1:length(ind.features)) {
pop[[i]]$mda_[[j]][ind.features[[j]]] <-feature.importance[ind.features[[j]]]
}
}
pop[[100]]$mda_
pop[[200]]$mda_
pop[[300]]$mda_
pop[[400]]$mda_
pop[[400]]$prev.cv_
pop[[300]]$prev.cv_
pop[[200]]$prev.cv_
pop[[100]]$prev.cv_
res.clf$classifier$models <- listOfModels2ModelCollection(pop)
res.clf$classifier$models
if(clf$experiment$save != "nothing")
{
experiment$params$lfolds <- lfolds
}
} # end cross.validate test
if(clf$experiment$save != "nothing")
{
experiment$params$lfolds <- lfolds
}
cat("... Learning process is finished succesfuly\n")
# STOP CLUSTER
if(clf$params$parallel)
{
stopCluster(clf$params$cluster)
cat("... Stopping the parallel cluster\n")
}
# SAVE EXPERIMENT
if(clf$experiment$save != "nothing")
{
experiment$results    <- res.clf
saveResults(experiment, paste(fileNames, "yml", sep = "."))
setwd("..")
cat("... Saving exmeriment finished\n")
}
# closing graphics
if(clf$params$plot)
{
dev.off()
}
cat("... Thank you for using Predomics. Don't forget to digest the results now.\n")
class(res.clf) <- c("experiment","predomics")
library(mcpredomics)
pop[[4]]$mda.cv_
pop[[2]]$mda.cv_
pop[[1]]$mda.cv_
knitr::opts_chunk$set(echo = TRUE)
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Create an index vector for data partitioning
X_general <- X_general[rowSums(X_general)!=0,]; dim(X_general) # filter out variables with only zero values
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE); dim(X_general)
set.seed(42)
y = as.vector(yvec_trie)
X = X_general
# Number of desired samples in each class
nombre_echantillons_par_classe <- min(table(y))
# Function to balance the classes
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe,seed =123) {
classes <- unique(y)
indices_equilibres <- integer(0)
for (classe in classes) {
indices_classe <- which(y == classe)
set.seed(seed)
indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
}
return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X
# Verify the distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)
# Split yvec_trie into 80% train and 20% test
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[,indices_division]
X.test <- X_equilibre[,-indices_division]
table(y)
table(y.test)
dim(X)
dim(X.test)
clf <- terBeam_mc(sparsity = c(2,3,4,5),
max.nb.features = 1000,
seed = 1,
nCores = 1,
evalToFit = "accuracy_",
objective = "auc",
experiment.id = "terBeam_mc",
experiment.save = "nothing")
printy(clf)
runit = TRUE
if(runit)
{
res_clf_ova <- fit_mc(X = X, y = y, clf = clf,approch="ova", cross.validate = TRUE, nfolds= 10); # class(res_clf)
# save results
save(res_clf_ova, clf, file = "res_clf_ova.rda", compression_level = 9)
}
# ... Database X is not a matrix! Converting ...
# ... Classification mode, computing factor(y) for speedup and robustness
# ... Loading feature correlation for speedup
# ... Correlation file loaded
# ... Storing data in the classifier object for speedup
# ... Computing ternary coefficients for speedup
# ... One seed found, setting by default
# ... Running the classifier terga2 with a single CPU
# ... Second and faster version of terga fitting based on Genetic Algorithm heuristics ...
# ... Cross validation mode
# ... Starting cross validation not in parallel
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ... Learning process is finished succesfuly
# ... Thank you for using Predomics. Don't forget to digest the results now.
#
# [1] "experiment" "predomics"
#if(!runit)
#{
load("res_clf_ova.rda")
#}
ovoterbeam <- digestmc(obj = res_clf_ova, penalty = 0.75/100, plot = TRUE)
#bestmodovo <- ovoterbeam$best$model
bestmodovo <- ovoterbeam$best$model
bestmodovo$confusionMatrix_
bestmodovo$mda.cv_
bestmodovo
res_clf_ova$crossVal$nfold
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Create an index vector for data partitioning
X_general <- X_general[rowSums(X_general)!=0,]; dim(X_general) # filter out variables with only zero values
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE); dim(X_general)
set.seed(42)
y = as.vector(yvec_trie)
X = X_general
# Number of desired samples in each class
nombre_echantillons_par_classe <- min(table(y))
# Function to balance the classes
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe,seed =123) {
classes <- unique(y)
indices_equilibres <- integer(0)
for (classe in classes) {
indices_classe <- which(y == classe)
set.seed(seed)
indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
}
return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X
# Verify the distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)
# Split yvec_trie into 80% train and 20% test
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[,indices_division]
X.test <- X_equilibre[,-indices_division]
table(y)
table(y.test)
dim(X)
dim(X.test)
clf <- terBeam_mc(sparsity = c(2,3,4,5),
max.nb.features = 1000,
seed = 1,
nCores = 1,
evalToFit = "accuracy_",
objective = "auc",
experiment.id = "terBeam_mc",
experiment.save = "nothing")
printy(clf)
runit = TRUE
if(runit)
{
res_clf_ovo <- fit_mc(X = X, y = y, clf = clf,approch="ovo", cross.validate = TRUE, nfolds= 10); # class(res_clf)
# save results
save(res_clf_ovo, clf, file = "res_clf_ovo.rda", compression_level = 9)
}
# ... Database X is not a matrix! Converting ...
# ... Classification mode, computing factor(y) for speedup and robustness
# ... Loading feature correlation for speedup
# ... Correlation file loaded
# ... Storing data in the classifier object for speedup
# ... Computing ternary coefficients for speedup
# ... One seed found, setting by default
# ... Running the classifier terga2 with a single CPU
# ... Second and faster version of terga fitting based on Genetic Algorithm heuristics ...
# ... Cross validation mode
# ... Starting cross validation not in parallel
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ... Learning process is finished succesfuly
# ... Thank you for using Predomics. Don't forget to digest the results now.
#
# [1] "experiment" "predomics"
