legend.title = element_blank(),
legend.position = "bottom")
best_model_ranking_aggregation = Population_ranking_Aggregation[[3]][[1]]
best_model_ranking_aggregation$accuracy_
best_model_ranking_aggregation$confusionMatrix_
best_model_ranking_aggregation$coeffs_
best_model_ranking_aggregation$predictions_aggre
library(caret)
library(e1071)
X_train = t(X)
y_train = y
X_test = t(X.test)
y_test =y.test
y_train <- as.factor(y_train)
y_test <- as.factor(y_test)
# Define 10-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)
# Train the multiclass random forest model with 10-fold cross-validation
rf_model <- train(x = X_train, y = y_train, method = "rf", trControl = train_control)
# Prediction on the training data
train_pred <- predict(rf_model, X_train)
# Calculation of metrics for the training data
train_metrics <- confusionMatrix(train_pred, y_train)
# Calculation of the AUC for training
roc_train <- multiclass.roc(y_train, as.numeric(train_pred))
auc_train <- auc(roc_train)
# Displaying metrics for training
print(train_metrics)
print(auc_train)
# Prediction on the test data
test_pred <- predict(rf_model, X_test)
# Calculation of metrics for the test data
test_metrics <- confusionMatrix(test_pred, y_test)
# Calculation of the AUC for the test data.
roc_test <- multiclass.roc(y_test, as.numeric(test_pred))
auc_test <- auc(roc_test)
# Displaying metrics for the test data
print(test_metrics)
print(auc_test)
# Calculating metrics for the training data
train_accuracy <- train_metrics$overall["Accuracy"]
train_precision <- mean(train_metrics$byClass[,"Precision"], na.rm = TRUE)
train_recall <- mean(train_metrics$byClass[,"Recall"], na.rm = TRUE)
train_auc <- auc_train
# Calculer le score F1 pour les données d'entraînement
f1_train <- 2 * (train_precision * train_recall) / (train_precision + train_recall)
# Afficher le score F1 pour les données d'entraînement
print(paste("Train F1 Score:", f1_train))
# Displaying metrics for the training data
print(paste("Train Accuracy:", train_accuracy))
print(paste("Train Precision:", train_precision))
print(paste("Train Recall:", train_recall))
print(paste("Train AUC:", train_auc))
# Calculating metrics for the test data
test_metrics <- confusionMatrix(test_pred, y_test)
test_accuracy <- test_metrics$overall["Accuracy"]
test_precision <- mean(test_metrics$byClass[,"Precision"], na.rm = TRUE)
test_recall <- mean(test_metrics$byClass[,"Recall"], na.rm = TRUE)
# Calculating the AUC for the test data
roc_test <- multiclass.roc(y_test, as.numeric(test_pred))
test_auc <- auc(roc_test)
# Calculer et afficher le score F1 pour les données de test
test_precision <- mean(test_metrics$byClass[,"Precision"], na.rm = TRUE)
test_recall <- mean(test_metrics$byClass[,"Recall"], na.rm = TRUE)
f1_test <- 2 * (test_precision * test_recall) / (test_precision + test_recall)
print(paste("Test F1 Score:", f1_test))
# Displaying metrics for the test data
print(paste("Test Accuracy:", test_accuracy))
print(paste("Test Precision:", test_precision))
print(paste("Test Recall:", test_recall))
print(paste("Test AUC:", test_auc))
library(ggplot2)
# Example of metrics for the training data
accuracy_train <- train_accuracy
precision_train <- train_precision
AUC_train <- train_auc
recall_train <- train_recall
f1_train <- f1_train
# Example of metrics for the test data
accuracy_test <- test_accuracy
precision_test <- test_precision
AUC_test <- test_auc
recall_test <- test_recall
f1_test <- f1_test
# Create a data frame for the training metrics
train_metrics <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_train, precision_train, f1_train, recall_train),
Dataset = "Training"
)
# Create a data frame for the test metrics
test_metrics <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_test, precision_test, f1_test, recall_test),
Dataset = "Test"
)
# Combine the training and test metrics
all_metrics <- rbind(train_metrics, test_metrics)
# Create the graph
plot <- ggplot(all_metrics, aes(x = Metric, y = Value, fill = Dataset)) +
geom_bar(stat = "identity", position = "dodge") +
geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
labs(title = "Comparison of test and training metrics Sota RF",
x = "Metrics", y = "Values") +
theme_minimal()
# Display the graph
print(plot)
library(nnet)
# Create a data.frame from matrix X
library(glmnet)
# Utilize cv.glmnet with 10-fold cross-validation
df <- as.data.frame(t(X))
df$y <- factor(y)
cv_model_rl <- cv.glmnet(as.matrix(df[, -ncol(df)]), df$y, family = "multinomial", alpha = 1, nfolds = 10)
# Get the best model based on cross-validation
best_model_rl <- cv_model_rl$glmnet.fit
# Obtain predictions directly from cross-validation
predictions_rl <- predict(cv_model_rl, newx = as.matrix(df[, -ncol(df)]), s = "lambda.min", type = "class")
predictions_rl <- factor(predictions_rl)
# Check the dimensions and create the confusion matrix
if(length(predictions_rl) == length(df$y)) {
confusionMatrix(predictions_rl, df$y)
} else {
print("The dimensions of the predictions do not match those of the ground truth")
}
conf_matrix_rl <- confusionMatrix(predictions_rl, df$y)
# Calcul de l'exactitude
accuracy_train_rl <- conf_matrix_rl$overall[["Accuracy"]]
# Calcul de la précision moyenne de chaque classe
precision_train_rl <- mean(conf_matrix_rl$byClass[ , "Precision"])
# Calcul de l'AUC en utilisant la précision équilibrée moyenne (Balanced Accuracy)
AUC_train_rl <- mean(conf_matrix_rl$byClass[ , "Balanced Accuracy"])
# Calcul du rappel moyen (Sensibilité) de chaque classe
recall_train_rl <- mean(conf_matrix_rl$byClass[ , "Sensitivity"])
# Calcul du score F1 pour les données d'entraînement
F1_train_rl <- 2 * (precision_train_rl * recall_train_rl) / (precision_train_rl + recall_train_rl)
# Affichage des métriques pour les données d'entraînement
print(paste("Train Accuracy:", accuracy_train_rl))
print(paste("Train Precision:", precision_train_rl))
print(paste("Train Recall (Sensitivity):", recall_train_rl))
print(paste("Train AUC (Balanced Accuracy):", AUC_train_rl))
print(paste("Train F1 Score:", F1_train_rl))
predictions_test1_rl <- predict(cv_model_rl, newx = as.matrix(X_test), type = "class")
library(caret)
predictions_test1_rl <- as.factor(predictions_test1_rl)
# Create the confusion matrix for predictions on the test dataset
conf_matrix_test_rl <- confusionMatrix(predictions_test1_rl, y_test)
# Calculate accuracy, precision, AUC, and recall for the test dataset
# Calculate accuracy, precision, AUC, and recall for the test dataset
accuracy_test1_rl <- conf_matrix_test_rl$overall[["Accuracy"]]
precision_test1_rl <- mean(conf_matrix_test_rl$byClass[ , "Precision"], na.rm = TRUE)
AUC_test1_rl <- mean(conf_matrix_test_rl$byClass[ , "Balanced Accuracy"], na.rm = TRUE)
recall_test1_rl <- mean(conf_matrix_test_rl$byClass[ ,"Sensitivity"], na.rm = TRUE)
# Calcul du score F1 pour les données d'entraînement
F1_test1_rl <- 2 * (precision_test1_rl * recall_test1_rl) / (precision_test1_rl + recall_test1_rl)
library(ggplot2)
# Create a data frame for the training metrics
train_metrics_rl <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_train_rl, precision_train_rl, F1_train_rl, recall_train_rl),
Dataset = "Training"
)
# Create a data frame for the test metrics
test_metrics_rl <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_test1_rl, precision_test1_rl, F1_test1_rl, recall_test1_rl),
Dataset = "Test"
)
# Combine the training and test metrics
all_metrics_rl <- rbind(test_metrics_rl,train_metrics_rl)
# Create the graph
plot <- ggplot(all_metrics_rl, aes(x = Metric, y = Value, fill = Dataset)) +
geom_bar(stat = "identity", position = "dodge") +
geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
labs(title = "Comparison of test and training metrics Sota LR",
x = "Metrics", y = "Values") +
theme_minimal()
# Display the graph
print(plot)
library(caret)
library(pROC)
# Train an SVM model with radial kernel and 10-fold cross-validation
svm_model <- train(x = X_train, y = y_train, method = "svmRadial", trControl = trainControl(method = "cv", number = 10))
# Make predictions with the SVM model
svm_predictions_train <- predict(svm_model, X_train)
svm_predictions_test <- predict(svm_model, X_test)
# Calculate the accuracy for the training and test sets
accuracy_train_svm <- confusionMatrix(data = svm_predictions_train, reference = y_train)$overall['Accuracy'][1]
accuracy_test_svm <- confusionMatrix(data = svm_predictions_test, reference = y_test)$overall['Accuracy'][1]
# Calculate precision, recall, and AUC for the training and test sets
precision_train_svm <- confusionMatrix(data = svm_predictions_train, reference = y_train)$byClass[, 'Pos Pred Value']
precision_test_svm <- confusionMatrix(data = svm_predictions_test, reference = y_test)$byClass[, 'Pos Pred Value']
recall_train_svm <- confusionMatrix(data = svm_predictions_train, reference = y_train)$byClass[, 'Sensitivity']
recall_test_svm <- confusionMatrix(data = svm_predictions_test, reference = y_test)$byClass[, 'Sensitivity']
F1_train_svm <- 2 * (precision_train_svm * recall_train_svm) / (precision_train_svm + recall_train_svm)
F1_train = mean(F1_train_svm)
auc_train_svm <- roc(y_train, as.numeric(svm_predictions_train))$auc
auc_test_svm <- roc(y_test, as.numeric(svm_predictions_test))$auc
precision_train_svm = mean(precision_train_svm)
precision_test_svm = mean(precision_test_svm)
recall_train_svm = mean(recall_train_svm)
recall_test_svm = mean(recall_test_svm)
F1_test_svm <- 2 * (precision_test_svm * recall_test_svm) / (precision_test_svm + recall_test_svm)
F1_test_svm <- mean(F1_test_svm)
library(ggplot2)
# Create a data frame for the training metrics
train_metrics_svm <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_train_svm, precision_train_svm, F1_train, recall_train_svm),
Dataset = "Training"
)
# Create a data frame for the test metrics
test_metrics_svm <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_test_svm, precision_test_svm, F1_test_svm, recall_test_svm),
Dataset = "Test"
)
# Combine the training and test metrics
all_metrics_svm <- rbind(test_metrics_svm,train_metrics_svm)
# Create the graph
plot <- ggplot(all_metrics_svm, aes(x = Metric, y = Value, fill = Dataset)) +
geom_bar(stat = "identity", position = "dodge") +
geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
labs(title = "Comparison of test and training metrics Sota SVM",
x = "Metrics", y = "Values") +
theme_minimal()
# Display the graph
print(plot)
accuracy_val = best_model_voting_aggregation$accuracy_
load("res_clf_ovo.rda")
bests_models_parsimony_ovo <- digestmc(obj = res_clf_ovo, penalty = 0.75/100, plot = TRUE)
pop = res_clf_ovo$classifier$models
Population_voting_Aggregation <- evaluatePopulation_aggregation(pop = pop, y = y.test, X = X.test, force.re.evaluation = TRUE, clf,  aggregation = "votingAggregation")
extract_metric_values <- function(aggregation_list, metric_name) {
values <- c()
for (i in seq_along(aggregation_list)) {
values <- c(values, aggregation_list[[i]][[1]][[metric_name]])
}
return(values)
}
data <- data.frame(
Metric = rep(c("Accuracy", "Precision", "Recall", "F1"), each = 4),
Value = c(
extract_metric_values(Population_voting_Aggregation, "accuracy_") * 100,
extract_metric_values(Population_voting_Aggregation, "precision_") * 100,
extract_metric_values(Population_voting_Aggregation, "recall_") * 100,
extract_metric_values(Population_voting_Aggregation, "f1_") * 100
),
Method = rep(c("K_2", "K_3", "K_4", "K_5"), times = 4)
)
colors <- c("Accuracy" = "#1f77b4", "Precision" = "#ff7f0e", "Recall" = "#2ca02c", "F1" = "#d62728")
ggplot(data, aes(x = Method, y = Value, fill = Metric)) +
geom_boxplot(color = "black", outlier.color = "black") +
scale_fill_manual(values = colors) +  # Utiliser la palette de couleurs définie
facet_wrap(~Metric, scales = "free_y") +
labs(x = "Metrics", y = "Value", title = "Metrics by sparsity") +
ylim(0, 100) +  # Définir la limite de l'axe y de 0 à 100
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA),
legend.title = element_blank(),
legend.position = "bottom")
best_model_voting_aggregation = Population_voting_Aggregation[[4]][[1]]
best_model_voting_aggregation$accuracy_
best_model_voting_aggregation$confusionMatrix_
best_model_voting_aggregation$coeffs_
best_model_voting_aggregation$predictions_aggre
pop = res_clf_ovo$classifier$models
Population_weighted_Aggregation <- evaluatePopulation_aggregation(pop = pop, y = y.test, X = X.test, force.re.evaluation = TRUE, clf,  aggregation = "weightedAggregation")
data <- data.frame(
Metric = rep(c("Accuracy", "Precision", "Recall", "F1"), each = 4),
Value = c(
extract_metric_values(Population_weighted_Aggregation, "accuracy_") * 100,
extract_metric_values(Population_weighted_Aggregation, "precision_") * 100,
extract_metric_values(Population_weighted_Aggregation, "recall_") * 100,
extract_metric_values(Population_weighted_Aggregation, "f1_") * 100
),
Method = rep(c("K_2", "K_3", "K_4", "K_5"), times = 4)
)
colors <- c("Accuracy" = "#1f77b4", "Precision" = "#ff7f0e", "Recall" = "#2ca02c", "F1" = "#d62728")
ggplot(data, aes(x = Method, y = Value, fill = Metric)) +
geom_boxplot(color = "black", outlier.color = "black") +
scale_fill_manual(values = colors) +  # Utiliser la palette de couleurs définie
facet_wrap(~Metric, scales = "free_y") +
labs(x = "Metrics", y = "Value", title = "Metrics by sparsity") +
ylim(0, 100) +  # Définir la limite de l'axe y de 0 à 100
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA),
legend.title = element_blank(),
legend.position = "bottom")
best_model_weighted_aggregation = Population_weighted_Aggregation[[4]][[1]]
best_model_weighted_aggregation$accuracy_
best_model_weighted_aggregation$confusionMatrix_
best_model_weighted_aggregation$coeffs_
best_model_weighted_aggregation$predictions_aggre
accuracy_val = best_model_voting_aggregation$accuracy_
precision_val = best_model_voting_aggregation$precision_
recall_val = best_model_voting_aggregation$recall_
f1_val <- best_model_voting_aggregation$f1_
# Création du dataframe sans la matrice de confusion
df_voting <- data.frame(
Algorithme = "Voting Aggregation",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_voting
library(ggplot2)
# Charger les données
df_voting <- data.frame(
Algorithme = c("Voting Aggregation", "Voting Aggregation", "Voting Aggregation", "Voting Aggregation"),
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = c("Test", "Test", "Test", "Test"),
Values = c(0.7272727, 0.7365591, 0.7272727, 0.7318865)
)
# Créer le graphique à barres
ggplot(df_voting, aes(x = Metrics, y = Values, fill = Metrics)) +
geom_bar(stat = "identity", position = "dodge", width = 0.7) +
labs(title = "Performance Metrics for Voting Aggregation",
x = "Metrics", y = "Values") +
theme_minimal() +
theme(legend.position = "none")
library(ggplot2)
# Charger les données
df_voting <- data.frame(
Algorithme = c("Voting Aggregation", "Voting Aggregation", "Voting Aggregation", "Voting Aggregation"),
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = c("Test", "Test", "Test", "Test"),
Values = c(0.7272727, 0.7365591, 0.7272727, 0.7318865)
)
# Créer le graphique en nuage de points
ggplot(df_voting, aes(x = Metrics, y = Values, color = Metrics)) +
geom_point(size = 3) +
labs(title = "Performance Metrics for Voting Aggregation",
x = "Metrics", y = "Values") +
theme_minimal() +
theme(legend.position = "none") +
geom_text(aes(label = round(Values, 3)), vjust = -0.5)
library(ggplot2)
# Charger les données
df_voting <- data.frame(
Algorithme = c("Voting Aggregation", "Voting Aggregation", "Voting Aggregation", "Voting Aggregation"),
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = c("Test", "Test", "Test", "Test"),
Values = c(0.7272727, 0.7365591, 0.7272727, 0.7318865)
)
# Créer le graphique en nuage de points avec des limites spécifiques
ggplot(df_voting, aes(x = Metrics, y = Values, color = Metrics)) +
geom_point(size = 3) +
labs(title = "Performance Metrics for Voting Aggregation",
x = "Metrics", y = "Values") +
theme_minimal() +
theme(legend.position = "none") +
geom_text(aes(label = round(Values, 3)), vjust = -0.5) +
xlim(0, 100) +
ylim(0, 100)
library(ggplot2)
# Charger les données
df_voting <- data.frame(
Algorithme = c("Voting Aggregation", "Voting Aggregation", "Voting Aggregation", "Voting Aggregation"),
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = c("Test", "Test", "Test", "Test"),
Values = c(0.7272727, 0.7365591, 0.7272727, 0.7318865)
)
# Créer le graphique en nuage de points avec des limites spécifiques
ggplot(df_voting, aes(x = Metrics, y = Values, color = Metrics)) +
geom_point(size = 3) +
labs(title = "Performance Metrics for Voting Aggregation",
x = "Metrics", y = "Values") +
theme_minimal() +
theme(legend.position = "none") +
geom_text(aes(label = round(Values, 3)), vjust = -0.5) +
scale_y_continuous(limits = c(0, 1)) +  # Ajuster l'échelle de l'axe y
scale_x_discrete()  # Utiliser une échelle discrète pour l'axe x
library(ggplot2)
# Charger les données
df_voting <- data.frame(
Algorithme = c("Voting Aggregation", "Voting Aggregation", "Voting Aggregation", "Voting Aggregation"),
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = c("Test", "Test", "Test", "Test"),
Values = c(0.7272727, 0.7365591, 0.7272727, 0.7318865)
)
# Créer le graphique en nuage de points avec des limites spécifiques
ggplot(df_voting, aes(x = Metrics, y = Values, color = Metrics)) +
geom_point(size = 3) +
labs(title = "Performance Metrics Best Model for Voting Aggregation",
x = "Metrics", y = "Values") +
theme_minimal() +
theme(legend.position = "none") +
geom_text(aes(label = round(Values, 3)), vjust = -0.5) +
scale_y_continuous(limits = c(0, 1)) +  # Ajuster l'échelle de l'axe y
scale_x_discrete()  # Utiliser une échelle discrète pour l'axe x
best_model_weighted_aggregation$accuracy_
best_model_weighted_aggregation$precision_
best_model_weighted_aggregation$recall_
best_model_weighted_aggregation$f1_
library(ggplot2)
# Charger les données
df_voting <- data.frame(
Algorithme = c("Voting Aggregation", "Voting Aggregation", "Voting Aggregation", "Voting Aggregation"),
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = c("Test", "Test", "Test", "Test"),
Values = c(0.7348, 0.7463, 0.7348, 0.74057)
)
# Créer le graphique en nuage de points avec des limites spécifiques
ggplot(df_voting, aes(x = Metrics, y = Values, color = Metrics)) +
geom_point(size = 3) +
labs(title = "Performance Metrics Best Model for Weighted Aggregation",
x = "Metrics", y = "Values") +
theme_minimal() +
theme(legend.position = "none") +
geom_text(aes(label = round(Values, 3)), vjust = -0.5) +
scale_y_continuous(limits = c(0, 1)) +  # Ajuster l'échelle de l'axe y
scale_x_discrete()  # Utiliser une échelle discrète pour l'axe x
# get the best model
best.model =  best_model_weighted_aggregation
printy(best.model)
printy_mc(best.model)
printModel_mc(best.model)
best.model$sign_
best.model$intercept_
load("res_clf_ova.rda")
bests_models_parsimony_ova <- digestmc(obj = res_clf_ova, penalty = 0.75/100, plot = TRUE)
pop = res_clf_ova$classifier$models
Population_newApproach_Aggregation <- evaluatePopulation_aggregation(pop = pop, y = y.test, X = X.test, force.re.evaluation = TRUE, clf,  aggregation = "NewApproach")
data <- data.frame(
Metric = rep(c("Accuracy", "Precision", "Recall", "F1"), each = 4),
Value = c(
extract_metric_values(Population_newApproach_Aggregation, "accuracy_") * 100,
extract_metric_values(Population_newApproach_Aggregation, "precision_") * 100,
extract_metric_values(Population_newApproach_Aggregation, "recall_") * 100,
extract_metric_values(Population_newApproach_Aggregation, "f1_") * 100
),
Method = rep(c("K_2", "K_3", "K_4", "K_5"), times = 4)
)
colors <- c("Accuracy" = "#1f77b4", "Precision" = "#ff7f0e", "Recall" = "#2ca02c", "F1" = "#d62728")
ggplot(data, aes(x = Method, y = Value, fill = Metric)) +
geom_boxplot(color = "black", outlier.color = "black") +
scale_fill_manual(values = colors) +  # Utiliser la palette de couleurs définie
facet_wrap(~Metric, scales = "free_y") +
labs(x = "Metrics", y = "Value", title = "Metrics by sparsity") +
ylim(0, 100) +  # Définir la limite de l'axe y de 0 à 100
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA),
legend.title = element_blank(),
legend.position = "bottom")
best_model_newapproach_aggregation = Population_newApproach_Aggregation[[3]][[1]]
best_model_newapproach_aggregation$accuracy_
best_model_newapproach_aggregation$confusionMatrix_
best_model_newapproach_aggregation$coeffs_
best_model_newapproach_aggregation$predictions_aggre
best_model_newapproach_aggregation$accuracy_
best_model_newapproach_aggregation$precision_
best_model_newapproach_aggregation$recall_
best_model_newapproach_aggregation$f1_
library(ggplot2)
# Charger les données
df_voting <- data.frame(
Algorithme = c("Voting Aggregation", "Voting Aggregation", "Voting Aggregation", "Voting Aggregation"),
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = c("Test", "Test", "Test", "Test"),
Values = c(0.6818, 0.7087, 0.6818, 0.6950)
)
# Créer le graphique en nuage de points avec des limites spécifiques
ggplot(df_voting, aes(x = Metrics, y = Values, color = Metrics)) +
geom_point(size = 3) +
labs(title = "Performance Metrics Best Model for New Approach Aggregation",
x = "Metrics", y = "Values") +
theme_minimal() +
theme(legend.position = "none") +
geom_text(aes(label = round(Values, 3)), vjust = -0.5) +
scale_y_continuous(limits = c(0, 1)) +  # Ajuster l'échelle de l'axe y
scale_x_discrete()  # Utiliser une échelle discrète pour l'axe x
best_model_maximization_aggregation$accuracy_
best_model_maximization_aggregation$precision_
best_model_maximization_aggregation$recall_
best_model_maximization_aggregation$f1_
library(ggplot2)
# Charger les données
df_voting <- data.frame(
Algorithme = c("Voting Aggregation", "Voting Aggregation", "Voting Aggregation", "Voting Aggregation"),
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = c("Test", "Test", "Test", "Test"),
Values = c(0.6590, 0.6681, 0.6590, 0.6636)
)
# Créer le graphique en nuage de points avec des limites spécifiques
ggplot(df_voting, aes(x = Metrics, y = Values, color = Metrics)) +
geom_point(size = 3) +
labs(title = "Performance Metrics Best Model for Maximization Aggregation",
x = "Metrics", y = "Values") +
theme_minimal() +
theme(legend.position = "none") +
geom_text(aes(label = round(Values, 3)), vjust = -0.5) +
scale_y_continuous(limits = c(0, 1)) +  # Ajuster l'échelle de l'axe y
scale_x_discrete()  # Utiliser une échelle discrète pour l'axe x
library(ggplot2)
# Charger les données
df_voting <- data.frame(
Algorithme = c("Voting Aggregation", "Voting Aggregation", "Voting Aggregation", "Voting Aggregation"),
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = c("Test", "Test", "Test", "Test"),
Values = c(0.6590, 0.6681, 0.6590, 0.6636)
)
# Créer le graphique en nuage de points avec des limites spécifiques
ggplot(df_voting, aes(x = Metrics, y = Values, color = Metrics)) +
geom_point(size = 3) +
labs(title = "Performance Metrics Best Model for Ranking Aggregation",
x = "Metrics", y = "Values") +
theme_minimal() +
theme(legend.position = "none") +
geom_text(aes(label = round(Values, 3)), vjust = -0.5) +
scale_y_continuous(limits = c(0, 1)) +  # Ajuster l'échelle de l'axe y
scale_x_discrete()  # Utiliser une échelle discrète pour l'axe x
