for(i in 1:length(obj$indices_)){
obj$indices_[[i]]  <- match(obj$names_[[i]] , features)
if(any(is.na(obj$indices_[[i]] )))
{
warning("Some indices are not found.")
}
}
return(obj)
}
################################################################
# DIGESTING RESULTS
################################################################
#' Summarize the results from an experiment object
#'
#' @description Sumarizes the results of an experiment object of the type
#' `obj$classifier` and `obj$crossval`. This is different from the digestMC(),
#' which sumarizes a model collection obj$models
#' @import ggplot2
#' @param obj: The experiment object resulting from the learning process `fit()`
#' @param penalty: A coefficient between 0 and 1, which is applied to penalize
#' the performance of models as a consequence of model-size. We use this to select
#' the best model of the population of models (default:NULL)
#' @param best.cv: Should we chose the best model based on information learnerd
#' cross validation (default:TRUE). This will work if the crossvalidation data is
#' available. If not the best model will be selected with empirical results.
#' @param best.k: If we do not wish to let the algorithm select the model size,
#' we can fix this by setting the best.k with an integer indicating the number of
#' variables in the model (default:NULL).
#' @param plot: Should the digested results be plotted ? (default:FALSE)
#' @param omit.na: Omit data with empty results (default:TRUE)
#' @return an object with digested information such as the best models for each
#' model-size, their respective scores, the best model.
#' @export
digestmc <- function(obj,
penalty = NULL,
best.cv = TRUE,
best.k = NULL,
plot = FALSE,
omit.na = TRUE)
{
if(!isExperiment(obj))
{
stop("digest: The object did not pass the sanity check for an Experiment object!")
}
# so that it works with below
if(is.null(penalty))
{
penalty <- 0
}
if(length(penalty) > 1)
{
stop("digest: Please provide a single value for penalty!")
}
res                       <- list()
res$learner               <- obj$classifier$learner
# Empirical
res$best                  <- list()
res$best$models           <- getNBestModels(obj = obj,
significance = TRUE,
by.k.sparsity = TRUE,
k.penalty = penalty,
n.best = 1,
single.best = FALSE,
single.best.cv = best.cv,
single.best.k = best.k,
max.min.prevalence = FALSE,
X = NULL,
verbose = FALSE,
evalToOrder = "fit_",
return.population = TRUE, # population
unique.control = TRUE
)
# sanity check
if(is.null(res$best$models))
{
warning("digest: no models are found. Returning empty handed.")
return(NULL)
}
res$best$model            <- getNBestModels(obj = obj,
significance = TRUE,
by.k.sparsity = TRUE,
k.penalty = penalty,
n.best = 5,
single.best = TRUE, # give best
single.best.cv = best.cv, # based on CV
single.best.k = best.k,
max.min.prevalence = FALSE,
X = NULL,
verbose = FALSE,
evalToOrder = "fit_",
return.population = FALSE # population
)
res$best$scores           <- list()
res$best$scores$fit_      <- populationGet_X(element2get = "fit_", toVec = TRUE, na.rm = FALSE)(pop = res$best$models)
res$best$scores$auc_      <- populationGet_X(element2get = "auc_", toVec = TRUE, na.rm = FALSE)(pop = res$best$models)
res$best$scores$accuracy_ <- populationGet_X(element2get = "accuracy_", toVec = TRUE, na.rm = FALSE)(pop = res$best$models)
res$best$scores$precision_<- populationGet_X(element2get = "precision_", toVec = TRUE, na.rm = FALSE)(pop = res$best$models)
res$best$scores$recall_   <- populationGet_X(element2get = "recall_", toVec = TRUE, na.rm = FALSE)(pop = res$best$models)
res$best$scores$f1_       <- populationGet_X(element2get = "f1_", toVec = TRUE, na.rm = FALSE)(pop = res$best$models)
res$best$scores$cor_      <- populationGet_X(element2get = "cor_", toVec = TRUE, na.rm = FALSE)(pop = res$best$models)
res$best$scores$ser_      <- populationGet_X(element2get = "ser_", toVec = TRUE, na.rm = FALSE)(pop = res$best$models)
res$best$scores$rsq_      <- populationGet_X(element2get = "rsq_", toVec = TRUE, na.rm = FALSE)(pop = res$best$models)
# Cross Validation (if activated)
res$cv <- list()
if(!is.null(obj$crossVal))
{
res$cv                  <-   obj$crossVal
crossval                <- TRUE
}else
{
warning(paste("crossval information unavailable for", obj$learner))
crossval                <- FALSE
}
maj.class <- NA # default value for maj.class
# Majoritary class for all folds
if(!is.null(obj$classifier$lfolds))
{
if(obj$classifier$params$objective == "auc")
{
if(obj$classifier$params$evalToFit == "accuracy_")
{
maj.class           <- rep(NA, length(obj$classifier$lfolds))
# if accuracy
for(i in 1:length(maj.class))
{
maj.class[i]      <- max(table(obj$classifier$data$y[-obj$classifier$lfolds[[i]]])/length(obj$classifier$data$y[-obj$classifier$lfolds[[i]]]))
}
}else
{
# if AUC
maj.class           <- 0.5
}
} # if correlation it is NA
}
##########################################################
# Plotting the results
##########################################################
if(plot)
{
# set the limits
ylim <- c(0,1)
# make an empty plot in case it does not work
g.empty <- ggplot(data.frame()) + geom_point() + xlim(0, 10) + ylim(ylim) +
theme_bw() + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) +
ylab("") +
xlab("Model parismony sparse") +
ggtitle("") +
geom_hline(aes(yintercept=1), lty=2, col="lightgray") +
geom_hline(aes(yintercept=0.5), lty=2, col="lightgray")
# if correlation
if(obj$classifier$params$objective=="cor")
{
if(crossval)
{
#-----------------------------------------------------
# CORRELATION
#-----------------------------------------------------
# overall training accuracy learner results
dat <- res$cv$scores$empirical.cor
if(omit.na)
{
dat <- dat[rowSums(!is.na(dat))!=0,]
}
df <- data.frame(parsimony = rownames(dat),dat)
df.melt <- melt(df, id.vars = "parsimony")
df.melt$parsimony <- as.factor(df.melt$parsimony)
df.melt$parsimony <- factor(df.melt$parsimony,
levels = levels(df.melt$parsimony)[order(as.numeric(gsub("k_","",levels(df.melt$parsimony))))]
)
g.cor.emp.cv <- ggplot(data = df.melt, aes(y = value, x = parsimony)) +
geom_point(aes(color = variable), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
geom_boxplot(notch = FALSE, outlier.colour = NA, position = position_dodge(width=0.9), alpha = 0.3) +
ylab("cor_") +
xlab("Model parsimony") +
ggtitle("Training performance (CV)") +
ylim(ylim) +
theme_bw() +
#geom_hline(yintercept = unique(maj.class), col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
# overall testing accuracy learner results
dat <- res$cv$scores$generalization.cor
if(omit.na)
{
dat <- dat[rowSums(!is.na(dat))!=0,]
}
df <- data.frame(parsimony = rownames(dat),dat)
df.melt <- melt(df, id.vars = "parsimony")
df.melt$parsimony <- as.factor(df.melt$parsimony)
df.melt$parsimony <- factor(df.melt$parsimony,
levels = levels(df.melt$parsimony)[order(as.numeric(gsub("k_","",levels(df.melt$parsimony))))]
)
g.cor.gen.cv <- ggplot(data = df.melt, aes(y = value, x = parsimony)) +
geom_point(aes(color = variable), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
geom_boxplot(notch = FALSE, outlier.colour = NA, position = position_dodge(width=0.9), alpha = 0.3) +
ylab("cor_") +
xlab("Model parsimony") +
ggtitle("Testing performance (CV)") +
ylim(ylim) +
theme_bw() +
#geom_hline(yintercept = unique(maj.class), col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
}else
{
g.cor.emp.cv <- g.empty
g.cor.gen.cv <- g.empty
}
# RHO Empirical
v <- res$best$scores$cor_
df <- data.frame(value = v, parsimony = names(v))
df$parsimony <- factor(df$parsimony,
levels = levels(df$parsimony)[order(as.numeric(gsub("k_","",levels(df$parsimony))))]
)
g.cor.emp <- ggplot(data = df, aes(x = parsimony, y = value, group = 1)) +
geom_line(aes(color = "gray")) +
geom_point(size = 2, alpha = 1) +
scale_color_manual(values = "gray") +
ylab("cor_") +
xlab("Model parsimony") +
ggtitle("Emprical performance") +
labs(subtitle = paste("L:",obj$classifier$learner,"|F:",signif(res$best$model$cor_,4),"|k:",length(res$best$model$indices_[[1]]), sep="")) +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = mean(maj.class), col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
# RSQ Empirical (R squared)
v <- res$best$scores$rsq_
df <- data.frame(value = v, parsimony = names(v))
df$parsimony <- factor(df$parsimony,
levels = levels(df$parsimony)[order(as.numeric(gsub("k_","",levels(df$parsimony))))]
)
g.rsq.emp <- ggplot(data = df, aes(x = parsimony, y = value, group = 1)) +
geom_line(aes(color = "gray")) +
geom_point(size = 2, alpha = 1) +
scale_color_manual(values = "gray") +
ylab("rsq_") +
xlab("Model parsimony") +
ggtitle("Emprical performance") +
labs(subtitle = paste("L:",obj$classifier$learner,"|F:",signif(res$best$model$rsq_,4),"|k:",length(res$best$model$indices_[[1]]), sep="")) +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = mean(maj.class), col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
# SER Empirical (Standar error of the regression)
v <- res$best$scores$ser_
df <- data.frame(value = v, parsimony = names(v))
df$parsimony <- factor(df$parsimony,
levels = levels(df$parsimony)[order(as.numeric(gsub("k_","",levels(df$parsimony))))]
)
g.ser.emp <- ggplot(data = df, aes(x = parsimony, y = value, group = 1)) +
geom_line(aes(color = "gray")) +
geom_point(size = 2, alpha = 1) +
scale_color_manual(values = "gray") +
ylab("ser_") +
xlab("Model parsimony") +
ggtitle("Emprical performance") +
labs(subtitle = paste("L:",obj$classifier$learner,"|F:",signif(res$best$model$ser_,4),"|k:",length(res$best$model$indices_[[1]]), sep="")) +
#ylim(ylim) +
theme_bw() +
geom_hline(yintercept = mean(maj.class), col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
grid.arrange(g.cor.emp.cv, g.cor.gen.cv,
# empricial
g.cor.emp, g.rsq.emp,
g.ser.emp,
ncol = 2)
}else # if classification
{
if(crossval)
{
# make an empty plot in case it does not work
g.empty <- ggplot(data.frame()) + geom_point() + xlim(0, 10) + ylim(ylim) +
theme_bw() + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) +
ylab("") +
xlab("Model parismony sparse") +
ggtitle("") +
geom_hline(aes(yintercept=1), lty=2, col="lightgray") +
geom_hline(aes(yintercept=0.5), lty=2, col="lightgray")
#-----------------------------------------------------
# ACCURACY
#-----------------------------------------------------
# overall training accuracy learner results
dat <- res$cv$scores$empirical.acc
if(omit.na)
{
dat <- dat[rowSums(!is.na(dat))!=0,]
}
df <- data.frame(parsimony = rownames(dat),dat)
df.melt <- melt(df, id.vars = "parsimony")
df.melt$parsimony <- as.factor(df.melt$parsimony)
df.melt$parsimony <- factor(df.melt$parsimony,
levels = levels(df.melt$parsimony)[order(as.numeric(gsub("k_","",levels(df.melt$parsimony))))]
)
g.accuracy.emp.cv <- ggplot(data = df.melt, aes(y = value, x = parsimony)) +
geom_point(aes(color = variable), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
geom_boxplot(notch = FALSE, outlier.colour = NA, position = position_dodge(width=0.9), alpha = 0.3) +
ylab("accuracy_") +
xlab("Model parsimony") +
ggtitle("Training performance (CV)") +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = unique(maj.class), col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
# overall testing accuracy learner results
dat <- res$cv$scores$generalization.acc
if(omit.na)
{
dat <- dat[rowSums(!is.na(dat))!=0,]
}
df <- data.frame(parsimony = rownames(dat),dat)
df.melt <- melt(df, id.vars = "parsimony")
df.melt$parsimony <- as.factor(df.melt$parsimony)
df.melt$parsimony <- factor(df.melt$parsimony,
levels = levels(df.melt$parsimony)[order(as.numeric(gsub("k_","",levels(df.melt$parsimony))))]
)
g.accuracy.gen.cv <- ggplot(data = df.melt, aes(y = value, x = parsimony)) +
geom_point(aes(color = variable), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
geom_boxplot(notch = FALSE, outlier.colour = NA, position = position_dodge(width=0.9), alpha = 0.3) +
ylab("accuracy_") +
xlab("Model parsimony") +
ggtitle("Testing performance (CV)") +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = unique(maj.class), col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
#-----------------------------------------------------
# AUC
#-----------------------------------------------------
# overall training accuracy learner results
dat <- res$cv$scores$empirical.auc
if(omit.na)
{
dat <- dat[rowSums(!is.na(dat))!=0,]
}
df <- data.frame(parsimony = rownames(dat),dat)
df.melt <- melt(df, id.vars = "parsimony")
df.melt$parsimony <- as.factor(df.melt$parsimony)
df.melt$parsimony <- factor(df.melt$parsimony,
levels = levels(df.melt$parsimony)[order(as.numeric(gsub("k_","",levels(df.melt$parsimony))))]
)
g.auc.emp.cv <- ggplot(data = df.melt, aes(y = value, x = parsimony)) +
geom_point(aes(color = variable), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
geom_boxplot(notch = FALSE, outlier.colour = NA, position = position_dodge(width=0.9), alpha = 0.3) +
ylab("auc_") +
xlab("Model parsimony") +
ggtitle("Training performance (CV)") +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = 0.5, col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
# overall testing accuracy learner results
dat <- res$cv$scores$generalization.auc
if(omit.na)
{
dat <- dat[rowSums(!is.na(dat))!=0,]
}
df <- data.frame(parsimony = rownames(dat),dat)
df.melt <- melt(df, id.vars = "parsimony")
df.melt$parsimony <- as.factor(df.melt$parsimony)
df.melt$parsimony <- factor(df.melt$parsimony,
levels = levels(df.melt$parsimony)[order(as.numeric(gsub("k_","",levels(df.melt$parsimony))))]
)
g.auc.gen.cv <- ggplot(data = df.melt, aes(y = value, x = parsimony)) +
geom_point(aes(color = variable), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
geom_boxplot(notch = FALSE, outlier.colour = NA, position = position_dodge(width=0.9), alpha = 0.3) +
ylab("auc_") +
xlab("Model parsimony") +
ggtitle("Testing performance (CV)") +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = 0.5, col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
}else
{
g.accuracy.emp.cv <- g.empty
g.accuracy.gen.cv <- g.empty
g.auc.emp.cv <- g.empty
g.auc.gen.cv <- g.empty
}
if(all(is.na(unlist(res$best$scores))))
{
g.accuracy.emp <- g.empty
g.auc.emp <- g.empty
g.recall.emp <- g.empty
g.precision.emp <- g.empty
}else
{
# ACCURACY Empirical
v <- res$best$scores$accuracy_
df <- data.frame(value = v, parsimony = names(v))
df$parsimony <- as.factor(df$parsimony)
df$parsimony <- factor(df$parsimony,
levels = levels(df$parsimony)[order(as.numeric(gsub("k_","",levels(df$parsimony))))]
)
g.accuracy.emp <- ggplot(data = df, aes(x = parsimony, y = value, group = 1)) +
geom_line(aes(color = "gray")) +
geom_point(size = 2, alpha = 1) +
scale_color_manual(values = "gray") +
ylab("accuracy_") +
xlab("Model parsimony") +
ggtitle("Emprical performance") +
labs(subtitle = paste("L:",obj$classifier$learner,"|F:",signif(res$best$model$accuracy_,4),"|k:",length(res$best$model$indices_[[1]]), sep="")) +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = mean(maj.class), col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
# AUC Empirical
v <- res$best$scores$auc_
df <- data.frame(value = v, parsimony = names(v))
df$parsimony <- as.factor(df$parsimony)
df$parsimony <- factor(df$parsimony,
levels = levels(df$parsimony)[order(as.numeric(gsub("k_","",levels(df$parsimony))))]
)
g.auc.emp <- ggplot(data = df, aes(x = parsimony, y = value, group = 1)) +
geom_line(aes(color = "gray")) +
geom_point(size = 2, alpha = 1) +
scale_color_manual(values = "gray") +
ylab("auc_") +
xlab("Model parsimony") +
ggtitle("Emprical performance") +
labs(subtitle = paste("L:",obj$classifier$learner,"|F:",signif(res$best$model$auc_,4),"|k:",length(res$best$model$indices_[[1]]), sep="")) +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = mean(maj.class), col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
# RECALL Empirical
v <- res$best$scores$recall_
df <- data.frame(value = v, parsimony = names(v))
df$parsimony <- as.factor(df$parsimony)
df$parsimony <- factor(df$parsimony,
levels = levels(df$parsimony)[order(as.numeric(gsub("k_","",levels(df$parsimony))))]
)
g.recall.emp <- ggplot(data = df, aes(x = parsimony, y = value, group = 1)) +
geom_line(aes(color = "gray")) +
geom_point(size = 2, alpha = 1) +
scale_color_manual(values = "gray") +
ylab("recall_") +
xlab("Model parsimony") +
ggtitle("Emprical performance") +
labs(subtitle = paste("L:",obj$classifier$learner,"|F:",signif(res$best$model$recall_,4),"|k:",length(res$best$model$indices_[[1]]), sep="")) +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = 0.5, col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
# PRECISION Empirical
v <- res$best$scores$precision_
df <- data.frame(value = v, parsimony = names(v))
df$parsimony <- as.factor(df$parsimony)
df$parsimony <- factor(df$parsimony,
levels = levels(df$parsimony)[order(as.numeric(gsub("k_","",levels(df$parsimony))))]
)
g.precision.emp <- ggplot(data = df, aes(x = parsimony, y = value, group = 1)) +
geom_line(aes(color = "gray")) +
geom_point(size = 2, alpha = 1) +
scale_color_manual(values = "gray") +
ylab("precision_") +
xlab("Model parsimony") +
ggtitle("Emprical performance") +
labs(subtitle = paste("L:",obj$classifier$learner,"|F:",signif(res$best$model$precision_,4),"|k:",length(res$best$model$indices_[[1]]), sep="")) +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = 0.5, col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
} # end existing scores
grid.arrange(g.accuracy.emp.cv, g.accuracy.gen.cv,
g.auc.emp.cv, g.auc.gen.cv,
# empricial
g.accuracy.emp, g.auc.emp,
g.recall.emp, g.precision.emp,
ncol = 2)
} # end classification
}
# return digested results
return(res)
}
feat1.import <- mergeMeltImportanceCV_mc(list.results = list(terBeam = terbeam_Majority_Voting_with_Tie_Breaking_metacardis_unconstrained_balance ),
filter.cv.prev = 0,
min.kfold.nb = FALSE,
learner.grep.pattern = "*",
#nb.top.features = 50,
feature.selection = fa,
scaled.importance = TRUE,
make.plot = TRUE,
cv.prevalence = FALSE)
feat2.import <- mergeMeltImportanceCV_mc(list.results = list(terBeam = terbeam_Majority_Voting_with_Tie_Breaking_metacardis_unconstrained_balance),
filter.cv.prev = 0,
min.kfold.nb = FALSE,
learner.grep.pattern = "*",
nb.top.features = 148,
#feature.selection = rownames(fa$pop.noz),
scaled.importance = TRUE,
make.plot = TRUE,
cv.prevalence = FALSE)
knitr::opts_chunk$set(echo = TRUE)
# get the best model
best.model = fbm[[1]]
# Visualize the model information (if needed)
printy_mc(best.model)
# Generate the plots
plots1 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ovo")
