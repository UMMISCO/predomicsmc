{
print("plotModel: please provide 2 colors for the ternary coefficients excluding zero")
return(NULL)
}
# disable importance for SOTA
if(isModelSota(mod) & importance)
{
importance <- FALSE
print("plotModel: importance graphic is disabled for SOTA models")
}
if(!isModelSotaRF(mod))
{
# Fix display names
if(isModelSotaSVM(mod))
{
mod$learner <- "sota"
}
# reset model attributes for glmnet for proper viewing
if(mod$learner == "terda" & mod$language == "logreg")
{
mod$learner <- "sota"
mod$language <- "glmnet"
}
if(sort.features)
{
if(is.null(sort.ind))
{
# get the order of the features in terms of discriminance compared to the class.
nClasse <- unique(y)
feature.cor   <- list() # List of different combinations of feature.cor
list_y <- list() #  List of different combinations of y
list_X <- list() #  List of different combinations of X
if (approch == "ovo") {
k <- 1
for (i in 1:(length(nClasse)-1)) {
for (j in (i+1):length(nClasse)) {
class_i <- nClasse[i]
class_j <- nClasse[j]
indices <- which(y == class_i | y == class_j)
y_pair <- y[indices]
X_pair <- X[, indices]
list_y[[k]] <- as.vector(y_pair)
list_X[[k]] <- X_pair
k <- k + 1
}
}
} else {
for (i in 1:length(nClasse)) {
class_i <- nClasse[i]
y_temp <- ifelse(y == class_i, as.character(class_i), "All")
list_y[[i]] <- as.vector(y_temp)
list_X[[i]] <- X
}
}
for (i in 1:(length(list_y))) {
ind[[i]]     <- order(filterfeaturesK(data = list_X[[i]],
trait = list_y[[i]],
k = k=nrow(X),
ind <- list()
# test model validity
if(!isModel(obj = mod))
{
print("plotModel: The model object is not valid!")
return(NULL)
}
if(length(col.sign) != 2)
{
print("plotModel: please provide 2 colors for the ternary coefficients excluding zero")
return(NULL)
}
# disable importance for SOTA
if(isModelSota(mod) & importance)
{
importance <- FALSE
print("plotModel: importance graphic is disabled for SOTA models")
}
if(!isModelSotaRF(mod))
{
# Fix display names
if(isModelSotaSVM(mod))
{
mod$learner <- "sota"
}
# reset model attributes for glmnet for proper viewing
if(mod$learner == "terda" & mod$language == "logreg")
{
mod$learner <- "sota"
mod$language <- "glmnet"
}
if(sort.features)
{
if(is.null(sort.ind))
{
# get the order of the features in terms of discriminance compared to the class.
nClasse <- unique(y)
feature.cor   <- list() # List of different combinations of feature.cor
list_y <- list() #  List of different combinations of y
list_X <- list() #  List of different combinations of X
if (approch == "ovo") {
k <- 1
for (i in 1:(length(nClasse)-1)) {
for (j in (i+1):length(nClasse)) {
class_i <- nClasse[i]
class_j <- nClasse[j]
indices <- which(y == class_i | y == class_j)
y_pair <- y[indices]
X_pair <- X[, indices]
list_y[[k]] <- as.vector(y_pair)
list_X[[k]] <- X_pair
k <- k + 1
}
}
} else {
for (i in 1:length(nClasse)) {
class_i <- nClasse[i]
y_temp <- ifelse(y == class_i, as.character(class_i), "All")
list_y[[i]] <- as.vector(y_temp)
list_X[[i]] <- X
}
}
for (i in 1:(length(list_y))) {
ind[[i]]     <- order(filterfeaturesK(data = list_X[[i]],
trait = list_y[[i]],
k =nrow(X),
sort = FALSE)$p,
decreasing = FALSE) # to avoid having to recompute this all the time
}
}else
{
ind       <- sort.ind
}
}else # no order (use the default X ordering)
{
ind         <- c(1:nrow(X))
}
# get the normalized coefficients
coeffsl <- normModelCoeffs_mc(mod = mod, X = X, y = y, sort.features = sort.features, sort.ind = ind)
# and the sparsity
k_sparsity <- mod$eval.sparsity
#k_sparsity <- k_sparsity[!unlist(lapply(coeffsl,is.null))]
#tmp <- unlist(lapply(coeffsl,length))
coeffs.name <- c()
coeffs.nr   <- c()
)
k_sparsity
ind <- list()
# test model validity
if(!isModel(obj = mod))
{
print("plotModel: The model object is not valid!")
return(NULL)
}
if(length(col.sign) != 2)
{
print("plotModel: please provide 2 colors for the ternary coefficients excluding zero")
return(NULL)
}
# disable importance for SOTA
if(isModelSota(mod) & importance)
{
importance <- FALSE
print("plotModel: importance graphic is disabled for SOTA models")
}
nClasse <- unique(y)
feature.cor   <- list() # List of different combinations of feature.cor
list_y <- list() #  List of different combinations of y
list_X <- list() #  List of different combinations of X
if (approch == "ovo") {
k <- 1
for (i in 1:(length(nClasse)-1)) {
for (j in (i+1):length(nClasse)) {
class_i <- nClasse[i]
class_j <- nClasse[j]
indices <- which(y == class_i | y == class_j)
y_pair <- y[indices]
X_pair <- X[, indices]
list_y[[k]] <- as.vector(y_pair)
list_X[[k]] <- X_pair
k <- k + 1
}
}
} else {
for (i in 1:length(nClasse)) {
class_i <- nClasse[i]
y_temp <- ifelse(y == class_i, as.character(class_i), "All")
list_y[[i]] <- as.vector(y_temp)
list_X[[i]] <- X
}
}
for (i in 1:(length(list_y))) {
ind[[i]]     <- order(filterfeaturesK(data = list_X[[i]],
trait = list_y[[i]],
k =nrow(X),
sort = FALSE)$p,
decreasing = FALSE) # to avoid having to recompute this all the time
}
}else
coeffsl
nClasse <- unique(y)
feature.cor   <- list() # List of different combinations of feature.cor
list_y <- list() #  List of different combinations of y
list_X <- list() #  List of different combinations of X
if (approch == "ovo") {
k <- 1
for (i in 1:(length(nClasse)-1)) {
for (j in (i+1):length(nClasse)) {
class_i <- nClasse[i]
class_j <- nClasse[j]
indices <- which(y == class_i | y == class_j)
y_pair <- y[indices]
X_pair <- X[, indices]
list_y[[k]] <- as.vector(y_pair)
list_X[[k]] <- X_pair
k <- k + 1
}
}
} else {
for (i in 1:length(nClasse)) {
class_i <- nClasse[i]
y_temp <- ifelse(y == class_i, as.character(class_i), "All")
list_y[[i]] <- as.vector(y_temp)
list_X[[i]] <- X
}
}
for (i in 1:(length(list_y))) {
ind[[i]]     <- order(filterfeaturesK(data = list_X[[i]],
trait = list_y[[i]],
k =nrow(X),
sort = FALSE)$p,
decreasing = FALSE) # to avoid having to recompute this all the time
}
length(ind)
coeffsl <- normModelCoeffs_mc(mod = mod, X = X, y = y, sort.features = sort.features, sort.ind = ind)
# and the sparsity
length(coeffsl)
coeffsl[[1]]
k_sparsity <- mod$eval.sparsity
k_sparsity
coeffs.name <- c()
coeffs.nr   <- c()
i = 1
coeffs.name  <- rep(mod$learner,length(coeffsl[[i]]))
coeffs.nr    <- c(1:length(coeffsl[[i]]))
coeffs.data         <- data.frame(coeffsl[[i]], coeffs.name, coeffs.nr)
colnames(coeffs.data) <- c("coeff","classifier","feature")
coeffs.data$coeff   <- as.numeric(coeffs.data$coeff)
coeffs.data$sign    <- sign(coeffs.data$coeff)
coeffs.data$sign[coeffs.data$sign == 0] <- NA
coeffs.data$col     <- col.sign[factor(coeffs.data$sign, levels = c(-1,1))]
# add information on importance
rownames(coeffs.data) <- rownames(X)[ind[[i]]]
coeffs.data$importance <- 0
coeffs.data$importance.col <- NA
if(!is.null(mod$mda.cv_[[i]]))
{
coeffs.data[mod$names_[[i]],]$importance <- mod$mda.cv_[[i]]
coeffs.data[mod$names_[[i]],]$importance.col <- "black"
}
# get the features
features <- rownames(coeffs.data)[which(!is.na(coeffs.data$sign))]
names(col.sign) <- c("-1","1")
col.sign <- as.character(col.sign[names(table(sign(coeffs.data$coeff[coeffs.data$coeff != 0])))])
# make the main plot
g1 <- ggplot(coeffs.data, aes(feature, coeff, fill=col)) +
geom_bar(stat="identity", position="dodge") + ylim(-1, 1) +
xlab("") +
theme(legend.position="none", axis.text=element_text(size=9)) +
scale_fill_manual("Sign", values = col.sign) +
geom_hline(yintercept = 0, col="gray") +
theme_bw() + guides(fill = "none") +
coord_flip() +
theme(
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
strip.background = element_rect(colour = "white", fill = "white", size = 0.3),
strip.text.x = element_text(colour = "darkred", size = 10))
g1
i = 2
coeffs.name  <- rep(mod$learner,length(coeffsl[[i]]))
coeffs.nr    <- c(1:length(coeffsl[[i]]))
coeffs.data         <- data.frame(coeffsl[[i]], coeffs.name, coeffs.nr)
colnames(coeffs.data) <- c("coeff","classifier","feature")
coeffs.data$coeff   <- as.numeric(coeffs.data$coeff)
coeffs.data$sign    <- sign(coeffs.data$coeff)
coeffs.data$sign[coeffs.data$sign == 0] <- NA
coeffs.data$col     <- col.sign[factor(coeffs.data$sign, levels = c(-1,1))]
# add information on importance
rownames(coeffs.data) <- rownames(X)[ind[[i]]]
coeffs.data$importance <- 0
coeffs.data$importance.col <- NA
if(!is.null(mod$mda.cv_[[i]]))
{
coeffs.data[mod$names_[[i]],]$importance <- mod$mda.cv_[[i]]
coeffs.data[mod$names_[[i]],]$importance.col <- "black"
}
# get the features
features <- rownames(coeffs.data)[which(!is.na(coeffs.data$sign))]
names(col.sign) <- c("-1","1")
col.sign <- as.character(col.sign[names(table(sign(coeffs.data$coeff[coeffs.data$coeff != 0])))])
# make the main plot
g1 <- ggplot(coeffs.data, aes(feature, coeff, fill=col)) +
geom_bar(stat="identity", position="dodge") + ylim(-1, 1) +
xlab("") +
theme(legend.position="none", axis.text=element_text(size=9)) +
scale_fill_manual("Sign", values = col.sign) +
geom_hline(yintercept = 0, col="gray") +
theme_bw() + guides(fill = "none") +
coord_flip() +
theme(
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
strip.background = element_rect(colour = "white", fill = "white", size = 0.3),
strip.text.x = element_text(colour = "darkred", size = 10))
g1
i = 3
coeffs.name  <- rep(mod$learner,length(coeffsl[[i]]))
coeffs.nr    <- c(1:length(coeffsl[[i]]))
coeffs.data         <- data.frame(coeffsl[[i]], coeffs.name, coeffs.nr)
colnames(coeffs.data) <- c("coeff","classifier","feature")
coeffs.data$coeff   <- as.numeric(coeffs.data$coeff)
coeffs.data$sign    <- sign(coeffs.data$coeff)
coeffs.data$sign[coeffs.data$sign == 0] <- NA
coeffs.data$col     <- col.sign[factor(coeffs.data$sign, levels = c(-1,1))]
# add information on importance
rownames(coeffs.data) <- rownames(X)[ind[[i]]]
coeffs.data$importance <- 0
coeffs.data$importance.col <- NA
if(!is.null(mod$mda.cv_[[i]]))
{
coeffs.data[mod$names_[[i]],]$importance <- mod$mda.cv_[[i]]
coeffs.data[mod$names_[[i]],]$importance.col <- "black"
}
# get the features
features <- rownames(coeffs.data)[which(!is.na(coeffs.data$sign))]
names(col.sign) <- c("-1","1")
i = 3
coeffs.name  <- rep(mod$learner,length(coeffsl[[i]]))
coeffs.nr    <- c(1:length(coeffsl[[i]]))
coeffs.data         <- data.frame(coeffsl[[i]], coeffs.name, coeffs.nr)
colnames(coeffs.data) <- c("coeff","classifier","feature")
coeffs.data$coeff   <- as.numeric(coeffs.data$coeff)
coeffs.data$sign    <- sign(coeffs.data$coeff)
coeffs.data$sign[coeffs.data$sign == 0] <- NA
coeffs.data$col     <- col.sign[factor(coeffs.data$sign, levels = c(-1,1))]
# add information on importance
rownames(coeffs.data) <- rownames(X)[ind[[i]]]
coeffs.data$importance <- 0
coeffs.data$importance.col <- NA
if(!is.null(mod$mda.cv_[[i]]))
{
coeffs.data[mod$names_[[i]],]$importance <- mod$mda.cv_[[i]]
coeffs.data[mod$names_[[i]],]$importance.col <- "black"
}
# get the features
features <- rownames(coeffs.data)[which(!is.na(coeffs.data$sign))]
names(col.sign) <- c("-1","1")
i = 1
coeffs.name  <- rep(mod$learner,length(coeffsl[[i]]))
coeffs.nr    <- c(1:length(coeffsl[[i]]))
coeffs.data         <- data.frame(coeffsl[[i]], coeffs.name, coeffs.nr)
colnames(coeffs.data) <- c("coeff","classifier","feature")
coeffs.data$coeff   <- as.numeric(coeffs.data$coeff)
coeffs.data$sign    <- sign(coeffs.data$coeff)
coeffs.data$sign[coeffs.data$sign == 0] <- NA
coeffs.data$col     <- col.sign[factor(coeffs.data$sign, levels = c(-1,1))]
# add information on importance
rownames(coeffs.data) <- rownames(X)[ind[[i]]]
coeffs.data$importance <- 0
coeffs.data$importance.col <- NA
if(!is.null(mod$mda.cv_[[i]]))
{
coeffs.data[mod$names_[[i]],]$importance <- mod$mda.cv_[[i]]
coeffs.data[mod$names_[[i]],]$importance.col <- "black"
}
# get the features
features <- rownames(coeffs.data)[which(!is.na(coeffs.data$sign))]
names(col.sign) <- c("-1","1")
coeffs.name  <- rep(mod$learner,length(coeffsl[[i]]))
coeffs.nr    <- c(1:length(coeffsl[[i]]))
coeffs.data         <- data.frame(coeffsl[[i]], coeffs.name, coeffs.nr)
colnames(coeffs.data) <- c("coeff","classifier","feature")
coeffs.data$coeff   <- as.numeric(coeffs.data$coeff)
coeffs.data$sign    <- sign(coeffs.data$coeff)
coeffs.data$sign[coeffs.data$sign == 0] <- NA
coeffs.data$col     <- col.sign[factor(coeffs.data$sign, levels = c(-1,1))]
# add information on importance
rownames(coeffs.data) <- rownames(X)[ind[[i]]]
coeffs.data$importance <- 0
coeffs.data$importance.col <- NA
if(!is.null(mod$mda.cv_[[i]]))
{
coeffs.data[mod$names_[[i]],]$importance <- mod$mda.cv_[[i]]
coeffs.data[mod$names_[[i]],]$importance.col <- "black"
}
features <- rownames(coeffs.data)[which(!is.na(coeffs.data$sign))]
names(col.sign) <- c("-1","1")
i
i = 2
names(col.sign) <- c("-1","1")
Indometh
ind
coeffsl
k_sparsity <- mod$eval.sparsity
k_sparsity
coeffs.name <- c()
coeffs.nr   <- c()
i = 3
coeffs.name  <- rep(mod$learner,length(coeffsl[[i]]))
coeffs.nr    <- c(1:length(coeffsl[[i]]))
coeffs.data         <- data.frame(coeffsl[[i]], coeffs.name, coeffs.nr)
colnames(coeffs.data) <- c("coeff","classifier","feature")
coeffs.data$coeff   <- as.numeric(coeffs.data$coeff)
coeffs.data$sign    <- sign(coeffs.data$coeff)
coeffs.data$sign[coeffs.data$sign == 0] <- NA
coeffs.data$col     <- col.sign[factor(coeffs.data$sign, levels = c(-1,1))]
# add information on importance
rownames(coeffs.data) <- rownames(X)[ind[[i]]]
coeffs.data$importance <- 0
coeffs.data$importance.col <- NA
coeffs.data[mod$names_[[i]],]$importance <- mod$mda.cv_[[i]]
coeffs.data[mod$names_[[i]],]$importance.col <- "black"
features <- rownames(coeffs.data)[which(!is.na(coeffs.data$sign))]
features
names(col.sign) <- c("-1","1")
names(col.sign) <- c("-1","1")
col.sign
col.sign = c("deepskyblue1", "firebrick1"
"
)
}
)
,
###
knitr::opts_chunk$set(echo = TRUE)
res_clf$classifier$models
knitr::opts_chunk$set(echo = TRUE)
load("Unique_votingAggregation.rda")
load("Unique_Predomics_aggregation_ova.rda")
load("Unique_maximizationAggregation.rda")
load("Unique_rankingAggregation.rda")
load("Unique_weightedAggregation.rda")
load("Multi_Predomics_aggregation_ova.rda")
load("Multi_maximizationAggregation.rda")
load("Multi_rankingAggregation.rda")
load("Multi_weightedAggregation.rda")
load("Multi_votingAggregation.rda")
res_clf$classifier$models
res_clf = Unique_Predomics_aggregation_ova
res_clf$classifier$models
pop <- modelCollectionToPopulation(res_clf$classifier$models)
length(pop)
pop
printy(pop)
printy_mc(pop)
pop.df <- populationToDataFrame(pop)
attributes = c("learner","language","fit_", "unpenalized_fit_",
"auc_", "accuracy_", "cor_", "aic_", "intercept_",
"eval.sparsity", "sign_","precision_", "recall_","f1_")
if(!isPopulation(pop))
{
stop("populationToDataFrame: Please provide a valid population object")
}
mod <- pop[[1]]
ind.match <- match(attributes, names(mod))
if(any(is.na(ind.match)))
{
stop(paste("populationToDataFrame: unknown attributes",attributes[is.na(ind.match)]))
}
df <- paste("mod",1:length(pop),sep="_")
df
ind.null <- c()
for(i in 1:length(attributes))
{
x <- populationGet_X(element2get = attributes[i], toVec = TRUE, na.rm = FALSE)(pop)
#df <- data.frame(df,x)
if(is.null(x))
{
ind.null <- c(ind.null,i)
}else
{
if(attributes[i]=="eval.sparsity")
{
x <- factor(as.character(x),levels = as.character(unique(x)))
}
df <- data.frame(df,x)
}
}
x
df <- df[,-1] # get out the first one that was used to start the df
df
if(length(ind.null)>1)
{
colnames(df) <- attributes[-ind.null]
}else
{
colnames(df) <- attributes
}
rownames(df) <- paste("mod",1:length(pop),sep="_")
df$accuracy_
df$intercept_
df$sign_
length(df$sign_)
length(df$sign_[1])
length(df$sign_[[1]])
x
length(x)
mod$Multi_confusionMatrix_
mod$unpenalized_fit_
mod$accuracy_
mod$confusionMatrix_
