} else # if smaller percentage
{
#if(clf$params$final.pop.perc>100)
nBest           <- round(return.perc * clf$params$nbVeryBest / 100)
res.mod.coll    <- listOfModels2ModelCollection(pop = fullPop, nBest = nBest)
}
if(clf$params$verbose) print(paste("... ... models are coverted onto a model collection"))
return(res.mod.coll)
}
clf <- terBeam_mc(sparsity = c(5),
max.nb.features = 1000,
seed = 1,
nCores = 1,
evalToFit = "accuracy_",
objective = "auc",
experiment.id = "terBeam_mc",
experiment.save = "nothing")
printy(clf)
runit = TRUE
if(runit)
{
res_clfi <- fit_mc(X = X, y = y, clf = clf,approch="ova", cross.validate = TRUE,aggregation_ = "Predomics_aggregation_ova", nfolds= 1);
#save(res_clf , clf, file ="res_clf.rda", compression_level = 9)
}
# ... Database X is not a matrix! Converting ...
# ... Classification mode, computing factor(y) for speedup and robustness
# ... Loading feature correlation for speedup
# ... Correlation file loaded
# ... Storing data in the classifier object for speedup
# ... Computing ternary coefficients for speedup
# ... One seed found, setting by default
# ... Running the classifier terga2 with a single CPU
# ... Second and faster version of terga fitting based on Genetic Algorithm heuristics ...
# ... Cross validation mode
# ... Starting cross validation not in parallel
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ... Learning process is finished succesfuly
# ... Thank you for using Predomics. Don't forget to digest the results now.
#
# [1] "experiment" "predomics"
res_clf.dig <- digestmc(obj = res_clfi, penalty = 0.75/100, plot = TRUE)
best.model <- res_clf.dig$best$model
printy_mc(best.model)
clf <- regenerate_clf(clf, X, y, approch = "ova")
best.model.test <- evaluateModel_mc(
mod = best.model,
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.test)
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
library(ggpubr)
library(dplyr)
library(tidyr)
library(tibble)
library(knitr)
library(kableExtra)
library(DT)
library(e1071)
library(glmnet)
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Create an index vector for data partitioning
X_general <- X_general[rowSums(X_general)!=0,]; dim(X_general) # filter out variables with only zero values
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE); dim(X_general)
set.seed(42)
y = as.vector(yvec_trie)
X = X_general
# Number of desired samples in each class
nombre_echantillons_par_classe <- min(table(y))
# Function to balance the classes
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe,seed =123) {
classes <- unique(y)
indices_equilibres <- integer(0)
for (classe in classes) {
indices_classe <- which(y == classe)
set.seed(seed)
indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
}
return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X
# Verify the distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)
# Split yvec_trie into 80% train and 20% test
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[,indices_division]
X.test <- X_equilibre[,-indices_division]
table(y)
table(y.test)
dim(X)
dim(X.test)
clf <- terBeam_mc(sparsity = c(2,3,4,5,6,7,8,9,10),
max.nb.features = 1000,
seed = 1,
nCores = 1,
evalToFit = "accuracy_",
objective = "auc",
experiment.id = "terBeam_mc",
experiment.save = "nothing")
printy(clf)
runit = TRUE
if(runit)
{
res_clfi <- fit_mc(X = X, y = y, clf = clf,approch="ova", cross.validate = TRUE,aggregation_ = "Predomics_aggregation_ova", nfolds= 1);
#save(res_clf , clf, file ="res_clf.rda", compression_level = 9)
}
# ... Database X is not a matrix! Converting ...
# ... Classification mode, computing factor(y) for speedup and robustness
# ... Loading feature correlation for speedup
# ... Correlation file loaded
# ... Storing data in the classifier object for speedup
# ... Computing ternary coefficients for speedup
# ... One seed found, setting by default
# ... Running the classifier terga2 with a single CPU
# ... Second and faster version of terga fitting based on Genetic Algorithm heuristics ...
# ... Cross validation mode
# ... Starting cross validation not in parallel
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ... Learning process is finished succesfuly
# ... Thank you for using Predomics. Don't forget to digest the results now.
#
# [1] "experiment" "predomics"
res_clf.dig <- digestmc(obj = res_clfi, penalty = 0.75/100, plot = TRUE)
# get the best model
best.model <- res_clf.dig$best$model
printy_mc(best.model)
plots1 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ova")
plots2 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE, approch = "ova")
# Extract the plots from each list and pass them to `grid.arrange
grid.arrange(grobs = c(plots1, plots2), ncol = 4)
clf <- regenerate_clf(clf, X, y, approch = "ova")
best.model.test <- evaluateModel_mc(
mod = best.model,
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.test)
tmp <- plotAUC_mc(best.model$score_, y, percent = TRUE, approch = "ova"); rm(tmp)
# Convert the model collection into a population of models scrambled by model size
pop <- modelCollectionToPopulation(res_clfi$classifier$models)
printy_mc(pop)
# Convert the population to a data frame
pop.df <- populationToDataFrame_mc(pop)
# Plotting for the original population
g.before <- list()  # Initialize a list to store the plots
for(i in 1: length(pop.df)) {
pop.dff  <- as.data.frame(pop.df[[i]])  # Convert each submodel to a data frame
# Display the head of the dataframe excluding some columns
head(pop.dff[,-c(3, 4, 7, 8, 14)])
# Melt the dataframe for ggplot
pop.df.melt <- melt(pop.dff, id.vars = c("accuracy_", "eval.sparsity"))
# Create the ggplot for the original population
g.before[[i]] <- ggplot(data = pop.df.melt, aes(y = accuracy_, x = eval.sparsity)) +
geom_boxplot(notch = FALSE, outlier.shape = NA, position = position_dodge(width=0.9), alpha = 0.3) +
geom_point(aes(color = eval.sparsity), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
ylim(c(0,1)) +
xlab("Model parsimony") +
ggtitle("Original population") +
theme_bw() +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour="none")
}
# Select the best population models
fbm <- selectBestPopulation(pop)
printy_mc(fbm)
# Convert the best population models to a data frame
fbm.df <- populationToDataFrame_mc(fbm)
# Plotting for the selected best models
g.after <- list()  # Initialize a list to store the plots
for(j in 1:length(fbm.df)) {
fbm.dff  <- as.data.frame(fbm.df[[j]])  # Convert each submodel to a data frame
# Melt the dataframe for ggplot
fbm.df.melt <- melt(fbm.dff, id.vars = c("accuracy_", "eval.sparsity"))
# Create the ggplot for the best population models
g.after[[j]] <- ggplot(data = fbm.df.melt, aes(y = accuracy_, x = eval.sparsity)) +
geom_boxplot(notch = FALSE, position = position_dodge(width=0.9), alpha = 0.3) +
geom_point(aes(color = eval.sparsity), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
ylim(c(0,1)) +
xlab("Model parsimony") +
ggtitle("FBM") +
theme_bw() +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour="none")
}
# Arrange the plots side by side
grid.arrange(g.before[[1]], g.after[[1]], ncol = 2)
best.model.testt <- evaluateModel_mc(
mod = fbm[[1]],
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testt)
best.model.testt <- evaluateModel_mc(
mod = fbm[[2]],
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testt)
best.model.testt <- evaluateModel_mc(
mod = fbm[[3]],
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testt)
best.model.testt <- evaluateModel_mc(
mod = fbm[[4]],
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testt)
best.model.testt <- evaluateModel_mc(
mod = fbm[[5]],
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testt)
best.model.testt <- evaluateModel_mc(
mod = fbm[[6]],
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testt)
best.model.testt <- evaluateModel_mc(
mod = fbm[[7]],
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testt)
best.model.testt <- evaluateModel_mc(
mod = fbm[[8]],
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testt)
best.model.testt <- evaluateModel_mc(
mod = fbm[[9]],
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testt)
best.model.testt <- evaluateModel_mc(
mod = fbm[[10]],
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testt)
best.model.testt <- evaluateModel_mc(
mod = fbm[[11]],
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testt)
best.model.testt <- evaluateModel_mc(
mod = fbm[[12]],
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testt)
best.model.testt <- evaluateModel_mc(
mod = fbm[[13]],
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testt)
best.model.testt <- evaluateModel_mc(
mod = fbm[[14]],
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testt)
best.model.testt <- evaluateModel_mc(
mod = fbm[[100]],
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testt)
best.model.testt <- evaluateModel_mc(
mod = fbm[[200]],
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testt)
best.model.testt <- evaluateModel_mc(
mod = fbm[[50]],
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testt)
best.model.testt <- evaluateModel_mc(
mod = fbm[[20]],
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testt)
fbm[[20]]$accuracy_
fbm[[20]]$Multi_confusionMatrix_
best.model.testt$Multi_confusionMatrix_
best.model$accuracy_
best.model
best.model.test <- evaluateModel_mc(
mod = best.model,
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.test)
res_clf.dig <- digestmc(obj = res_clfi, penalty = 0.75/100, plot = TRUE)
res_clf.dig$best$models[[4]]
res_clf.dig$best$models[[4]]$accuracy_
res_clf.dig$best$models[[6]]$accuracy_
best.model = res_clf.dig$best$models[[6]]
best.model$accuracy_
best.model.testtt <- evaluateModel_mc(
mod = best.model,
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testtt)
res_clf.dig$best$models[[4]]$accuracy_
boo = res_clf.dig$best$models[[4]]
boo = res_clf.dig$best$models[[9]]$accuracy_
boo = res_clf.dig$best$models[[4]]
res_clf.dig$best$models[[9]]$accuracy_
res_clf.dig$best$models[[8]]$accuracy_
res_clf.dig$best$models[[7]]$accuracy_
res_clf.dig$best$models[[6]]$accuracy_
res_clf.dig$best$models[[5]]$accuracy_
res_clf.dig$best$models[[4]]$accuracy_
res_clf.dig$best$models[[3]]$accuracy_
res_clf.dig$best$models[[2]]$accuracy_
res_clf.dig$best$models[[1]]$accuracy_
best.model.testtv <- evaluateModel_mc(
mod = boo,
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.testtv)
