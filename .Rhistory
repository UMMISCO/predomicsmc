# Define colors based on the values (-1, 0, 1)
col.n <- c("-1", "0", "1")
tab.v <- table(data.m$value)
if (length(tab.v) < 3) {
col <- col[col.n %in% names(tab.v)]  # Adjust colors based on the available values
}
# Create the plot for this combination
p <- ggplot(data.m, aes(models, feature)) +
geom_tile(aes(fill = value), colour = "darkgray") +
theme_bw() +
scale_fill_gradientn(colours = col) +
ggtitle(paste(combination))  # Add the combination as the title
# Adjust vertical or horizontal labels
if (vertical.label) {
p <- p + theme(legend.position = "none", axis.text = element_text(size = 9), axis.text.x = element_text(angle = 90, hjust = 1))
} else {
p <- p + theme(legend.position = "none", axis.text = element_text(size = 9))
}
# Define the legend text for the combination
legend_text <- if (approch == "ova") {
paste0(classes[i], ": Red | ALL: Blue")
} else {
class_pair <- strsplit(combination, "_vs_")[[1]]
paste0(class_pair[1], ": Red | ", class_pair[2], ": Blue")
}
# Add the legend as a caption at the bottom of the plot
p <- p + theme(
plot.margin = unit(c(1, 1, 2, 1), "cm"),  # Extra margin at the bottom for the legend
plot.caption = element_text(hjust = 0.5)  # Center the caption
) +
labs(caption = legend_text)  # Use caption to add the legend text
# Add this plot to the list
plot_list[[combination]] <- p
}
# Use the number of combinations to set ncol for horizontal layout
#ncol <- ceiling(sqrt(length(plot_list)))  # Use square root of the number of plots to determine columns
ncol = length(combinations)
# Create the arranged grid of plots
arranged_plot <- gridExtra::grid.arrange(grobs = plot_list, ncol = ncol)
# Return the arranged plot
return(arranged_plot)
}
#' Prints as text the detail on a given experiment along with summarized results (if computed)
#'
#' @description This function takes a population of models and creates a table with annotation on the features,
#' such as prevalence in the models and dataset as well as different statistics
#' @param pop: a population of models
#' @param X: the X dataset where to compute the abundance and prevalence
#' @param y: the target class
#' @param approch: approach ovo or ova
#' @param clf: an object containing the different parameters of the classifier
#' @return a list with two data.frames one containing the coefficients per each model and the other a data.frame on the features
#' @export
# Function to generate feature annotations for multi-class classification
makeFeatureAnnot_mc <- function(pop, X, y, clf, approch = "ovo") {
# Extract the unique classes present in the labels
nClasse <- unique(y)
# Initialize lists to store various data components
list_y <- list()       # Stores subsets of labels
list_X <- list()       # Stores subsets of features
listfa <- list()       # Stores feature annotations for each subset
listcoeffs <- list()   # Stores classifier coefficients for each subset
listX <- list()        # Stores the input feature matrix for each subset
listXmin <- list()     # Stores minimum values of features for normalization
listXmax <- list()     # Stores maximum values of features for normalization
listy <- list()        # Stores labels for training
# Populate initial values for classifier data
listcoeffs <- clf$coeffs_
listX <- clf$data$X
listXmin <- clf$data$X.min
listXmax <- clf$data$X.max
listy <- clf$data$y
# Initialize the list to store the populations
populations <- list()
# Construct populations for each classifier subset
for (i in 1:length(pop[[1]]$indices_)) {
sub_population <- list()
# Iterate over each population and assign subset-specific values
for (j in 1:length(pop)) {
popp <- pop[[j]]  # Get the j-th population
# Update population-specific indices, names, and parameters
popp$indices_ <- pop[[j]]$indices_[[i]]
popp$names_ <- pop[[j]]$names_[[i]]
popp$coeffs_ <- pop[[j]]$coeffs_[[i]]
popp$intercept_ <- pop[[j]]$list_intercept_mc[[i]]
popp$sign_ <- pop[[j]]$sign_[[i]]
popp$score_ <- pop[[j]]$score_[[i]]
popp$confusionMatrix_ <- pop[[j]]$confusionMatrix_[[i]]
sub_population[[j]] <- popp
}
populations[[i]] <- sub_population
}
combi <- generate_combinations_with_factors(y, X, approch = approch)
list_y <- combi$list_y
list_X <- combi$list_X
# Generate feature annotations for each subset of the data
for (i in 1:length(list_X)) {
# Update classifier parameters for the current subset
clf$coeffs_ <- listcoeffs[[i]]
clf$data$X <- listX[[i]]
clf$data$X.min <- listXmin[[i]]
clf$data$X.max <- listXmax[[i]]
clf$data$y <- listy[[i]]
# Generate feature annotations for the current subset
listfa[[i]] <- makeFeatureAnnot(pop = populations[[i]], X = list_X[[i]], y = list_y[[i]], clf = clf)
}
# Return the list of feature annotations for all subsets
return(listfa)
}
################################################################
# Multi-class Model Plots for Predomics
################################################################
#' @title Plots a model or a population of model objectsas barplots of scaled coefficients.
#'
#' @description Plots a model or a population of models as a barplots, representing each feature, the length being the coefficient
#' @import ggplot2
#' @param mod: a model to plot
#' @param X: the data matrix with variables in the rows and observations in the columns
#' @param y: the class vector
#' @param sort.features: wether the features need to be sorted by correlation with 'y' or not (default: TRUE)
#' @param sort.ind: computing sorting can take time if computed for every model and can be computed outside the function and passed as a parameter
#' @param feature.name: show the name of the features (default:FALSE)
#' @param col.sign: the colors of the cofficients based on the sign of the coefficients (default: -1=deepskyblue1, 1:firebrick1)
#' @param main: possibility to change the title of the function (default:"")
#' @param slim: plot without axis information (default:FALSE)
#' @param importance: the importance (mda) of the features in crossval
#' @param res_clf: the result of the learning process (default:NULL). If provided information on MDA will be extracted for the importance graphic.
#' @export
printModel_mc <- function(mod, method = "short", score = "fit_")
{
if(!isModel(obj = mod))
{
print("printModel: please provide a valid predomics model object.")
return(NULL)
}
if(!score %in% names(mod))
{
print("printModel: please provide a valid score that is found as an attribute in the model object.")
return(NULL)
}
list_res <- list()
list_matrices <- list()
combinaison <- list()
listindices <- mod$indices_
listnames <- mod$names_
listcoeffs <- mod$coeffs_
listsign <- mod$sign_
listscore <- mod$score_
listpos <- mod$pos_score_
listneg <- mod$neg_score_
list_intercept <- mod$list_intercept_
list_matrices <- mod$confusionMatrix_
combinaison = mod$predictions
list_accura <- list()
accura <- mod$accuracy
for(km in 1:length(listindices)){
combn_class <-  unique(combinaison[[km]])
mod$indices_ <- listindices[[km]]
mod$names_ <- listnames[[km]]
mod$coeffs_ <- listcoeffs[[km]]
mod$sign_ <-  listsign[[km]]
mod$score_ <- listscore[[km]]
mod$pos_score_ <- listpos[[km]]
mod$neg_score_ <- listneg[[km]]
mod$intercept_ <- list_intercept[[km]]
mod$fit_ <- accura
switch(method,
short={
if(!isModelSota(mod))
{
if(all(sapply(mod$coeffs_, myAssertNotNullNorNa)))
{
ind.pos <- sapply(mod$coeffs_, function(coeffs) sign(coeffs) == 1)
ind.neg <- sapply(mod$coeffs_, function(coeffs) sign(coeffs) == -1)
if(mod$language == "ratio")
{
signs <- rep("+", length(mod$coeffs_))
term.pos <- ifelse(all(!ind.pos), "0", paste("(", paste(signs[ind.pos], mod$indices_[ind.pos], sep = "", collapse = ""), ")"))
term.neg <- ifelse(all(!ind.neg), "0", paste("(", paste(signs[ind.neg], mod$indices_[ind.neg], sep = "", collapse = ""), ")"))
res <- paste(term.pos, "/", term.neg)
mod.inter <- paste(mod$sign_, signif(mod$intercept_, 2))
mod.fit <- mod[[score]]
if(!is.na(mod.fit)) res <- paste("|", res, " ", mod.inter, decision, "| (F=", signif(mod.fit, 4), sep = "")
res <- paste(res, "|K=", mod$eval.sparsity, "|Le=", mod$learner, "|La=", mod$language, ")", sep = "")
}else{
signs <- rep("+", length(mod$coeffs_))
term.pos <- ifelse(all(!ind.pos), "0", paste("(", paste(signs[ind.pos], mod$indices_[ind.pos], sep = "", collapse = ""), ")"))
term.neg <- ifelse(all(!ind.neg), "0", paste("(", paste(signs[ind.neg], mod$indices_[ind.neg], sep = "", collapse = ""), ")"))
res <- paste(term.pos, " - ", term.neg)
mod.inter <- paste(mod$sign_, signif(mod$intercept_, 2))
mod.fit <- mod[[score]]
if(!is.na(mod.fit)) res <- paste("|", res, " ", mod.inter, "| (F=", signif(mod.fit, 4), sep = "")
res <- paste(res, "|K=", mod$eval.sparsity, "|Le=", mod$learner, "|La=", mod$language, ")", sep = "")
}
}
}else{
if(!is.null(mod$coeffs_))
{
coeffs <- signif(unlist(mod$coeffs_), 2)
}else{
coeffs <- ""
}
res <- unlist(mod$indices_)
res <- paste(coeffs, res, sep = " * ")
res <- paste(res, collapse = " ")
res <- paste(res, mod$learner, sep = "|")
mod.fit <- mod[[score]]
if(!is.na(mod.fit)) res <- paste("|", res, " ", "| (F=", signif(mod.fit, 4), sep = "")
res <- paste(res, "|K=", mod$eval.sparsity, "|Le=", mod$learner, "|La=", mod$language, ")", sep = "")
}
},
long={
if(!isModelSota(mod))
{
if(all(sapply(mod$coeffs_, myAssertNotNullNorNa)))
{
ind.pos <- sapply(mod$coeffs_, function(coeffs) sign(coeffs) == 1)
ind.neg <- sapply(mod$coeffs_, function(coeffs) sign(coeffs) == -1)
if(mod$language == "ratio")
{
signs <- rep("+ ", length(mod$coeffs_))
term.pos <- ifelse(all(!ind.pos), "0", paste("(", paste(signs[ind.pos], mod$names_[ind.pos], sep = "", collapse = ""), ")"))
term.neg <- ifelse(all(!ind.neg), "0", paste("(", paste(signs[ind.neg], mod$names_[ind.neg], sep = "", collapse = ""), ")"))
res <- paste(term.pos, "/", term.neg)
mod.inter <- paste(mod$sign_, signif(mod$intercept_, 2))
mod.fit <- mod[[score]]
if(!is.na(mod.fit)) res <- paste("|", res, " ", mod.inter, "| (F=", signif(mod.fit, 4), sep = "")
res <- paste(res, "|K=", mod$eval.sparsity, "|Le=", mod$learner, "|La=", mod$language, ")", sep = "")
}else{
signs <- rep("+ ", length(mod$coeffs_))
term.pos <- ifelse(all(!ind.pos), "0", paste("(", paste(signs[ind.pos], mod$names_[ind.pos], sep = "", collapse = ""), ")"))
term.neg <- ifelse(all(!ind.neg), "0", paste("(", paste(signs[ind.neg], mod$names_[ind.neg], sep = "", collapse = ""), ")"))
res <- paste(term.pos, " - ", term.neg)
mod.inter <- paste(mod$sign_, signif(mod$intercept_, 2))
mod.fit <- mod[[score]]
if(!is.na(mod.fit)) res <- paste("|", res, " ", mod.inter, "| (F=", signif(mod.fit, 4), sep = "")
res <- paste(res, "|K=", mod$eval.sparsity, "|L=", mod$learner, "|La=", mod$language, ")", sep = "")
}
}
}else{
if(!is.null(mod$coeffs_))
{
coeffs <- signif(unlist(mod$coeffs_), 2)
}else{
coeffs <- ""
}
res <- unlist(mod$names_)
res <- paste(coeffs, res, sep = " * ")
res <- paste(res, collapse = " ")
res <- paste(res, mod$learner, sep = "|")
mod.fit <- mod[[score]]
res <- paste("|", res, " ", "| (F=", signif(mod.fit, 4), sep = "")
res <- paste(res, "|K=", mod$eval.sparsity, "|L=", mod$learner, "|La=", mod$language, ")", sep = "")
}
},
str={
res <- str(mod)
},
{
warning('This method does not exist! Try one of these: short, long or str')
}
)
list_res[[km]] <- res
}
res <- list()
res <- list_res
return(res)
}
################################################################
# Multi-class Model Plots for Predomics
################################################################
#' @title Plots a model or a population of model objectsas barplots of scaled coefficients.
#'
#' @description Plots a model or a population of models as a barplots, representing each feature, the length being the coefficient
#' @import ggplot2
#' @param mod: a model to plot
#' @param X: the data matrix with variables in the rows and observations in the columns
#' @param y: the class vector
#' @param sort.features: wether the features need to be sorted by correlation with 'y' or not (default: TRUE)
#' @param sort.ind: computing sorting can take time if computed for every model and can be computed outside the function and passed as a parameter
#' @param feature.name: show the name of the features (default:FALSE)
#' @param col.sign: the colors of the cofficients based on the sign of the coefficients (default: -1=deepskyblue1, 1:firebrick1)
#' @param main: possibility to change the title of the function (default:"")
#' @param slim: plot without axis information (default:FALSE)
#' @param importance: the importance (mda) of the features in crossval
#' @param res_clf: the result of the learning process (default:NULL). If provided information on MDA will be extracted for the importance graphic.
#' @export
plotModel_mc <- function(mod, X, y,
sort.features = FALSE,
sort.ind = NULL,
feature.name = FALSE,
col.sign = c("deepskyblue1", "firebrick1"),
main = "",
slim = FALSE,
importance = FALSE,
res_clf = NULL, approch = "ova") {
list_mod <- list()
plot_sub_model <- list()
# Retrieve the mod elements to create the main title
alg = mod$learner
lang = mod$language
k = mod$eval.sparsity
# Loop to fill the list of sub-models
for (i in 1:length(mod$names_)) {
list_mod[[i]] <- list(
learner = mod$learner,
language = mod$language,
objective = mod$objective,
indices_ = mod$indices_[[i]],
names_ = mod$names_[[i]],
coeffs_ = mod$coeffs_[[i]],
fit_ = mod$fit_,
unpenalized_fit_ = mod$unpenalized_fit_,
auc_ = mod$auc_,
accuracy_ = mod$accuracy_,
cor_ = mod$cor_,
aic_ = mod$aic_,
intercept_ = mod$list_intercept_[[i]],
eval.sparsity = mod$eval.sparsity,
precision_ = mod$precision_,
recall_ = mod$recall_,
f1_ = mod$f1_,
sign_ = mod$sign_[[i]],
rsq_ = mod$rsq_[[i]],
ser_ = mod$ser_[[i]],
score_ = mod$score_[[i]],
mda.cv_ = mod$mda.cv_[[i]],
prev.cv_ = mod$prev.cv_[[i]],
mda_ = mod$mda_[[i]]
)
}
listComb <- generate_combinations_with_factors(y = y, X = X, approch = approch)
l_y = lapply(l_y, unique)
nbreClasse <- lapply(l_y, sort)
nClasse <- unique(y)
list_y <- list()
list_X <- list()
if (approch == "ovo") {
f <- 1
for (i in 1:(length(nClasse) - 1)) {
for (j in (i + 1):length(nClasse)) {
class_i <- nClasse[i]
class_j <- nClasse[j]
indices <- which(y == class_i | y == class_j)
y_pair <- y[indices]
X_pair <- X[, indices]
list_y[[f]] <- as.vector(y_pair)
list_X[[f]] <- X_pair
f <- f + 1
}
}
} else {
for (i in 1:length(nClasse)) {
class_i <- nClasse[i]
y_temp <- ifelse(y == class_i, as.character(class_i), "All")
list_y[[i]] <- as.vector(y_temp)
list_X[[i]] <- X
}
}
# Loop to generate sub-models with custom titles
for (i in 1:length(list_y)) {
# Determine the main title with alg, lang, and k
combination_title <- paste("alg =", alg, "\nlang =", lang, "\nk =", k)
# Addition of a specific title for each sub-model
if (approch == "ovo") {
combination_title <- paste0(
combination_title,
"\n",
nbreClasse[[i]][1],
" vs ",
nbreClasse[[i]][2]
)
} else {
combination_title <- paste(combination_title, "\n", paste(nClasse[i], "vs All"))
}
plot_sub_model[[i]] <- plotModel(mod = list_mod[[i]], X = list_X[[i]], y = list_y[[i]],
sort.features = sort.features,
feature.name = feature.name,
importance = importance,
main = combination_title)
}
return(plot_sub_model)
}
# get the best model
best.model = fbm[[1]]
# Visualize the model information (if needed)
printy_mc(best.model)
# Generate the plots
plots1 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ovo")
knitr::opts_chunk$set(echo = TRUE)
# Package multi class predomics
library(mcpredomics)
# Package predomics
library(predomics)
# Visualization library for creating complex plots
library(ggplot2)
# Arranges multiple ggplot objects on a single page
library(gridExtra)
# ROC curve analysis and AUC calculation
library(pROC)
# Reshaping and melting data frames
library(reshape2)
# Implementation of the Random Forest algorithm for classification and regression
library(randomForest)
# Comprehensive library for classification and regression training
library(caret)
# Various R programming tools and functions, including data manipulation
library(gtools)
# Adding statistical comparisons and publication-ready visualizations
library(ggpubr)
# Data manipulation and transformation (part of the tidyverse)
library(dplyr)
# Tidying messy data by gathering and spreading
library(tidyr)
# Enhanced data frames with row names as a column (tibble format)
library(tibble)
# Dynamic report generation and displaying results in tables
library(knitr)
# Creating aesthetically pleasing and customizable HTML tables
library(kableExtra)
# Interactive tables for data visualization and exploration
library(DT)
# Functions for statistical learning, including SVM and Naive Bayes
library(e1071)
# Lasso and ridge regression via generalized linear models
library(glmnet)
# Reading data from files (including CSV and text files)
library(readr)
# String manipulation and regular expression functions
library(stringr)
# Package multi class predomics
library(mcpredomics)
# Package predomics
library(predomics)
# Visualization library for creating complex plots
library(ggplot2)
# Arranges multiple ggplot objects on a single page
library(gridExtra)
# ROC curve analysis and AUC calculation
library(pROC)
# Reshaping and melting data frames
library(reshape2)
# Implementation of the Random Forest algorithm for classification and regression
library(randomForest)
# Comprehensive library for classification and regression training
library(caret)
# Various R programming tools and functions, including data manipulation
library(gtools)
# Adding statistical comparisons and publication-ready visualizations
library(ggpubr)
# Data manipulation and transformation (part of the tidyverse)
library(dplyr)
# Tidying messy data by gathering and spreading
library(tidyr)
# Enhanced data frames with row names as a column (tibble format)
library(tibble)
# Dynamic report generation and displaying results in tables
library(knitr)
# Creating aesthetically pleasing and customizable HTML tables
library(kableExtra)
# Interactive tables for data visualization and exploration
library(DT)
# Functions for statistical learning, including SVM and Naive Bayes
library(e1071)
# Lasso and ridge regression via generalized linear models
library(glmnet)
# Reading data from files (including CSV and text files)
library(readr)
# String manipulation and regular expression functions
library(stringr)
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
# Filter null values
X_general <- mc.input$X[, colSums(mc.input$X) != 0]
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE)
set.seed(42)
y = as.vector(yvec)
X = X_general
# Determine the number of samples per class
nombre_echantillons_par_classe <- min(table(y))
# Function to balance classes and maintain order
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe, seed = 123) {
classes <- unique(y)
indices_equilibres <- integer(0)
for (classe in classes) {
indices_classe <- which(y == classe)
set.seed(seed)
indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
}
# Sort balanced indices to maintain original order
indices_equilibres <- sort(indices_equilibres)
return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}
# Get balanced data
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X
# Check distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)
# Split balanced data into 80% for training and 20% for testing
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[, indices_division, drop = FALSE]
X.test <- X_equilibre[, -indices_division, drop = FALSE]
table(y)
dim(X)
write.csv(X, file = "X.csv", row.names = FALSE)
write.csv(data.frame(y = y), file = "y.csv", row.names = FALSE)
write.csv(data.frame(y = y), file = "y.csv", row.names = FALSE)
