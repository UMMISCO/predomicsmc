aggregate_predictions <- function(classes_list, score_list) {
# Initialization of the aggregation vector
aggregated_predictions <- character(length = length(classes_list[[1]]))
names_class = unique(y)
names_class <- as.character(class_names)
# Loop for each position
for (i in seq_along(aggregated_predictions)) {
# Retrieve classes and scores for this position
current_classes <- sapply(classes_list, function(class_vector) class_vector[i])
scores <- sapply(score_list, function(score_vector) score_vector[i])
# Check if all positions are 'ALL'
if (all(current_classes == "ALL")) {
# Predict the class with the highest score
predicted_class <- names_class[which.max(scores)]
} else {
# Check if 'ALL' is present in the classes
if ("ALL" %in% current_classes) {
# Filter the classes that are not 'ALL'
non_all_classes <- current_classes[current_classes != "ALL"]
# If non-'ALL' classes are present, choose the one with the highest score.
if (length(non_all_classes) > 0) {
max_score_index <- which.max(scores[current_classes %in% non_all_classes])
predicted_class <- non_all_classes[max_score_index]
} else {
# If all classes are 'ALL,' choose either 'ALL' or another class at random.
predicted_class <- "ALL"
}
} else {
# If 'ALL' is not present, choose the class with the highest score.
max_score_index <- which.max(scores)
predicted_class <- current_classes[max_score_index]
}
}
# Fill the aggregation vector at position i.
aggregated_predictions[i] <- predicted_class
}
return(aggregated_predictions)
}
aggregate_predictions <- aggregate_predictions(classes_list = predict_ova, score_list = normalize_scores)
aggregate_predictions
EvaluateAdditionnelGlobaleMetrics <- EvaluateAdditionnelGlobaleMetrics(predictions = aggregate_predictions, actual_labels = y)
EvaluateAdditionnelGlobaleMetrics
#' Function to predict the class for each "Class vs ALL" combination.
#' @title predict_ova
#' @description Function to predict the class for each "Class vs ALL" combination.
#' @param y: the response vector
#' @param mod: object model
#' @return list of class predict
#' @export
predict_ova <- function(mod, y) {
intercept_list <- mod$intercept_
score_list <- mod$score_
class_names <- unique(y)
class_names <- as.character(class_names)
# List of prediction vectors for each combination.
predictions_list <- lapply(seq_along(intercept_list), function(j) {
class_name <- class_names[j]
sapply(score_list[[j]], function(score) {
ifelse(score > intercept_list[[j]], class_name, "ALL")
})
})
return(predictions_list)
}
#' Function to predict the class for each "Class 1 vs Class 2" combination.
#' @title predict_ovo
#' @description Function to predict the class for each "Class1 vs Class2" combination.
#' @param y: the response vector
#' @param mod: object model
#' @return list of class predict
#' @export
predict_ovo <- function(mod, y) {
list_y <- list()
nClasse <- unique(y)
# Dataset decomposition phase using the one-versus-one and one-versus-all approaches
k <- 1
for (i in 1:(length(nClasse)-1)) {
for (j in (i+1):length(nClasse)) {
class_i <- nClasse[i]
class_j <- nClasse[j]
indices <- which(y == class_i | y == class_j)
y_pair <- y[indices]
list_y[[k]] <- as.vector(y_pair)
k <- k + 1
}
}
num_combinations <- length(list_y)
predictions_list <- list()
for (comb_index in 1:num_combinations) {
intercept <- mod$intercept_[[comb_index]]
scores <- mod$score[[comb_index]]
num_samples <- length(scores)
# Get the unique values of the current combination.
class_values <- list_y[[comb_index]]
unique_classes <- unique(class_values)
# Initialize the prediction vector for this combination
predictions <- character(length = num_samples)
for (sample_index in 1:num_samples) {
# Prediction for the current sample
decision_value <- scores[sample_index]
predictions[sample_index] <- ifelse(decision_value > intercept,unique_classes[2],unique_classes[1])
}
# Add the prediction vector to the list.
predictions_list[[comb_index]] <- predictions
}
return(predictions_list)
}
#' This function calculates the maximum score combinations.
#' @title CalculateDistanceScores_
#' @description This function calculates the maximum score  combinations..
#' @param mod: object model
#' @return list of max scores
#' @export
CalculateDistanceScores_ <- function(mod){
score_list <- list()
intercept <- list()
scores <- list()
score_list <- mod$score_
intercept <- mod$intercept_
for(i in 1: length(intercept)){
scores[[i]] = abs((score_list[[i]]) - intercept[[i]])
}
return(scores)
}
#' Function to normalize a vector between 0 and 1.
#' @title normalize_scores
#' @description Function to normalize a vector between 0 and 1.
#' @param mod: object model
#' @return list of scores normalize
#' @export
normalize_scores <- function(scores) {
# Function to normalize a vector between 0 and 1
normalize <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
# Normalize each vector in the list
normalized_scores <- lapply(scores, normalize)
return(normalized_scores)
}
#' predictions aggregation function.
#' @title aggregate_predictions
#' @description One versus all predictions aggregation function.
#' @param classes_list: List of one versus all predictions
#' @param score_list: List of one versus all score
#' @return vector of class aggregate
#' @export
aggregate_predictions <- function(classes_list, score_list) {
# Initialization of the aggregation vector
aggregated_predictions <- character(length = length(classes_list[[1]]))
names_class = unique(y)
names_class <- as.character(class_names)
# Loop for each position
for (i in seq_along(aggregated_predictions)) {
# Retrieve classes and scores for this position
current_classes <- sapply(classes_list, function(class_vector) class_vector[i])
scores <- sapply(score_list, function(score_vector) score_vector[i])
# Check if all positions are 'ALL'
if (all(current_classes == "ALL")) {
# Predict the class with the highest score
predicted_class <- names_class[which.max(scores)]
} else {
# Check if 'ALL' is present in the classes
if ("ALL" %in% current_classes) {
# Filter the classes that are not 'ALL'
non_all_classes <- current_classes[current_classes != "ALL"]
# If non-'ALL' classes are present, choose the one with the highest score.
if (length(non_all_classes) > 0) {
max_score_index <- which.max(scores[current_classes %in% non_all_classes])
predicted_class <- non_all_classes[max_score_index]
} else {
# If all classes are 'ALL,' choose either 'ALL' or another class at random.
predicted_class <- "ALL"
}
} else {
# If 'ALL' is not present, choose the class with the highest score.
max_score_index <- which.max(scores)
predicted_class <- current_classes[max_score_index]
}
}
# Fill the aggregation vector at position i.
aggregated_predictions[i] <- predicted_class
}
return(aggregated_predictions)
}
#' Overall additional evaluation metrics.
#' @title EvaluateAdditionnelGlobaleMetrics
#' @description Overall additional evaluation metrics.
#' @param  actual_labels: current class (y)
#' @param aggregated predictions
#' @return mod object
#' @export
EvaluateAdditionnelGlobaleMetrics <- function(predictions, actual_labels) {
# Calculation of the confusion matrix
confusion_matrix <- table(predictions, actual_labels)
# Calculation of the overall accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# Calculation of overall precision, recall, and f1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1 <- 2 * precision * recall / (precision + recall)
# Creation of the model object containing all metrics
mod <- list(
accuracy_ = accuracy,
fit_ = accuracy,
unpenalized_fit_ = accuracy,
precision_ = precision,
recall_ = recall,
f1 = f1,
confusion_matrix = confusion_matrix
)
return(mod)
}
#' Overall population evaluation
#' @title EvaluateAdditionnelGlobaleMetrics
#' @description Overall population evaluation.
#' @param  pop : object pop
#' @param approch: approach
#' @return overall pop object
#' @export
# Defining a function to evaluate the overall performance of a population of models.
evaluatePopulation_overall <- function(pop, approach = "ovo") {
# Initializing an empty list to store the overall evaluation of each model.
pop_overall <- list
# Looping through each model in the population.
for(i in 1:length(pop)){
# Checking if approach is 'one-vs-one' (ovo) or 'one-vs-all' (ova).
if (approach == "ovo"){
# If 'ovo', use the predict_ovo function to generate predictions from the current model.
predictions <- predict_ovo(mod = pop[[i]], y)
}
else{
# If not 'ovo', use the predict_ova function instead.
predictions <- predict_ova(mod = pop[[i]], y)
}
# Calculate the distance scores for the current model.
DistanceScores_ <- CalculateDistanceScores_(mod = pop[[i]])
# Normalize the distance scores.
normalize_scores <- normalize_scores(DistanceScores_)
# Aggregate the predictions based on the classes list and the normalized scores.
aggregate_predictions <- aggregate_predictions(classes_list = predictions, score_list = normalize_scores)
# Evaluate the global additional metrics for the current model.
mod <- EvaluateAdditionnelGlobaleMetrics(predictions = aggregate_predictions, actual_labels = y)
# Add the evaluated model to the overall population list.
pop_overall[[i]] = mod
}
# Return the list containing evaluated models with their respective metrics.
return(pop_overall)
}
pop_overall <- evaluatePopulation_overall(pop = pop.last.eval, approach = approch)
evaluation    <- as.numeric(populationGet_X(element2get = "fit_", toVec = TRUE, na.rm = TRUE)(pop = pop.last.eval))
# get the evaluatio
evaluation
evaluation.ord <- order(abs(evaluation), decreasing = TRUE)
evaluation.ord
evaluation <- evaluation[evaluation.ord]
evaluation
i = 46
if (approach == "ovo"){
# If 'ovo', use the predict_ovo function to generate predictions from the current model.
predictions <- predict_ovo(mod = pop[[i]], y)
}
if (approch == "ovo"){
# If 'ovo', use the predict_ovo function to generate predictions from the current model.
predictions <- predict_ovo(mod = pop[[i]], y)
}
else{
if (approch == "ovo"){
# If 'ovo', use the predict_ovo function to generate predictions from the current model.
predictions <- predict_ovo(mod = pop[[i]], y)
}
else{
if (approch == "ovo"){
# If 'ovo', use the predict_ovo function to generate predictions from the current model.
predictions <- predict_ovo(mod = pop[[i]], y)
} else{
# If not 'ovo', use the predict_ova function instead.
predictions <- predict_ova(mod = pop[[i]], y)
}
pop = pop.last.eval
if (approch == "ovo"){
# If 'ovo', use the predict_ovo function to generate predictions from the current model.
predictions <- predict_ovo(mod = pop[[i]], y)
} else{
# If not 'ovo', use the predict_ova function instead.
predictions <- predict_ova(mod = pop[[i]], y)
}
predictions
DistanceScores_ <- CalculateDistanceScores_(mod = pop[[i]])
normalize_scores <- normalize_scores(DistanceScores_)
# Aggregate the predictions based on the classes list and the normalized scores.
aggregate_predictions <- aggregate_predictions(classes_list = predictions, score_list = normalize_scores)
# Evaluate the global additional metrics for the current model.
mod <- EvaluateAdditionnelGlobaleMetrics(predictions = aggregate_predictions, actual_labels = y)
mod
mod = pop[[46]]
mod
#' Function to predict the class for each "Class vs ALL" combination.
#' @title predict_ova
#' @description Function to predict the class for each "Class vs ALL" combination.
#' @param y: the response vector
#' @param mod: object model
#' @return list of class predict
#' @export
predict_ova <- function(mod, y) {
intercept_list <- mod$intercept_
score_list <- mod$score_
class_names <- unique(y)
class_names <- as.character(class_names)
# List of prediction vectors for each combination.
predictions_list <- lapply(seq_along(intercept_list), function(j) {
class_name <- class_names[j]
sapply(score_list[[j]], function(score) {
ifelse(score > intercept_list[[j]], class_name, "ALL")
})
})
return(predictions_list)
}
#' Function to predict the class for each "Class 1 vs Class 2" combination.
#' @title predict_ovo
#' @description Function to predict the class for each "Class1 vs Class2" combination.
#' @param y: the response vector
#' @param mod: object model
#' @return list of class predict
#' @export
predict_ovo <- function(mod, y) {
list_y <- list()
nClasse <- unique(y)
# Dataset decomposition phase using the one-versus-one and one-versus-all approaches
k <- 1
for (i in 1:(length(nClasse)-1)) {
for (j in (i+1):length(nClasse)) {
class_i <- nClasse[i]
class_j <- nClasse[j]
indices <- which(y == class_i | y == class_j)
y_pair <- y[indices]
list_y[[k]] <- as.vector(y_pair)
k <- k + 1
}
}
num_combinations <- length(list_y)
predictions_list <- list()
for (comb_index in 1:num_combinations) {
intercept <- mod$intercept_[[comb_index]]
scores <- mod$score[[comb_index]]
num_samples <- length(scores)
# Get the unique values of the current combination.
class_values <- list_y[[comb_index]]
unique_classes <- unique(class_values)
# Initialize the prediction vector for this combination
predictions <- character(length = num_samples)
for (sample_index in 1:num_samples) {
# Prediction for the current sample
decision_value <- scores[sample_index]
predictions[sample_index] <- ifelse(decision_value > intercept,unique_classes[2],unique_classes[1])
}
# Add the prediction vector to the list.
predictions_list[[comb_index]] <- predictions
}
return(predictions_list)
}
#' This function calculates the maximum score combinations.
#' @title CalculateDistanceScores_
#' @description This function calculates the maximum score  combinations..
#' @param mod: object model
#' @return list of max scores
#' @export
CalculateDistanceScores_ <- function(mod){
score_list <- list()
intercept <- list()
scores <- list()
score_list <- mod$score_
intercept <- mod$intercept_
for(i in 1: length(intercept)){
scores[[i]] = abs((score_list[[i]]) - intercept[[i]])
}
return(scores)
}
#' Function to normalize a vector between 0 and 1.
#' @title normalize_scores
#' @description Function to normalize a vector between 0 and 1.
#' @param mod: object model
#' @return list of scores normalize
#' @export
normalize_scores <- function(scores) {
# Function to normalize a vector between 0 and 1
normalize <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
# Normalize each vector in the list
normalized_scores <- lapply(scores, normalize)
return(normalized_scores)
}
#' predictions aggregation function.
#' @title aggregate_predictions
#' @description One versus all predictions aggregation function.
#' @param classes_list: List of one versus all predictions
#' @param score_list: List of one versus all score
#' @return vector of class aggregate
#' @export
aggregate_predictions <- function(classes_list, score_list) {
# Initialization of the aggregation vector
aggregated_predictions <- character(length = length(classes_list[[1]]))
names_class = unique(y)
names_class <- as.character(class_names)
# Loop for each position
for (i in seq_along(aggregated_predictions)) {
# Retrieve classes and scores for this position
current_classes <- sapply(classes_list, function(class_vector) class_vector[i])
scores <- sapply(score_list, function(score_vector) score_vector[i])
# Check if all positions are 'ALL'
if (all(current_classes == "ALL")) {
# Predict the class with the highest score
predicted_class <- names_class[which.max(scores)]
} else {
# Check if 'ALL' is present in the classes
if ("ALL" %in% current_classes) {
# Filter the classes that are not 'ALL'
non_all_classes <- current_classes[current_classes != "ALL"]
# If non-'ALL' classes are present, choose the one with the highest score.
if (length(non_all_classes) > 0) {
max_score_index <- which.max(scores[current_classes %in% non_all_classes])
predicted_class <- non_all_classes[max_score_index]
} else {
# If all classes are 'ALL,' choose either 'ALL' or another class at random.
predicted_class <- "ALL"
}
} else {
# If 'ALL' is not present, choose the class with the highest score.
max_score_index <- which.max(scores)
predicted_class <- current_classes[max_score_index]
}
}
# Fill the aggregation vector at position i.
aggregated_predictions[i] <- predicted_class
}
return(aggregated_predictions)
}
#' Overall additional evaluation metrics.
#' @title EvaluateAdditionnelGlobaleMetrics
#' @description Overall additional evaluation metrics.
#' @param  actual_labels: current class (y)
#' @param aggregated predictions
#' @return mod object
#' @export
EvaluateAdditionnelGlobaleMetrics <- function(predictions, actual_labels) {
# Calculation of the confusion matrix
confusion_matrix <- table(predictions, actual_labels)
# Calculation of the overall accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# Calculation of overall precision, recall, and f1-score
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1 <- 2 * precision * recall / (precision + recall)
# Creation of the model object containing all metrics
mod <- list(
accuracy_ = accuracy,
fit_ = accuracy,
unpenalized_fit_ = accuracy,
precision_ = precision,
recall_ = recall,
f1 = f1,
confusion_matrix = confusion_matrix
)
return(mod)
}
#' Overall population evaluation
#' @title EvaluateAdditionnelGlobaleMetrics
#' @description Overall population evaluation.
#' @param  pop : object pop
#' @param approch: approach
#' @return overall pop object
#' @export
# Defining a function to evaluate the overall performance of a population of models.
evaluatePopulation_overall <- function(pop, approach = "ovo") {
# Initializing an empty list to store the overall evaluation of each model.
pop_overall <- list
# Looping through each model in the population.
for(i in 1:length(pop)){
# Checking if approach is 'one-vs-one' (ovo) or 'one-vs-all' (ova).
if (approch == "ovo"){
# If 'ovo', use the predict_ovo function to generate predictions from the current model.
predictions <- predict_ovo(mod = pop[[i]], y)
} else{
# If not 'ovo', use the predict_ova function instead.
predictions <- predict_ova(mod = pop[[i]], y)
}
# Calculate the distance scores for the current model.
DistanceScores_ <- CalculateDistanceScores_(mod = pop[[i]])
# Normalize the distance scores.
normalize_scores <- normalize_scores(DistanceScores_)
# Aggregate the predictions based on the classes list and the normalized scores.
aggregate_predictions <- aggregate_predictions(classes_list = predictions, score_list = normalize_scores)
# Evaluate the global additional metrics for the current model.
mod <- EvaluateAdditionnelGlobaleMetrics(predictions = aggregate_predictions, actual_labels = y)
# Add the evaluated model to the overall population list.
pop_overall[[i]] = mod
}
# Return the list containing evaluated models with their respective metrics.
return(pop_overall)
}
pop_overall <- evaluatePopulation_overall(pop = pop.last.eval, approach = approch)
best.model_terga1_mcovo
mod <- list()
mod$intercept_ = best.model_terga1_mcovo$list_intercept_mc
mod$score_ = best.model_terga1_mcovo$score_
predictions <- predict_ova(mod , y)
DistanceScores_ <- CalculateDistanceScores_(mod)
normalize_scores <- normalize_scores(DistanceScores_)
aggregate_predictions <- aggregate_predictions(classes_list = predictions, score_list = normalize_scores)
# Evaluate the global additional metrics for the current model.
mod <- EvaluateAdditionnelGlobaleMetrics(predictions = aggregate_predictions, actual_labels = y)
mod
library(mcpredomics)
