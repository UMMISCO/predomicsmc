cat("... Loading feature correlation for speedup\n")
# restore precomputed object from the hard drive
load(path.feature.cor)
cat("... Correlation file loaded\n")
if(exists("feature.cor"))
{
if(any(is.na(match(rownames(feature.cor), rownames(X)))))
{
stop(paste("... feature.cor does not match X... needs to be recomputed. You can delete the following file",path.feature.cor))
}
}else
{
stop("feature.cor object does not exist. Please check the name.")
}
}else
{
cat("... Computing feature correlation for speedup\n")
if(clf$params$objective == "cor") # correlation
{
# if any NA in y omit them
if(any(is.na(y)))
{
ina <- is.na(y)
cat(paste("... y contains ",sum(ina), "NA values ... ommiting the observations in both y and X\n"))
# transforming to dataframe first will make sure to keep the dimnames such as for instance when it is sparse table or otu_table (phyloseq)
y.nona <- y[!ina]
X.nona <- X[,!ina]
}else
{
y.nona <- y
X.nona <- X
}
feature.cor     <- filterfeaturesK(data = t(apply(X.nona, 1, rank)), # for speedup
trait = rank(y.nona),             # for speedup
k = max.nb.features,
type = "pearson",                 # for speedup
sort = TRUE,
verbose = clf$params$verbose,
return.data = FALSE) # to avoid having to recompute this all the time
}else # classification
{
# Dataset decomposition phase using the one-versus-one and one-versus-all approaches
nClasse <- unique(y)
feature.cor   <- list() # List of different combinations of feature.cor
list_y <- list() #  List of different combinations of y
list_X <- list() #  List of different combinations of X
if (approch == "ovo") {
k <- 1
for (i in 1:(length(nClasse)-1)) {
for (j in (i+1):length(nClasse)) {
class_i <- nClasse[i]
class_j <- nClasse[j]
indices <- which(y == class_i | y == class_j)
y_pair <- y[indices]
X_pair <- X[, indices]
list_y[[k]] <- as.vector(y_pair)
list_X[[k]] <- X_pair
k <- k + 1
}
}
} else {
for (i in 1:length(nClasse)) {
class_i <- nClasse[i]
y_temp <- ifelse(y == class_i, as.character(class_i), "All")
list_y[[i]] <- as.vector(y_temp)
list_X[[i]] <- X
}
}
for (i in 1:(length(list_y))) {
feature.cor[[i]]     <- filterfeaturesK(data = list_X[[i]],
trait = list_y[[i]],
k = max.nb.features,
type = "wilcoxon",
sort = TRUE,
verbose = clf$params$verbose,
return.data = FALSE) # to avoid having to recompute this all the time
}
}
if(!is.null(path))
{
save(feature.cor, file = path.feature.cor)
cat("... Correlation file saved\n")
}
}
cat("... Storing data in the classifier object for speedup\n")
clf$feature.cor <- feature.cor # add them to the clf
if(all(is.na(clf$feature.cor[[1]]$p)))
{
warning("runClassifier: does not seem to have produced a pvalue")
}
# store the initial order and indexes
clf$data          <- list()
list_features <- list()
clf$data$features <- list_features
names(clf$data$features) <- clf$data$features
list_XX <- list() # List of X combinations
list_min <- list() # List min of X
list_max <- list() # List max of X
for (i in 1:(length(list_X))) {
Xi <- list_X[[i]][rownames(clf$feature.cor[[i]])[1:max.nb.features],]
mino <- min(Xi, na.rm=TRUE)
maxo <- max(Xi, na.rm=TRUE)
list_XX[[i]] <- Xi
list_min[[i]] <-  mino
list_max[[i]] <-  maxo
}
clf$data$X        <- list_XX
clf$data$X.min    <- list_min
clf$data$X.max    <- list_max
clf$data$y        <- list_y
# compute the coefficients once for all to improve performance
cat("... Computing ternary coefficients for speedup\n")
coeffs          <- getSign_mc(X = X, y = y, clf = clf, approch = approch, parallel.local = FALSE)
clf$coeffs_     <- coeffs # add them to the clf
# check sparsity not to be larger than variables in X
if(any(is.na(clf$params$sparsity > nrow(X))))
{
# adding the maximum number of featuers
clf$params$sparsity <- c(clf$params$sparsity, nrow(X))
}
# mark NAs the bigger ones
clf$params$sparsity[clf$params$sparsity > nrow(X)] <- NA
# delete them
clf$params$sparsity <- clf$params$sparsity[!is.na(clf$params$sparsity)]
# set the seed while sanity checking
if(!(any(clf$params$seed=="NULL")))
{
# convert from list to a vector
if(is.list(clf$params$seed))
{
clf$params$seed <- unlist(clf$params$seed)
if(any(class(clf$params$seed)!="numeric"))
{
stop("fit: convertion of seed from list to numeric vector failed.")
}
}
# we can have multiple k-folds per experiment if the seed is vectors of seeds
if(length(clf$params$seed)==1)
{
set.seed(clf$params$seed)
cat("... One seed found, setting by default\n")
}else
{
if(length(clf$params$seed)==0)
{
stop("fit: the seed should have at least one value.")
}
# if we are here this means that it everything is as expected seed is a
# vector of numeric values.
set.seed(clf$params$seed[1])
cat("... Multiple seeds found, setting the default\n")
}
}
# check and set the number of folds
if(!is.null(lfolds))
{
cat("... Custom folds are provided\n")
if(!is.list(lfolds))
{
lfolds = NULL
}else #if lfolds exists
{
nfolds = length(lfolds)
}
}
if(nfolds == 1)
{
cat("... The number of folds is set to 1. In this case I'm deactivating the cross-validation process\n")
cross.validate = FALSE
}
# set the parallelize.folds parameter. If no crossval than it is deactivated
clf$params$parallelize.folds <- parallelize.folds & cross.validate & clf$params$parallel
# add a parallel.local parameter if we wish to speed out some local steps
clf$params$parallel.local <- FALSE
# START CLUSTER if parallel computing set the cluster
if(clf$params$parallel)
{
cat(paste("... Running the classifier", clf$learner,"in parallel mode with ",clf$params$nCores + 1,"CPUs\n"))
# adding another core for the whole dataset. If it is taken into account
# during the launch this will be a sleeping thread so no harm, if not it
# will allow to run faster as we won't forget to increment it
registerDoSNOW(clf$params$cluster <- makeCluster(clf$params$nCores + 1, type = "SOCK", outfile = log.file))
if(!clf$params$parallelize.folds)  # if folds are not parallelized
{
clf$params$parallel.local <- TRUE # switch the local parallel to TRUE
}
}else
{
cat(paste("... Running the classifier", clf$learner,"with a single CPU\n"))
}
# for all the predomics learners
if(!isLearnerSota(clf)) # if runing the BTR algorithms
{
# set the epsilon
if(!is.null(clf$params$epsilon))
{
if(clf$params$epsilon=="NULL")
{
#clf$params$epsilon        <- .Machine$double.xmin
clf$params$epsilon        <- 0
if(clf$params$verbose) cat("... Setting epsilon for predomics learners\n")
}
}
}
# save the data step by step to be able to resume
if(clf$experiment$save != "nothing")
{
if(clf$params$verbose) cat("... Saving experiments\n")
fileNames                 <- gsub(" ", "_", clf$experiment$id)
dir.create(fileNames)
setwd(fileNames)
saveResults(X, paste("X", fileNames, "yml", sep = "."))
saveResults(y, paste("Y", fileNames, "yml", sep = "."))
experiment <- list()
experiment$desc           <- clf$experiment$description
experiment$params         <- list(clf=list(learner = clf$learner, params = clf$params, experiment = clf$experiment))
if(clf$experiment$save == "full")
{
if((clf$params$popSaveFile=="NULL"))
{
clf$params$popSaveFile <- fileNames
}
}
}
switch(clf$learner,
terda=
{
# Here we handle also the sota.glmnet as well since this is a derivate of terda
cat('... terda fitting based on Linear programming relaxation ...\n')
},
terga1=
{
cat('... First version of terga fitting based on Genetic Algorithm heuristics ...\n')
},
terga1_mc=
{
cat('... First version of terga fitting based on Genetic Algorithm heuristics ...\n')
},
terga2=
{
cat('... Second and faster version of terga fitting based on Genetic Algorithm heuristics ...\n')
},
terBeam=
{
cat('... terbeam fitting based on Exhaustive Heuristic beam search ...\n')
},
terBeam_mc=
{
cat('... terbeam fitting based on Exhaustive Heuristic beam search ...\n')
},
metal=
{
cat('... model fitting based on aggregating different Heuristics ...\n')
},
sota.svm=
{
cat('... SOTA: state of the art SVM fitting ...\n')
},
sota.rf=
{
cat('... SOTA: state of the art Ranfom Forest fitting ...\n')
},
sota.rf_ovo =
{
cat('... SOTA: state of the art Ranfom Forest fitting MC ...\n')
},
{
warning('This method does not exist !')
}
)
clf$data$y,table
(clf$data$y,table)
lapply(lapply(clf$data$y,table), names)
unlist(lapply(lapply(lapply(clf$data$y,table), names), paste))
lapply(lapply(lapply(clf$data$y,table), names), paste, collapse = "_")
to = lapply(lapply(lapply(clf$data$y,table), names), paste, collapse = "_")
to
class(to)
ta <- list()
ta[[1]]to[[1]]$fit_1
ta[[1]]$to[[1]]$fit_1
dfx = to[[1]]$fit_1
dfx <- list()
dfx=to[[1]]
dfx$fit_1
dfx
to
mod
mod.res = mod$indices_
length(mod.res)
datadata = clf
# Setting the language environment
switch(clf$params$language,
ter=
{
# ternary language without intercept (maximize the auc)
if(clf$params$verbose){print("Setting environment for the language 'ter'")}
if(clf$params$objective == "cor")
{
clf$params$evalToFit <- "cor_"
}else
{
# note that here with the ter language we could not use auc to fit since the intercept should be 0
clf$params$intercept = 0
if(clf$params$evalToFit == "auc_")
{
clf$params$evalToFit <- "accuracy_"
warning("terga1_fit: changing evalToFit from auc_ to accuracy_ because of the language.")
}
}
},
terinter=
{
# ternary language without intercept (maximize the accuracy)
if(clf$params$verbose){print("Setting environment for the language 'terinter'")}
if(clf$params$objective == "cor")
{
clf$params$evalToFit <- "cor_"
}
},
bin=
{
# ternary language without intercept (maximize the auc)
if(clf$params$verbose){print("Setting environment for the language 'bin'")}
if(clf$params$objective == "cor")
{
clf$params$evalToFit <- "cor_"
}else
{
# note that here with the ter language we could not use auc to fit since the intercept should be 0
clf$params$intercept = 0
if(clf$params$evalToFit == "auc_")
{
clf$params$evalToFit <- "accuracy_"
warning("terga1_fit: changing evalToFit from auc_ to accuracy_ because of the language.")
}
}
},
bininter=
{
# ternary language without intercept (maximize the auc)
if(clf$params$verbose){print("Setting environment for the language 'bininter'")}
if(clf$params$objective == "cor")
{
clf$params$evalToFit <- "cor_"
}
},
ratio=
{
# ternary language without intercept (maximize the auc)
if(clf$params$verbose){print("Setting environment for the language 'ratio'")}
if(clf$params$objective == "cor")
{
clf$params$evalToFit <- "cor_"
}
},
{
stop(paste("The language",clf$params$language, "is not implemented !"))
}
)
# set the size of the world
clf$params$size_world <- nrow(X)
# Print the experiment configuration
if(clf$params$verbose) printClassifier(obj = clf)
res <- list() # the final object containing the evolved models
for(i in clf$params$sparsity) # sparsity is = k, i.e. the number of features in a model
{
cat("\t\tResolving problem with\t", i, "\tvariables ...\n")
# set the current_sparsity
clf$params$current_sparsity <- i
# test for
if (clf$params$current_sparsity == 1 & !clf$params$evolve_k1) # if we want to evolve features for k_sparse=1 we create a normal population
{
pop_last      <- as.list(1:nrow(X)) # create population with k_sparse = 1
}else
{
best_ancestor = NULL
if (exists("pop_last"))
{
if(length(evaluation) != 0)
{
best_ancestor = pop_last[[which.max(evaluation)]]
}
# build a new population seeded by the last one
pop         <- population(clf = clf,
size_ind = i,
#size_world = nrow(list_X[[1]])
size_world = nrow(X),
best_ancestor = best_ancestor,
size_pop = clf$params$size_pop,
seed = clf$params$current_seed)
}else
{
# build a new population from scratch
pop         <- population(clf = clf,
size_ind = i,
size_world = nrow(X),
best_ancestor = best_ancestor,
size_pop = clf$params$size_pop,
seed = clf$params$current_seed)
}
# if this is population with model objects we transform in an index population
if(isPopulation(obj = pop))
{
pop <- listOfModelsToListOfSparseVec(list.models = pop)
}
# then we evolve
pop_last      <- evolve_mc(X, y, clf, pop, seed = clf$params$current_seed,approch = approch)
list_indices <- list() # list of indices
# through the indices
for (i in 1:length(pop_last[[1]])) {
# Initialize the list of indices for each iteration
list_indices[[i]] <- list()
# Iterate through the different lists in list_evolved_pop.
for (j in 1:length(pop_last)) {
if (i <= length(pop_last[[j]])) {
list_indices[[i]][[j]] <- pop_last[[j]][[i]]
} else {
list_indices[[i]][[j]] <- list_indices[[i - 1]][[j]]
}
}
}
}
pop_last = list_indices
# evaluate the fitting function for all the models of the populaion
# transform to a population of model objects
#pop_last.mod <- list()
pop_last.mod <- listOfSparseVecToListOfModels_mc(X, y , clf = clf, v = pop_last,approch = approch)
}
pop_last.mod
mo = pop_last.mod[[1]]
eval.all = FALSE
force.re.evaluation = FALSE
estim.feat.importance = FALSE
mode = "train"
delete.null.models = TRUE
lfolds = NULL
re <- evaluateModel_mc(mod = mo,
X = X,
y = y,
clf = clf,
eval.all = eval.all,
force.re.evaluation = force.re.evaluation,
estim.feat.importance = estim.feat.importance,
approch = approch,
mode = mode)
predict_ovo <- predict_ovo(mod = re, y, X, clf, force.re.evaluation = TRUE )
predict_ovo
aggregate_majoritaire_vote_ovo <- aggregate_majoritaire_vote_ovo(predictions_list = predict_ovo)
aggregate_majoritaire_vote_ovo
predict_ovo <- predict_ovo(mod = re, y, X, clf, force.re.evaluation = TRUE )
predict_ovo
re
library(mcpredomics)
knitr::opts_chunk$set(echo = TRUE)
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Créer un vecteur d'indices pour la division des données
set.seed(42)  # Vous pouvez définir une graine pour la reproductibilité
y = as.vector(yvec_trie)
X = X_general
# Nombre d'échantillons désiré dans chaque classe
nombre_echantillons_par_classe <- min(table(y))
# Fonction pour équilibrer les classes
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe,seed =123) {
classes <- unique(y)
indices_equilibres <- integer(0)
for (classe in classes) {
indices_classe <- which(y == classe)
set.seed(seed)
indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
}
return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}
# Équilibrer les classes dans l'ensemble d'entraînement
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
# Utiliser les données équilibrées
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X
# Vérifier la répartition après équilibrage
#table(y_equilibre)
set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.1, list = FALSE)
# Diviser yvec_trie en 80% train et 20% test
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[,indices_division]
X.test <- X_equilibre[,-indices_division]
# Vérifier la répartition dans chaque ensemble
table(y)
table(y.test)
dim(X)
dim(X.test)
clf <- terga1_mc(nCores = 1,
seed = 1,
plot = TRUE
)
printy(clf) # print the object for more information
isClf(clf)  # test whether the object is a classifier
class(clf)  # the class of the classifier object
runit = TRUE
if(runit)
{
res_clf <- fit_mc(X = X, y = y, clf = clf,approch="ova", cross.validate = TRUE, nfolds = 1); # class(res_clf)
# save results
save(res_clf, clf, file = "res_clf.rda", compression_level = 9)
}
