) %>%
mutate(
Constraint_factor = factor(Constraint_factor,
levels = c("None", "Unconstrained", "Semi_Constrained", "Full_Constrained"))
)
# Plot the mean accuracy ± SD per method, for each dataset and combination of approach + binarization
ggplot(df_all, aes(x = Methods, y = mean_acc,
color = Constraint_factor)) +
geom_point(position = position_dodge(width = 0.6), size = 3) +   # plot mean points
geom_errorbar(aes(ymin = mean_acc - sd_acc, ymax = mean_acc + sd_acc),
position = position_dodge(width = 0.6), width = 0.2) +  # add error bars
facet_grid(Dataset ~ Approach + Binarisation,                     # create subplots by dataset, approach, and binarization
scales = "free_x", space = "free_x") +
labs(
title = "Comparison of Methods: Test Accuracy under 10-Fold Cross-Validation – Predomics vs SOTA",
x = "Method",
y = "Accuracy (mean ± SD)",
color = "Constraints"
) +
theme_bw() +   # use a clean black-and-white theme
theme(
axis.text.x = element_text(angle = 45, hjust = 1),         # rotate x-axis labels
strip.background = element_rect(fill = "grey95"),          # style subplot labels
legend.position = "bottom"                                 # place legend at the bottom
)
# Filter out rows where the binarization strategy is set to "NO"
df_filtereds <- Analysis_Dataset_Complet_Sota_Predo  %>%
filter(Binarisation != "NO")
# From the filtered data, keep only the test set results,
# then group by dataset, method, constraint level, approach, and binarization strategy
# Compute the mean and standard deviation of Recall for each group
# Finally, convert the constraint factor to an ordered factor for consistent plotting
df_all <- df_filtereds %>%
filter(Set == "Test") %>%
group_by(Dataset, Methods, Constraint_factor, Approach, Binarisation) %>%
summarise(
mean_rec = mean(Recall, na.rm = TRUE),       # average test Recall
sd_rec = sd(Recall, na.rm = TRUE),           # standard deviation of Recall
.groups = "drop"
) %>%
mutate(
Constraint_factor = factor(Constraint_factor,
levels = c("None", "Unconstrained", "Semi_Constrained", "Full_Constrained"))
)
# Plot the mean Recall ± SD per method, for each dataset and combination of approach + binarization
ggplot(df_all, aes(x = Methods, y = mean_rec,
color = Constraint_factor)) +
geom_point(position = position_dodge(width = 0.6), size = 3) +   # plot mean Recall points
geom_errorbar(aes(ymin = mean_rec - sd_rec, ymax = mean_rec + sd_rec),
position = position_dodge(width = 0.6), width = 0.2) +  # add error bars
facet_grid(Dataset ~ Approach + Binarisation,                     # create subplots by dataset, approach, and binarization
scales = "free_x", space = "free_x") +
labs(
title = "Comparison of Methods: Test Recall under 10-Fold Cross-Validation – Predomics vs SOTA",
x = "Method",
y = "Recall (mean ± SD)",
color = "Constraints"
) +
theme_bw() +   # use a clean black-and-white theme
theme(
axis.text.x = element_text(angle = 45, hjust = 1),         # rotate x-axis labels
strip.background = element_rect(fill = "grey95"),          # style subplot labels
legend.position = "bottom"                                 # place legend at the bottom
)
# Load the dataset containing performance results
data("Analysis_Dataset_Complet_Sota_Predo")
# Filter to keep only experiments where binarization was applied
df_filtereds <- Analysis_Dataset_Complet_Sota_Predo %>%
filter(Binarisation != "NO")
# Select only test set results and compute mean and standard deviation of Precision
df_all <- df_filtereds %>%
filter(Set == "Test") %>%
group_by(Dataset, Methods, Constraint_factor, Approach, Binarisation) %>%
summarise(
mean_prec = mean(Precision, na.rm = TRUE),
sd_prec = sd(Precision, na.rm = TRUE),
.groups = "drop"
) %>%
mutate(
Constraint_factor = factor(
Constraint_factor,
levels = c("None", "Unconstrained", "Semi_Constrained", "Full_Constrained")
)
)
# Plot mean Precision ± SD using ggplot2
ggplot(df_all, aes(x = Methods, y = mean_prec, color = Constraint_factor)) +
geom_point(position = position_dodge(width = 0.6), size = 3) +
geom_errorbar(aes(ymin = mean_prec - sd_prec, ymax = mean_prec + sd_prec),
position = position_dodge(width = 0.6), width = 0.2) +
facet_grid(Dataset ~ Approach + Binarisation,
scales = "free_x", space = "free_x") +
labs(
title = "Comparison of Methods: Test Precision under 10-Fold Cross-Validation – Predomics vs SOTA",
x = "Method",
y = "Precision (mean ± SD)",
color = "Constraints"
) +
theme_bw() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
strip.background = element_rect(fill = "grey95"),
legend.position = "bottom"
)
# Load the dataset containing performance results
data("Analysis_Dataset_Complet_Sota_Predo")
# Filter to include only experiments with binarization applied
df_filtereds <- Analysis_Dataset_Complet_Sota_Predo %>%
filter(Binarisation != "NO")
# Select only test results and calculate mean and standard deviation of F1-score
df_all <- df_filtereds %>%
filter(Set == "Test") %>%
group_by(Dataset, Methods, Constraint_factor, Approach, Binarisation) %>%
summarise(
mean_f1 = mean(F1, na.rm = TRUE),
sd_f1 = sd(F1, na.rm = TRUE),
.groups = "drop"
) %>%
mutate(
Constraint_factor = factor(
Constraint_factor,
levels = c("None", "Unconstrained", "Semi_Constrained", "Full_Constrained")
)
)
# Create the plot of F1-score (mean ± SD)
ggplot(df_all, aes(x = Methods, y = mean_f1, color = Constraint_factor)) +
geom_point(position = position_dodge(width = 0.6), size = 3) +
geom_errorbar(aes(ymin = mean_f1 - sd_f1, ymax = mean_f1 + sd_f1),
position = position_dodge(width = 0.6), width = 0.2) +
facet_grid(Dataset ~ Approach + Binarisation,
scales = "free_x", space = "free_x") +
labs(
title = "Comparison of Methods: Test Accuracy under 10-Fold Cross-Validation – Predomics vs SOTA",
x = "Method",
y = "F1-score (mean ± SD)",
color = "Constraints"
) +
theme_bw() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
strip.background = element_rect(fill = "grey95"),
legend.position = "bottom"
)
# 1. Filter relevant Predomics configurations
# Only keep Test set results for OVA and OVO binarization strategies,
# and restrict to standard methods used in both.
df_predomics_1 <- Analysis_Dataset_Complet_Sota_Predo %>%
filter(
Set == "Test",
Binarisation %in% c("OVA", "OVO"),
Methods %in% c("Voting", "Voting_with_Tie_Breaking", "Maximization")
)
# 2. Fit a mixed-effects linear model with Accuracy as outcome
# Fixed effects: Binarisation strategy and Approach (Terbeam vs Terga1)
# Random effects: Dataset and Fold (to account for repeated measures)
mod.binarisation <- lmer(
Accuracy ~ Binarisation + Approach + (1 | Dataset) + (1 | Fold),
data = df_predomics_1
)
# 3. Display the summary of the model
summary(mod.binarisation)
# 4. Extract fixed effects estimates
coefs <- tidy(mod.binarisation, effects = "fixed")
# 5. Construct reference configuration: OVA + Terbeam
base <- coefs %>%
filter(term == "(Intercept)") %>%
transmute(
Condition = "OVA + Terbeam",
Estimate = estimate,
Std_Error = std.error
)
# 6. Compute derived configuration: OVO + Terbeam
ovo_terbeam <- coefs %>%
filter(term == "BinarisationOVO") %>%
transmute(
Condition = "OVO + Terbeam",
Estimate = base$Estimate + estimate,
Std_Error = sqrt(std.error^2 + base$Std_Error^2)
)
# 7. Compute derived configuration: OVA + Terga1
ova_terga1 <- coefs %>%
filter(term == "ApproachTerga1 Predomics") %>%
transmute(
Condition = "OVA + Terga1",
Estimate = base$Estimate + estimate,
Std_Error = sqrt(std.error^2 + base$Std_Error^2)
)
# 8. Compute derived configuration: OVO + Terga1
ovo_terga1 <- tibble(
Condition = "OVO + Terga1",
Estimate = base$Estimate +
coefs$estimate[coefs$term == "BinarisationOVO"] +
coefs$estimate[coefs$term == "ApproachTerga1 Predomics"],
Std_Error = sqrt(
base$Std_Error^2 +
coefs$std.error[coefs$term == "BinarisationOVO"]^2 +
coefs$std.error[coefs$term == "ApproachTerga1 Predomics"]^2
)
)
# 9. Combine all configurations and compute confidence intervals
df_plot1 <- bind_rows(base, ovo_terbeam, ova_terga1, ovo_terga1) %>%
mutate(
Lower = Estimate - 1.96 * Std_Error,
Upper = Estimate + 1.96 * Std_Error
)
# 10. Generate a forest plot to visualize model estimates
ggplot(df_plot1, aes(x = Estimate, y = reorder(Condition, Estimate))) +
geom_point(size = 3) +
geom_errorbarh(aes(xmin = Lower, xmax = Upper), height = 0.2) +
geom_vline(xintercept = base$Estimate, linetype = "dashed", color = "gray") +
labs(
title = "Forest Plot of OVA/OVO × Terbeam/Terga1 Configurations",
x = "Estimated Accuracy (95% Confidence Interval)",
y = "Configuration"
) +
theme_minimal()
# 0. Subset the dataset: keep only rows with OVO binarisation and Terbeam approach
df_ovo_terbeam <- df_predomics_1 %>%
filter(Binarisation == "OVO", Approach == "Terbeam Predomics")
# 1. Fit a linear mixed-effects model
# Accuracy is modeled as a function of Constraint_factor with random intercepts for Dataset and Fold
mod.constraint <- lmer(Accuracy ~ Constraint_factor + (1 | Dataset) + (1 | Fold),
data = df_ovo_terbeam)
# 2. Display the model summary (fixed and random effects)
summary(mod.constraint)
# 3. Extract fixed effects estimates and standard errors
coefs <- tidy(mod.constraint, effects = "fixed")
# 4. Reference configuration: Full-Constrained (Intercept term)
full <- coefs %>% filter(term == "(Intercept)") %>%
transmute(
Condition = "Full-Constrained",
Estimate = estimate,
Std_Error = std.error
)
# 5. Semi-Constrained: compute estimate and propagated standard error
semi <- coefs %>% filter(term == "Constraint_factorSemi_Constrained") %>%
transmute(
Condition = "Semi-Constrained",
Estimate = full$Estimate + estimate,
Std_Error = sqrt(full$Std_Error^2 + std.error^2)
)
# 6. Unconstrained: compute estimate and propagated standard error
unco <- coefs %>% filter(term == "Constraint_factorUnconstrained") %>%
transmute(
Condition = "Unconstrained",
Estimate = full$Estimate + estimate,
Std_Error = sqrt(full$Std_Error^2 + std.error^2)
)
# 7. Combine all three constraint configurations and calculate 95% confidence intervals
df_plot2 <- bind_rows(full, semi, unco) %>%
mutate(
Lower = Estimate - 1.96 * Std_Error,
Upper = Estimate + 1.96 * Std_Error
)
# 8. Plot a forest plot to visualize estimated accuracy with confidence intervals
ggplot(df_plot2, aes(x = Estimate, y = reorder(Condition, Estimate))) +
geom_point(size = 3) +
geom_errorbarh(aes(xmin = Lower, xmax = Upper), height = 0.2) +
geom_vline(xintercept = full$Estimate, linetype = "dashed", color = "gray") +
labs(
title = "Forest Plot: Accuracy by Constraint Factor (OVO + Terbeam)",
x = "Estimated Accuracy (95% CI)",
y = "Constraint Level"
) +
theme_minimal()
# Filter the dataset to retain only the methods of interest (Voting, Voting with Tie Breaking, Maximization)
df_methods <- df_ovo_terbeam %>%
filter(Methods %in% c("Voting", "Voting_with_Tie_Breaking", "Maximization"))
# Fit a linear mixed-effects model to evaluate the effect of aggregation method on Accuracy
mod.methods <- lmer(Accuracy ~ Methods + (1 | Dataset) + (1 | Fold), data = df_methods)
# View model summary (fixed and random effects)
summary(mod.methods)
# Extract fixed effects coefficients
coefs <- tidy(mod.methods, effects = "fixed")
# Reference condition: Maximization (Intercept term)
ref <- coefs %>% filter(term == "(Intercept)") %>%
transmute(
Condition = "Maximization",
Estimate = estimate,
Std_Error = std.error
)
# Voting method effect
voting <- coefs %>% filter(term == "MethodsVoting") %>%
transmute(
Condition = "Voting",
Estimate = ref$Estimate + estimate,
Std_Error = sqrt(ref$Std_Error^2 + std.error^2)
)
# Voting with Tie Breaking method effect
vtb <- coefs %>% filter(term == "MethodsVoting_with_Tie_Breaking") %>%
transmute(
Condition = "Voting_with_Tie_Breaking",
Estimate = ref$Estimate + estimate,
Std_Error = sqrt(ref$Std_Error^2 + std.error^2)
)
# Combine all methods and compute 95% confidence intervals
df_plot3 <- bind_rows(ref, voting, vtb) %>%
mutate(
Lower = Estimate - 1.96 * Std_Error,
Upper = Estimate + 1.96 * Std_Error
)
# Forest plot to visualize estimated Accuracy by aggregation method
ggplot(df_plot3, aes(x = Estimate, y = reorder(Condition, Estimate))) +
geom_point(size = 3) +
geom_errorbarh(aes(xmin = Lower, xmax = Upper), height = 0.2) +
geom_vline(xintercept = ref$Estimate, linetype = "dashed", color = "gray") +
labs(
title = "Forest Plot: Accuracy by Aggregation Method (OVO + Terbeam)",
x = "Estimated Accuracy (95% CI)",
y = "Method"
) +
theme_minimal()
# 1. Filter the dataset to include only SOTA models, with OVA or OVO binarisation, and only test set results
df_sota_ova_ovo_test <- Analysis_Dataset_Complet_Sota_Predo %>%
filter(
Approach == "Sota",                 # Keep only SOTA models
Binarisation %in% c("OVA", "OVO"),  # Compare OVA vs OVO
Set == "Test"                       # Use only test results for model evaluation
)
# 2. Fit a linear mixed-effects model
# Accuracy is predicted from Binarisation strategy, accounting for random effects across datasets
model_sota_binarisation <- lmer(
Accuracy ~ Binarisation + (1 | Dataset),
data = df_sota_ova_ovo_test
)
# 3. Display model summary to assess fixed and random effects
summary(model_sota_binarisation)
# 1. Filter the data: keep only SOTA methods with OVO binarisation on the Test set
df_sota_ovo_test <- Analysis_Dataset_Complet_Sota_Predo %>%
filter(Approach == "Sota",
Binarisation == "OVO",
Set == "Test")
# 2. Fit the linear mixed-effects model with Method as fixed effect
# and Dataset as a random intercept
model_sota_ovo <- lmer(Accuracy ~ Methods + (1 | Dataset), data = df_sota_ovo_test)
# 3. Display model summary
summary(model_sota_ovo)
# 4. Extract fixed effects
coefs <- tidy(model_sota_ovo, effects = "fixed")
intercept <- coefs$estimate[coefs$term == "(Intercept)"]
# 5. Automatically detect the reference method (alphabetical order used by lmer)
ref_method_name <- df_sota_ovo_test %>%
pull(Methods) %>%
unique() %>%
sort() %>%
.[1]
# 6. Build row for reference method
ref_method <- tibble(
Method = ref_method_name,
Estimate = intercept,
Std_Error = coefs$std.error[1],
Lower = intercept - 1.96 * coefs$std.error[1],
Upper = intercept + 1.96 * coefs$std.error[1]
)
# 7. Compute estimates for other methods (add to intercept)
df_methods_ovo <- coefs %>%
filter(term != "(Intercept)") %>%
mutate(
Method = gsub("Methods", "", term),
Estimate = estimate + intercept,
Lower = Estimate - 1.96 * std.error,
Upper = Estimate + 1.96 * std.error
) %>%
select(Method, Estimate, std.error, Lower, Upper) %>%
rename(Std_Error = std.error)
# 8. Combine reference method with other methods
df_methods_ovo <- bind_rows(ref_method, df_methods_ovo) %>%
arrange(desc(Estimate))  # Order from best to worst
# 9. Print the results table
print(df_methods_ovo)
# 10. Visualize using a forest plot
ggplot(df_methods_ovo, aes(x = Estimate, y = reorder(Method, Estimate))) +
geom_point(size = 3) +
geom_errorbarh(aes(xmin = Lower, xmax = Upper), height = 0.2) +
geom_vline(xintercept = intercept, linetype = "dashed", color = "gray") +
labs(
title = "Accuracy by Method (SOTA - OVO, Test Set)",
x = "Estimated Accuracy (95% Confidence Interval)",
y = "Method"
) +
theme_minimal()
# Load necessary packages
library(lme4)
library(broom.mixed)
# Step 1: Filter Predomics data (OVO + Voting_with_Tie_Breaking, Test set)
df_predomics <- df_ovo_terbeam %>%
filter(Methods == "Voting_with_Tie_Breaking", Set == "Test") %>%
mutate(Method_Group = "Predomics")
# Step 2: Filter Random Forest (SOTA, OVO, Test set)
df_rf <- df_filtereds %>%
filter(Approach == "Sota",
Binarisation == "OVO",
Set == "Test",
Methods == "Random Forest") %>%
mutate(Method_Group = "Random Forest (OVO)")
# Step 3: Merge both datasets
df_comparaison <- bind_rows(df_predomics, df_rf)
# Step 4: Fit the linear mixed-effects model
model_comparaison <- lmer(Accuracy ~ Method_Group + (1 | Dataset), data = df_comparaison)
# Step 5: Display full summary of the model (REML, fixed and random effects)
summary(model_comparaison)
# Step 6: Extract fixed effects for plot
coefs <- tidy(model_comparaison, effects = "fixed")
# Reference: Intercept = Predomics
intercept <- coefs$estimate[coefs$term == "(Intercept)"]
std_err <- coefs$std.error[coefs$term == "(Intercept)"]
predomics <- tibble(
Condition = "Predomics",
Estimate = intercept,
Std_Error = std_err,
Lower = intercept - 1.96 * std_err,
Upper = intercept + 1.96 * std_err
)
# Random Forest (OVO)
rf_offset <- coefs$estimate[coefs$term == "Method_GroupRandom Forest (OVO)"]
rf_se <- coefs$std.error[coefs$term == "Method_GroupRandom Forest (OVO)"]
random_forest <- tibble(
Condition = "Random Forest",
Estimate = intercept + rf_offset,
Std_Error = sqrt(std_err^2 + rf_se^2),
Lower = (intercept + rf_offset) - 1.96 * sqrt(std_err^2 + rf_se^2),
Upper = (intercept + rf_offset) + 1.96 * sqrt(std_err^2 + rf_se^2)
)
# Step 7: Combine both results
df_plot6 <- bind_rows(predomics, random_forest)
# Step 8: Forest plot
ggplot(df_plot6, aes(x = Estimate, y = reorder(Condition, Estimate))) +
geom_point(size = 3) +
geom_errorbarh(aes(xmin = Lower, xmax = Upper), height = 0.2) +
geom_vline(xintercept = predomics$Estimate, linetype = "dashed", color = "gray") +
labs(
title = "Forest Plot: Predomics vs. Random Forest",
x = "Estimated Accuracy (95% Confidence Interval)",
y = "Model"
) +
theme_minimal()
# Add panel labels to each model dataframe
df_plot1$Panel <- "Model 1: Binarization & Approach"
df_plot2$Panel <- "Model 2: Structural Constraints"
df_plot3$Panel <- "Model 3: Aggregation Methods"
df_plot6$Panel <- "Model 6: Predomics vs. SOTA"
# Combine the four datasets into a single dataframe
df_all <- bind_rows(df_plot1, df_plot2, df_plot3, df_plot6)
# Create forest plots with facets
ggplot(df_all, aes(x = Estimate, y = reorder(Condition, Estimate))) +
geom_point(size = 3) +  # point estimate
geom_errorbarh(aes(xmin = Lower, xmax = Upper), height = 0.2) +  # confidence interval
geom_vline(xintercept = 0, linetype = "dashed", color = "gray") +  # vertical reference line
facet_wrap(~ Panel, scales = "free_y") +  # one facet per model
labs(
title = "Forest Plots of Estimated Accuracy by Modeling Strategy",
x = "Estimated Accuracy (95% CI)",
y = "Configuration"
) +
theme_minimal() +
theme(
strip.text = element_text(size = 12, face = "bold"),
axis.text.y = element_text(size = 9),
plot.title = element_text(hjust = 0.5)
)
devtools::build_vignettes()
rmarkdown::render("vignettes/Analyse_Resultat_Finale.Rmd")
devtools::build_vignettes()
callr::r(function() devtools::build_vignettes(), show = TRUE)
load("data/Analysis_Dataset_Complet_Sota_Predo.rda")
ls()
# Load the dataset containing benchmark results for Predomics and SOTA
data("Analysis_Dataset_Complet_Sota_Predo")
stopifnot(exists("Analysis_Dataset_Complet_Sota_Predo"))
library(mcpredomics)
devtools::build_vignettes()
devtools::check()
devtools::check()
devtools::document()
devtools::build_vignettes()
devtools::check(args = "--as-cran")
pkgfile <- devtools::build()
files  <- utils::untar(pkgfile, list=TRUE)
grep("tests/testthat/testdata/res_clf_mc_enterotype_Majority_Voting_with_Tie_Breaking_ovo\\.rda$",
files, value = TRUE)
library(mcpredomics)
devtools::test()
reo <- devtools::check()
reo$warnings
reo$notes
tools::resaveRdaFiles("data", compress = "xz")
library(mcpredomics)
resu <- devtools::check()
resu$errors
resu$warnings
resu$notes
library(mcpredomics)
devtools::check()
resul <- devtools::check()
resul$errors
resul$warnings
resul$notes
grep("library\\(|require\\(", list.files("R", "\\.[rR]$", full.names = TRUE), value = TRUE)
system("grep -R --line-number --ignore-case \"\\<library\\s*\\(\\s*pROC\\s*\\)\\|\\<require\\s*\\(\\s*pROC\\s*\\)\" .")
\\<library\\s*\\(\\s*RColorBrewer\\s*\\)\\|\\<require\\s*\\(\\s*RColorBrewer\\s*\\)\" .")
system("grep -R -n 'library(pROC)' .")
knitr::opts_chunk$set(echo = TRUE)
# no library(pROC)
# plus bas dans le code, utilise pROC::plot.roc, pROC::ci.auc, etc.
pROC::plot.roc(...)
library(mcpredomics)
fab = devtools::check(args = "--as-cran")
fab$errors
fab$warnings
fab$notes
library(mcpredomics)
devtools::check(args = "--as-cran")
library(mcpredomics)
