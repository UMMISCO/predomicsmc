# Création de la liste vide pour stocker les sous-modèles
list_mod <- list()
plot_sub_model <- list()
# Boucle pour remplir la liste de sous-modèles
for (i in 1:length(mod$names_)) {
# Initialiser une liste pour chaque sous-modèle
list_mod[[i]] <- list()
# Assigner les éléments de 'mod' aux sous-modèles
list_mod[[i]]$learner <- mod$learner
list_mod[[i]]$language <- mod$language
list_mod[[i]]$objective <- mod$objective
list_mod[[i]]$indices_ <- mod$indices_[[i]]
list_mod[[i]]$names_ <- mod$names_[[i]]
list_mod[[i]]$coeffs_ <- mod$coeffs_[[i]]
list_mod[[i]]$fit_ <- mod$fit_
list_mod[[i]]$unpenalized_fit_ <-mod$unpenalized_fit_
list_mod[[i]]$auc_ <- mod$auc_
list_mod[[i]]$accuracy_ <- mod$accuracy_
list_mod[[i]]$cor_ <- mod$cor_
list_mod[[i]]$aic_ <- mod$aic_
list_mod[[i]]$intercept_ <- mod$list_intercept_[[i]]
list_mod[[i]]$eval.sparsity <- mod$eval.sparsity
list_mod[[i]]$precision_ <- mod$precision_
list_mod[[i]]$recall_ <- mod$recall_
list_mod[[i]]$f1_ <- mod$f1_
list_mod[[i]]$sign_ <- mod$sign_[[i]]
list_mod[[i]]$rsq_ <- mod$rsq_[[i]]
list_mod[[i]]$ser_ <- mod$ser_[[i]]
list_mod[[i]]$score_ <- mod$score_[[i]]
list_mod[[i]]$mda.cv_ <- mod$mda.cv_[[i]]
list_mod[[i]]$prev.cv_ <- mod$prev.cv_[[i]]
list_mod[[i]]$mda_ <- mod$mda_[[i]]
}
nClasse <- unique(y)
list_y <- list()   # List for different combinations of y
list_X <- list()   # List for different combinations of X
if (approch == "ovo") {
k <- 1
for (i in 1:(length(nClasse) - 1)) {
for (j in (i + 1):length(nClasse)) {
class_i <- nClasse[i]
class_j <- nClasse[j]
indices <- which(y == class_i | y == class_j)
y_pair <- y[indices]
X_pair <- X[, indices]
list_y[[k]] <- as.vector(y_pair)
list_X[[k]] <- X_pair
k <- k + 1
}
}
} else {
for (i in 1:length(nClasse)) {
class_i <- nClasse[i]
y_temp <- ifelse(y == class_i, as.character(class_i), "All")
list_y[[i]] <- as.vector(y_temp)
list_X[[i]] <- X
}
}
for (i in 1:length(list_y)){
plot_sub_model[[i]] <- plotModel(mod = list_mod[[i]], X = list_X[[i]], y=list_y[[i]],
sort.features = sort.features,
feature.name = feature.name, importance = importance)
}
return(plot_sub_model)
}
#' Analyze the results from a list of classifiers
#'
#' @description Analyze the results from a list of classifiers.
#' @param scores: a list where each element is a vector of scores from a given models
#' @param y: the class to be predicted
#' @param main: title of the graph
#' @param ci: the point shape for the graph
#' @param percent: color for the graph
#' @return a list of roc objects
#' @export
plotAUC_mc <- function(scores, y, main = "", ci = TRUE, percent = TRUE, approch = "ovo") {
nClasse <- unique(y)
list_y <- list()
list_plo <- list()
list_scores <- list()
list_scores <- scores
if (approch == "ovo") {
k <- 1
for (i in 1:(length(nClasse) - 1)) {
for (j in (i + 1):length(nClasse)) {
class_i <- nClasse[i]
class_j <- nClasse[j]
# Select the indices of classes i and j
indices <- which(y == class_i | y == class_j)
y_pair <- y[indices]
# Fill with random values if necessary
if (length(y_pair) < length(scores[[k]])) {
missing_len <- length(scores[[k]]) - length(y_pair)
# Add random values from y and X
y_random <- sample(y_pair, missing_len, replace = TRUE)
# Combine existing data with random values
y_pair <- c(y_pair, y_random)
}
# Store results in lists
list_y[[k]] <- as.vector(y_pair)
k <- k + 1
}
}
} else {
for (i in 1:length(nClasse)) {
class_i <- nClasse[i]
# Create a new variable y_temp with "class_i" vs "All"
y_temp <- ifelse(y == class_i, as.character(class_i), "All")
# Fill with random values if necessary
if (length(y_temp) < length(scores[[i]])) {
missing_len <- length(scores[[i]]) - length(y_temp)
# Add random values from y and X
y_random <- sample(y_temp, missing_len, replace = TRUE)
# Combine existing data with random values
y_temp <- c(y_temp, y_random)
}
# Store in lists
list_y[[i]] <- as.vector(y_temp)
}
}
# Loop to generate plots for each class combination
for (i in 1:length(list_y)) {
list_plo[[i]] <- plotAUC(score = list_scores[[i]], y = list_y[[i]], percent = TRUE)
}
return(list_plo)
}
#' Plots the prevalence of a list of features in the whole dataset and per each class
#'
#' @description Plots the abundance of a given number of features for each class and tests significance
#' @import reshape2
#' @import ggplot2
#' @param features: a list of features or features indexes for which we wish to compute prevalence
#' @param X: dataset where to compute the prevalence
#' @param y: if provided it will also compute hte prevalence per each class (default:NULL)
#' @param topdown: showing features from top-down or the other way around (default:TRUE)
#' @param main: main title (default:none)
#' @param plot: if TRUE this provides a plot, otherwise will return different metrics such as prevalence and enrichment statistics
#' @param col.pt: colors for the point border (-1:deepskyblue4, 1:firebrick4)
#' @param col.bg: colors for the point fill (-1:deepskyblue1, 1:firebrick1)
#' @return a ggplot object
#' @export
plotAbundanceByClass_mc <- function(features, X, y, topdown = TRUE,
main = "", plot = TRUE,
col.pt = c("deepskyblue4", "firebrick4"),
col.bg = c("deepskyblue1", "firebrick1"), approch = "ovo")
{
nClasse <- unique(y)  # Récupère les classes uniques
list_y <- list()   # Liste pour les différentes combinaisons de y
list_X <- list()   # Liste pour les différentes combinaisons de X
list_plot <- list()  # Liste pour stocker les graphiques
# Si l'approche est one-vs-one (ovo)
if (approch == "ovo") {
k <- 1
for (i in 1:(length(nClasse) - 1)) {
for (j in (i + 1):length(nClasse)) {
class_i <- nClasse[i]
class_j <- nClasse[j]
indices <- which(y == class_i | y == class_j)  # Sélection des indices pour les deux classes
y_pair <- y[indices]  # Récupère les labels pour les deux classes
X_pair <- X[, indices]  # Récupère les données correspondantes
list_y[[k]] <- as.vector(y_pair)  # Ajoute le vecteur y dans la liste
list_X[[k]] <- X_pair  # Ajoute les données X dans la liste
k <- k + 1
}
}
} else {  # Pour les autres approches (e.g. one-vs-all)
for (i in 1:length(nClasse)) {
class_i <- nClasse[i]
y_temp <- ifelse(y == class_i, as.character(class_i), "All")  # Crée un vecteur one-vs-all
list_y[[i]] <- as.vector(y_temp)  # Ajoute ce vecteur à la liste
list_X[[i]] <- X  # Ajoute toutes les données X (elles ne changent pas pour one-vs-all)
}
}
# Générer les graphiques
for (i in 1:length(list_y)) {
list_plot[[i]] <- plotAbundanceByClass(
features = rownames(features[[i]]$pop.noz),
X = list_X[[i]],
y = list_y[[i]],
topdown = topdown,  # Transmet l'argument topdown
main = main,  # Transmet l'argument main
plot = plot,  # Transmet l'argument plot
col.pt = col.pt,  # Transmet les couleurs pour les points
col.bg = col.bg   # Transmet les couleurs pour le fond
)
}
return(list_plot)  # Retourne la liste des graphiques
}
#' Plots the prevalence of a list of features in the whole dataset and per each class
#'
#' @description Plots the prevalence of a given number of features
#' @import ggplot2
#' @param features: a list of features or features indexes for which we wish to compute prevalence
#' @param X: dataset where to compute the prevalence
#' @param y: if provided it will also compute hte prevalence per each class (default:NULL)
#' @param topdown: showing features from top-down or the other way around (default:TRUE)
#' @param main: main title (default:none)
#' @param plot: if TRUE this provides a plot, otherwise will return different metrics such as prevalence and enrichment statistics
#' @param col.pt: colors for the point border (-1:deepskyblue4, 1:firebrick4)
#' @param col.bg: colors for the point fill (-1:deepskyblue1, 1:firebrick1)
#' @param zero.value: the value that specifies what is zero. This can be a different than 0 in log transformed data for instance (default = 0)
#' @return a ggplot object
#' @export
plotPrevalence_mc <- function(features, X, y, topdown = TRUE, main = "", plot = TRUE,
col.pt = c("deepskyblue4", "firebrick4"),
col.bg = c("deepskyblue1", "firebrick1"),
zero.value = 0, approch="ovo")
{
nClasse <- unique(y)  # Récupère les classes uniques
list_y <- list()   # Liste pour les différentes combinaisons de y
list_X <- list()   # Liste pour les différentes combinaisons de X
list_plot <- list()  # Liste pour stocker les graphiques
# Si l'approche est one-vs-one (ovo)
if (approch == "ovo") {
k <- 1
for (i in 1:(length(nClasse) - 1)) {
for (j in (i + 1):length(nClasse)) {
class_i <- nClasse[i]
class_j <- nClasse[j]
indices <- which(y == class_i | y == class_j)  # Sélection des indices pour les deux classes
y_pair <- y[indices]  # Récupère les labels pour les deux classes
X_pair <- X[, indices]  # Récupère les données correspondantes
list_y[[k]] <- as.vector(y_pair)  # Ajoute le vecteur y dans la liste
list_X[[k]] <- X_pair  # Ajoute les données X dans la liste
k <- k + 1
}
}
} else {  # Pour les autres approches (e.g. one-vs-all)
for (i in 1:length(nClasse)) {
class_i <- nClasse[i]
y_temp <- ifelse(y == class_i, as.character(class_i), "All")  # Crée un vecteur one-vs-all
list_y[[i]] <- as.vector(y_temp)  # Ajoute ce vecteur à la liste
list_X[[i]] <- X  # Ajoute toutes les données X (elles ne changent pas pour one-vs-all)
}
}
# Générer les graphiques
for (i in 1:length(list_y)) {
list_plot[[i]] <- plotPrevalence(
features = rownames(features[[i]]$pop.noz),
X = list_X[[i]],
y = list_y[[i]],
topdown = topdown,  # Transmet l'argument topdown
main = main,  # Transmet l'argument main
plot = plot,  # Transmet l'argument plot
col.pt = col.pt,  # Transmet les couleurs pour les points
col.bg = col.bg,  # Transmet les couleurs pour le fond
zero.value = 0
)
}
return(list_plot)  # Retourne la liste des graphiques
}
fa <- makeFeatureAnnot_mc(pop = fbm,
X = X,
y = y,
clf = clf,
approch = "ova")
dim(fa[[1]]$pop.noz)
for (i in 1:length(fa)) {
g <- plotFeatureModelCoeffs(feat.model.coeffs = fa[[i]]$pop.noz)
print(g)
}
(g2 <- plotAbundanceByClass_mc(features = fa, X = X, y = y, approch = "ova"))
(g3 <- plotPrevalence_mc(features = fa, X = X, y = y , approch = "ova"))
library(mcpredomics)
knitr::opts_chunk$set(echo = TRUE)
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
library(ggpubr)
library(dplyr)
library(tidyr)
library(tibble)
library(knitr)
library(kableExtra)
library(DT)
library(e1071)
library(glmnet)
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Create an index vector for data partitioning
X_general <- X_general[rowSums(X_general)!=0,]; dim(X_general) # filter out variables with only zero values
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE); dim(X_general)
set.seed(42)
y = as.vector(yvec_trie)
X = X_general
# Number of desired samples in each class
nombre_echantillons_par_classe <- min(table(y))
# Function to balance the classes
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe,seed =123) {
classes <- unique(y)
indices_equilibres <- integer(0)
for (classe in classes) {
indices_classe <- which(y == classe)
set.seed(seed)
indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
}
return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X
# Verify the distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)
# Split yvec_trie into 80% train and 20% test
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[,indices_division]
X.test <- X_equilibre[,-indices_division]
table(y)
table(y.test)
dim(X)
dim(X.test)
clf <- terBeam_mc(sparsity = c(2,3,4),
max.nb.features = 1000,
seed = 1,
nCores = 1,
evalToFit = "accuracy_",
objective = "auc",
experiment.id = "terBeam_mc",
experiment.save = "nothing")
printy(clf)
runit = TRUE
if(runit)
{
res_clf <- fit_mc(X = X, y = y, clf = clf,approch="ova", cross.validate = TRUE,aggregation_ = "Predomics_aggregation_ova", nfolds= 10);
save(res_clf , clf, file ="res_clf.rda", compression_level = 9)
}
# ... Database X is not a matrix! Converting ...
# ... Classification mode, computing factor(y) for speedup and robustness
# ... Loading feature correlation for speedup
# ... Correlation file loaded
# ... Storing data in the classifier object for speedup
# ... Computing ternary coefficients for speedup
# ... One seed found, setting by default
# ... Running the classifier terga2 with a single CPU
# ... Second and faster version of terga fitting based on Genetic Algorithm heuristics ...
# ... Cross validation mode
# ... Starting cross validation not in parallel
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ... Learning process is finished succesfuly
# ... Thank you for using Predomics. Don't forget to digest the results now.
#
# [1] "experiment" "predomics"
res_clf.dig <- digestmc(obj = res_clf, penalty = 0.75/100, plot = TRUE)
# get the best model
best.model <- res_clf.dig$best$model
printy_mc(best.model)
plots1 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ova")
plots2 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE, approch = "ova")
# Extract the plots from each list and pass them to `grid.arrange
grid.arrange(grobs = c(plots1, plots2), ncol = 2)
clf <- regenerate_clf(clf, X, y, approch = "ova")
best.model.test <- evaluateModel_mc(
mod = best.model,
X = X.test,
y = y.test,
clf = clf,
eval.all = TRUE,
force.re.evaluation = TRUE,
approch = "ova",
aggregation_ = "Predomics_aggregation_ova",
mode = "test"
)
printy_mc(best.model.test)
tmp <- plotAUC_mc(best.model$score_, y, percent = TRUE, approch = "ova"); rm(tmp)
library(pROC)
library(ggplot2)
# Create ROC objects for the training set
roc_objects_train <- lapply(best.model$score_, function(score) {
roc(response = y, predictor = score)
})
# Create ROC objects for the test set
roc_objects_test <- lapply(best.model.test$score_, function(score) {
roc(response = y.test, predictor = score)
})
# Assign labels and combine ROC objects
roc_objects_train <- lapply(seq_along(roc_objects_train), function(i) {
roc_obj <- roc_objects_train[[i]]
roc_obj$dataset <- "Train"
roc_obj$submodel <- paste("Submodel", i)
return(roc_obj)
})
roc_objects_test <- lapply(seq_along(roc_objects_test), function(i) {
roc_obj <- roc_objects_test[[i]]
roc_obj$dataset <- "Test"
roc_obj$submodel <- paste("Submodel", i)
return(roc_obj)
})
# Combine ROC objects into a single list
all_roc_objects <- c(roc_objects_train, roc_objects_test)
# Convert to a format compatible with ggplot
roc_df <- do.call(rbind, lapply(all_roc_objects, function(roc_obj) {
data.frame(
FPR = 1 - roc_obj$specificities,  # False Positive Rate
TPR = roc_obj$sensitivities,       # True Positive Rate
Submodel = roc_obj$submodel,
Dataset = roc_obj$dataset
)
}))
# Plot ROC curves using ggplot
ggplot(roc_df, aes(x = FPR, y = TPR, color = Dataset)) +
geom_line() +
facet_wrap(~ Submodel, scales = "free", labeller = label_both) +
labs(title = "ROC Curves", x = "False Positive Rate", y = "True Positive Rate") +
theme_minimal() +
theme(legend.position = "bottom")
# Convert the model collection into a population of models scrambled by model size
pop <- modelCollectionToPopulation(res_clf$classifier$models)
printy_mc(pop)
# Convert the population to a data frame
pop.df <- populationToDataFrame_mc(pop)
# Plotting for the original population
g.before <- list()  # Initialize a list to store the plots
for(i in 1: length(pop.df)) {
pop.dff  <- as.data.frame(pop.df[[i]])  # Convert each submodel to a data frame
# Display the head of the dataframe excluding some columns
head(pop.dff[,-c(3, 4, 7, 8, 14)])
# Melt the dataframe for ggplot
pop.df.melt <- melt(pop.dff, id.vars = c("accuracy_", "eval.sparsity"))
# Create the ggplot for the original population
g.before[[i]] <- ggplot(data = pop.df.melt, aes(y = accuracy_, x = eval.sparsity)) +
geom_boxplot(notch = FALSE, outlier.shape = NA, position = position_dodge(width=0.9), alpha = 0.3) +
geom_point(aes(color = eval.sparsity), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
ylim(c(0,1)) +
xlab("Model parsimony") +
ggtitle("Original population") +
theme_bw() +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour="none")
}
# Select the best population models
fbm <- selectBestPopulation(pop)
printy_mc(fbm)
# Convert the best population models to a data frame
fbm.df <- populationToDataFrame_mc(fbm)
# Plotting for the selected best models
g.after <- list()  # Initialize a list to store the plots
for(j in 1:length(fbm.df)) {
fbm.dff  <- as.data.frame(fbm.df[[j]])  # Convert each submodel to a data frame
# Melt the dataframe for ggplot
fbm.df.melt <- melt(fbm.dff, id.vars = c("accuracy_", "eval.sparsity"))
# Create the ggplot for the best population models
g.after[[j]] <- ggplot(data = fbm.df.melt, aes(y = accuracy_, x = eval.sparsity)) +
geom_boxplot(notch = FALSE, position = position_dodge(width=0.9), alpha = 0.3) +
geom_point(aes(color = eval.sparsity), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
ylim(c(0,1)) +
xlab("Model parsimony") +
ggtitle("FBM") +
theme_bw() +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour="none")
}
# Arrange the plots side by side
grid.arrange(g.before[[1]], g.after[[1]], ncol = 2)
fa <- makeFeatureAnnot_mc(pop = fbm,
X = X,
y = y,
clf = clf,
approch = "ova")
dim(fa[[1]]$pop.noz)
for (i in 1:length(fa)) {
g <- plotFeatureModelCoeffs(feat.model.coeffs = fa[[i]]$pop.noz)
print(g)
}
(g2 <- plotAbundanceByClass_mc(features = fa, X = X, y = y, approch = "ova"))
(g3 <- plotPrevalence_mc(features = fa, X = X, y = y , approch = "ova"))
library(mcpredomics)
(g3 <- plotPrevalence_mc(features = fa, X = X, y = y , approch = "ova"))
knitr::opts_chunk$set(echo = TRUE)
gridExtra::grid.arrange(grobs = g3, ncol = 2)  # Ici, ncol spécifie 2 colonnes
gridExtra::grid.arrange(grobs = g3, ncol = 2)  # Ici, ncol spécifie 2 colonnes
gridExtra::grid.arrange(grobs = g2, ncol = 2)  # Ici, ncol spécifie 2 colonnes
gridExtra::grid.arrange(grobs = g2, ncol = 2)  # Ici, ncol spécifie 2 colonnes
fa <- makeFeatureAnnot_mc(pop = fbm,
X = X,
y = y,
clf = clf,
approch = "ova")
g <- list()
dim(fa[[1]]$pop.noz)
for (i in 1:length(fa)) {
g[[i]] <- plotFeatureModelCoeffs(feat.model.coeffs = fa[[i]]$pop.noz)
print(g)
}
(g2 <- plotAbundanceByClass_mc(features = fa, X = X, y = y, approch = "ova"))
(g3 <- plotPrevalence_mc(features = fa, X = X, y = y , approch = "ova"))
gridExtra::grid.arrange(grobs = g, ncol = 2)  # Ici, ncol spécifie 2 colonnes
gridExtra::grid.arrange(grobs = g, ncol = 2)  # Ici, ncol spécifie 2 colonnes
grid.arrange(grobs = plots1, ncol = 2)
grid.arrange(grobs = plots2, ncol = 2)
