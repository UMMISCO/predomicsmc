all_metrics <- rbind(train_metrics, test_metrics)
# Create the graph
plot <- ggplot(all_metrics, aes(x = Metric, y = Value, fill = Dataset)) +
geom_bar(stat = "identity", position = "dodge") +
geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
labs(title = "Comparison of test and training metrics Sota RF",
x = "Metrics", y = "Values") +
theme_minimal()
# Display the graph
print(plot)
library(nnet)
# Create a data.frame from matrix X
library(glmnet)
# Utilize cv.glmnet with 10-fold cross-validation
df <- as.data.frame(t(X))
df$y <- factor(y)
cv_model_rl <- cv.glmnet(as.matrix(df[, -ncol(df)]), df$y, family = "multinomial", alpha = 1, nfolds = 10)
# Get the best model based on cross-validation
best_model_rl <- cv_model_rl$glmnet.fit
# Obtain predictions directly from cross-validation
predictions_rl <- predict(cv_model_rl, newx = as.matrix(df[, -ncol(df)]), s = "lambda.min", type = "class")
predictions_rl <- factor(predictions_rl)
# Check the dimensions and create the confusion matrix
if(length(predictions_rl) == length(df$y)) {
confusionMatrix(predictions_rl, df$y)
} else {
print("The dimensions of the predictions do not match those of the ground truth")
}
conf_matrix_rl <- confusionMatrix(predictions_rl, df$y)
# Calcul de l'exactitude
accuracy_train_rl <- conf_matrix_rl$overall[["Accuracy"]]
# Calcul de la précision moyenne de chaque classe
precision_train_rl <- mean(conf_matrix_rl$byClass[ , "Precision"])
# Calcul de l'AUC en utilisant la précision équilibrée moyenne (Balanced Accuracy)
AUC_train_rl <- mean(conf_matrix_rl$byClass[ , "Balanced Accuracy"])
# Calcul du rappel moyen (Sensibilité) de chaque classe
recall_train_rl <- mean(conf_matrix_rl$byClass[ , "Sensitivity"])
# Calcul du score F1 pour les données d'entraînement
F1_train_rl <- 2 * (precision_train_rl * recall_train_rl) / (precision_train_rl + recall_train_rl)
# Affichage des métriques pour les données d'entraînement
print(paste("Train Accuracy:", accuracy_train_rl))
print(paste("Train Precision:", precision_train_rl))
print(paste("Train Recall (Sensitivity):", recall_train_rl))
print(paste("Train AUC (Balanced Accuracy):", AUC_train_rl))
print(paste("Train F1 Score:", F1_train_rl))
predictions_test1_rl <- predict(cv_model_rl, newx = as.matrix(X_test), type = "class")
library(caret)
predictions_test1_rl <- as.factor(predictions_test1_rl)
# Create the confusion matrix for predictions on the test dataset
conf_matrix_test_rl <- confusionMatrix(predictions_test1_rl, y_test)
# Calculate accuracy, precision, AUC, and recall for the test dataset
# Calculate accuracy, precision, AUC, and recall for the test dataset
accuracy_test1_rl <- conf_matrix_test_rl$overall[["Accuracy"]]
precision_test1_rl <- mean(conf_matrix_test_rl$byClass[ , "Precision"], na.rm = TRUE)
AUC_test1_rl <- mean(conf_matrix_test_rl$byClass[ , "Balanced Accuracy"], na.rm = TRUE)
recall_test1_rl <- mean(conf_matrix_test_rl$byClass[ ,"Sensitivity"], na.rm = TRUE)
# Calcul du score F1 pour les données d'entraînement
F1_test1_rl <- 2 * (precision_test1_rl * recall_test1_rl) / (precision_test1_rl + recall_test1_rl)
library(ggplot2)
# Create a data frame for the training metrics
train_metrics_rl <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_train_rl, precision_train_rl, F1_train_rl, recall_train_rl),
Dataset = "Training"
)
# Create a data frame for the test metrics
test_metrics_rl <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_test1_rl, precision_test1_rl, F1_test1_rl, recall_test1_rl),
Dataset = "Test"
)
# Combine the training and test metrics
all_metrics_rl <- rbind(test_metrics_rl,train_metrics_rl)
# Create the graph
plot <- ggplot(all_metrics_rl, aes(x = Metric, y = Value, fill = Dataset)) +
geom_bar(stat = "identity", position = "dodge") +
geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
labs(title = "Comparison of test and training metrics Sota LR",
x = "Metrics", y = "Values") +
theme_minimal()
# Display the graph
print(plot)
library(caret)
library(pROC)
# Train an SVM model with radial kernel and 10-fold cross-validation
svm_model <- train(x = X_train, y = y_train, method = "svmRadial", trControl = trainControl(method = "cv", number = 10))
# Make predictions with the SVM model
svm_predictions_train <- predict(svm_model, X_train)
svm_predictions_test <- predict(svm_model, X_test)
# Calculate the accuracy for the training and test sets
accuracy_train_svm <- confusionMatrix(data = svm_predictions_train, reference = y_train)$overall['Accuracy'][1]
accuracy_test_svm <- confusionMatrix(data = svm_predictions_test, reference = y_test)$overall['Accuracy'][1]
# Calculate precision, recall, and AUC for the training and test sets
precision_train_svm <- confusionMatrix(data = svm_predictions_train, reference = y_train)$byClass[, 'Pos Pred Value']
precision_test_svm <- confusionMatrix(data = svm_predictions_test, reference = y_test)$byClass[, 'Pos Pred Value']
recall_train_svm <- confusionMatrix(data = svm_predictions_train, reference = y_train)$byClass[, 'Sensitivity']
recall_test_svm <- confusionMatrix(data = svm_predictions_test, reference = y_test)$byClass[, 'Sensitivity']
F1_train_svm <- 2 * (precision_train_svm * recall_train_svm) / (precision_train_svm + recall_train_svm)
F1_train = mean(F1_train_svm)
auc_train_svm <- roc(y_train, as.numeric(svm_predictions_train))$auc
auc_test_svm <- roc(y_test, as.numeric(svm_predictions_test))$auc
precision_train_svm = mean(precision_train_svm)
precision_test_svm = mean(precision_test_svm)
recall_train_svm = mean(recall_train_svm)
recall_test_svm = mean(recall_test_svm)
F1_test_svm <- 2 * (precision_test_svm * recall_test_svm) / (precision_test_svm + recall_test_svm)
F1_test_svm <- mean(F1_test_svm)
library(ggplot2)
# Create a data frame for the training metrics
train_metrics_svm <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_train_svm, precision_train_svm, F1_train, recall_train_svm),
Dataset = "Training"
)
# Create a data frame for the test metrics
test_metrics_svm <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_test_svm, precision_test_svm, F1_test_svm, recall_test_svm),
Dataset = "Test"
)
# Combine the training and test metrics
all_metrics_svm <- rbind(test_metrics_svm,train_metrics_svm)
# Create the graph
plot <- ggplot(all_metrics_svm, aes(x = Metric, y = Value, fill = Dataset)) +
geom_bar(stat = "identity", position = "dodge") +
geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
labs(title = "Comparison of test and training metrics Sota SVM",
x = "Metrics", y = "Values") +
theme_minimal()
# Display the graph
print(plot)
load("res_clf_ovo.rda")
bests_models_parsimony_ovo <- digestmc(obj = res_clf_ovo, penalty = 0.75/100, plot = TRUE)
pop = res_clf_ovo$classifier$models
Population_voting_Aggregation <- evaluatePopulation_aggregation(pop = pop, y = y.test, X = X.test, force.re.evaluation = TRUE, clf,  aggregation = "votingAggregation")
extract_metric_values <- function(aggregation_list, metric_name) {
values <- c()
for (i in seq_along(aggregation_list)) {
values <- c(values, aggregation_list[[i]][[1]][[metric_name]])
}
return(values)
}
data <- data.frame(
Metric = rep(c("Accuracy", "Precision", "Recall", "F1"), each = 4),
Value = c(
extract_metric_values(Population_voting_Aggregation, "accuracy_") * 100,
extract_metric_values(Population_voting_Aggregation, "precision_") * 100,
extract_metric_values(Population_voting_Aggregation, "recall_") * 100,
extract_metric_values(Population_voting_Aggregation, "f1_") * 100
),
Method = rep(c("K_2", "K_3", "K_4", "K_5"), times = 4)
)
colors <- c("Accuracy" = "#1f77b4", "Precision" = "#ff7f0e", "Recall" = "#2ca02c", "F1" = "#d62728")
ggplot(data, aes(x = Method, y = Value, fill = Metric)) +
geom_boxplot(color = "black", outlier.color = "black") +
scale_fill_manual(values = colors) +  # Utiliser la palette de couleurs définie
facet_wrap(~Metric, scales = "free_y") +
labs(x = "Metrics", y = "Value", title = "Metrics by sparsity") +
ylim(0, 100) +  # Définir la limite de l'axe y de 0 à 100
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA),
legend.title = element_blank(),
legend.position = "bottom")
best_model_voting_aggregation = Population_voting_Aggregation[[4]][[1]]
best_model_voting_aggregation$accuracy_
best_model_voting_aggregation$confusionMatrix_
best_model_voting_aggregation$coeffs_
best_model_voting_aggregation$predictions_aggre
pop = res_clf_ovo$classifier$models
Population_weighted_Aggregation <- evaluatePopulation_aggregation(pop = pop, y = y.test, X = X.test, force.re.evaluation = TRUE, clf,  aggregation = "weightedAggregation")
data <- data.frame(
Metric = rep(c("Accuracy", "Precision", "Recall", "F1"), each = 4),
Value = c(
extract_metric_values(Population_weighted_Aggregation, "accuracy_") * 100,
extract_metric_values(Population_weighted_Aggregation, "precision_") * 100,
extract_metric_values(Population_weighted_Aggregation, "recall_") * 100,
extract_metric_values(Population_weighted_Aggregation, "f1_") * 100
),
Method = rep(c("K_2", "K_3", "K_4", "K_5"), times = 4)
)
colors <- c("Accuracy" = "#1f77b4", "Precision" = "#ff7f0e", "Recall" = "#2ca02c", "F1" = "#d62728")
ggplot(data, aes(x = Method, y = Value, fill = Metric)) +
geom_boxplot(color = "black", outlier.color = "black") +
scale_fill_manual(values = colors) +  # Utiliser la palette de couleurs définie
facet_wrap(~Metric, scales = "free_y") +
labs(x = "Metrics", y = "Value", title = "Metrics by sparsity") +
ylim(0, 100) +  # Définir la limite de l'axe y de 0 à 100
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA),
legend.title = element_blank(),
legend.position = "bottom")
best_model_weighted_aggregation = Population_weighted_Aggregation[[4]][[1]]
best_model_weighted_aggregation$accuracy_
best_model_weighted_aggregation$confusionMatrix_
best_model_weighted_aggregation$coeffs_
best_model_weighted_aggregation$predictions_aggre
load("res_clf_ova.rda")
bests_models_parsimony_ova <- digestmc(obj = res_clf_ova, penalty = 0.75/100, plot = TRUE)
pop = res_clf_ova$classifier$models
Population_newApproach_Aggregation <- evaluatePopulation_aggregation(pop = pop, y = y.test, X = X.test, force.re.evaluation = TRUE, clf,  aggregation = "NewApproach")
data <- data.frame(
Metric = rep(c("Accuracy", "Precision", "Recall", "F1"), each = 4),
Value = c(
extract_metric_values(Population_newApproach_Aggregation, "accuracy_") * 100,
extract_metric_values(Population_newApproach_Aggregation, "precision_") * 100,
extract_metric_values(Population_newApproach_Aggregation, "recall_") * 100,
extract_metric_values(Population_newApproach_Aggregation, "f1_") * 100
),
Method = rep(c("K_2", "K_3", "K_4", "K_5"), times = 4)
)
colors <- c("Accuracy" = "#1f77b4", "Precision" = "#ff7f0e", "Recall" = "#2ca02c", "F1" = "#d62728")
ggplot(data, aes(x = Method, y = Value, fill = Metric)) +
geom_boxplot(color = "black", outlier.color = "black") +
scale_fill_manual(values = colors) +  # Utiliser la palette de couleurs définie
facet_wrap(~Metric, scales = "free_y") +
labs(x = "Metrics", y = "Value", title = "Metrics by sparsity") +
ylim(0, 100) +  # Définir la limite de l'axe y de 0 à 100
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA),
legend.title = element_blank(),
legend.position = "bottom")
best_model_newapproach_aggregation = Population_newApproach_Aggregation[[3]][[1]]
best_model_newapproach_aggregation$accuracy_
best_model_newapproach_aggregation$confusionMatrix_
best_model_newapproach_aggregation$coeffs_
best_model_newapproach_aggregation$predictions_aggre
pop = res_clf_ova$classifier$models
Population_maximization_Aggregation <- evaluatePopulation_aggregation(pop = pop, y = y.test, X = X.test, force.re.evaluation = TRUE, clf,  aggregation = "maximizationAggregation")
data <- data.frame(
Metric = rep(c("Accuracy", "Precision", "Recall", "F1"), each = 4),
Value = c(
extract_metric_values(Population_maximization_Aggregation, "accuracy_") * 100,
extract_metric_values(Population_maximization_Aggregation, "precision_") * 100,
extract_metric_values(Population_maximization_Aggregation, "recall_") * 100,
extract_metric_values(Population_maximization_Aggregation, "f1_") * 100
),
Method = rep(c("K_2", "K_3", "K_4", "K_5"), times = 4)
)
colors <- c("Accuracy" = "#1f77b4", "Precision" = "#ff7f0e", "Recall" = "#2ca02c", "F1" = "#d62728")
ggplot(data, aes(x = Method, y = Value, fill = Metric)) +
geom_boxplot(color = "black", outlier.color = "black") +
scale_fill_manual(values = colors) +  # Utiliser la palette de couleurs définie
facet_wrap(~Metric, scales = "free_y") +
labs(x = "Metrics", y = "Value", title = "Metrics by sparsity") +
ylim(0, 100) +  # Définir la limite de l'axe y de 0 à 100
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA),
legend.title = element_blank(),
legend.position = "bottom")
best_model_maximization_aggregation = Population_maximization_Aggregation[[3]][[1]]
best_model_maximization_aggregation$accuracy_
best_model_maximization_aggregation$confusionMatrix_
best_model_maximization_aggregation$coeffs_
best_model_maximization_aggregation$predictions_aggre
pop = res_clf_ova$classifier$models
Population_ranking_Aggregation <- evaluatePopulation_aggregation(pop = pop, y = y.test, X = X.test, force.re.evaluation = TRUE, clf,  aggregation = "rankingAggregation")
data <- data.frame(
Metric = rep(c("Accuracy", "Precision", "Recall", "F1"), each = 4),
Value = c(
extract_metric_values(Population_ranking_Aggregation, "accuracy_") * 100,
extract_metric_values(Population_ranking_Aggregation, "precision_") * 100,
extract_metric_values(Population_ranking_Aggregation, "recall_") * 100,
extract_metric_values(Population_ranking_Aggregation, "f1_") * 100
),
Method = rep(c("K_2", "K_3", "K_4", "K_5"), times = 4)
)
colors <- c("Accuracy" = "#1f77b4", "Precision" = "#ff7f0e", "Recall" = "#2ca02c", "F1" = "#d62728")
ggplot(data, aes(x = Method, y = Value, fill = Metric)) +
geom_boxplot(color = "black", outlier.color = "black") +
scale_fill_manual(values = colors) +  # Utiliser la palette de couleurs définie
facet_wrap(~Metric, scales = "free_y") +
labs(x = "Metrics", y = "Value", title = "Metrics by sparsity") +
ylim(0, 100) +  # Définir la limite de l'axe y de 0 à 100
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA),
legend.title = element_blank(),
legend.position = "bottom")
best_model_newapproach_aggregation = Population_newApproach_Aggregation[[4]][[1]]
best_model_newapproach_aggregation$accuracy_
best_model_newapproach_aggregation$confusionMatrix_
best_model_newapproach_aggregation$coeffs_
best_model_newapproach_aggregation$predictions_aggre
best_model_newapproach_aggregation$precision_
best_model_newapproach_aggregation$f1_
best_model_newapproach_aggregation$recall_
pop = res_clf_ova$classifier$models
Population_maximization_Aggregation <- evaluatePopulation_aggregation(pop = pop, y = y.test, X = X.test, force.re.evaluation = TRUE, clf,  aggregation = "maximizationAggregation")
data <- data.frame(
Metric = rep(c("Accuracy", "Precision", "Recall", "F1"), each = 4),
Value = c(
extract_metric_values(Population_maximization_Aggregation, "accuracy_") * 100,
extract_metric_values(Population_maximization_Aggregation, "precision_") * 100,
extract_metric_values(Population_maximization_Aggregation, "recall_") * 100,
extract_metric_values(Population_maximization_Aggregation, "f1_") * 100
),
Method = rep(c("K_2", "K_3", "K_4", "K_5"), times = 4)
)
colors <- c("Accuracy" = "#1f77b4", "Precision" = "#ff7f0e", "Recall" = "#2ca02c", "F1" = "#d62728")
ggplot(data, aes(x = Method, y = Value, fill = Metric)) +
geom_boxplot(color = "black", outlier.color = "black") +
scale_fill_manual(values = colors) +  # Utiliser la palette de couleurs définie
facet_wrap(~Metric, scales = "free_y") +
labs(x = "Metrics", y = "Value", title = "Metrics by sparsity") +
ylim(0, 100) +  # Définir la limite de l'axe y de 0 à 100
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA),
legend.title = element_blank(),
legend.position = "bottom")
best_model_maximization_aggregation = Population_maximization_Aggregation[[4]][[1]]
best_model_maximization_aggregation$accuracy_
best_model_maximization_aggregation$confusionMatrix_
best_model_maximization_aggregation$coeffs_
best_model_maximization_aggregation$predictions_aggre
best_model_maximization_aggregation$precision_
best_model_maximization_aggregation$recall_
best_model_maximization_aggregation$f1_
pop = res_clf_ova$classifier$models
Population_ranking_Aggregation <- evaluatePopulation_aggregation(pop = pop, y = y.test, X = X.test, force.re.evaluation = TRUE, clf,  aggregation = "rankingAggregation")
data <- data.frame(
Metric = rep(c("Accuracy", "Precision", "Recall", "F1"), each = 4),
Value = c(
extract_metric_values(Population_ranking_Aggregation, "accuracy_") * 100,
extract_metric_values(Population_ranking_Aggregation, "precision_") * 100,
extract_metric_values(Population_ranking_Aggregation, "recall_") * 100,
extract_metric_values(Population_ranking_Aggregation, "f1_") * 100
),
Method = rep(c("K_2", "K_3", "K_4", "K_5"), times = 4)
)
colors <- c("Accuracy" = "#1f77b4", "Precision" = "#ff7f0e", "Recall" = "#2ca02c", "F1" = "#d62728")
ggplot(data, aes(x = Method, y = Value, fill = Metric)) +
geom_boxplot(color = "black", outlier.color = "black") +
scale_fill_manual(values = colors) +  # Utiliser la palette de couleurs définie
facet_wrap(~Metric, scales = "free_y") +
labs(x = "Metrics", y = "Value", title = "Metrics by sparsity") +
ylim(0, 100) +  # Définir la limite de l'axe y de 0 à 100
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA),
legend.title = element_blank(),
legend.position = "bottom")
best_model_ranking_aggregation = Population_ranking_Aggregation[[4]][[1]]
best_model_ranking_aggregation$accuracy_
best_model_ranking_aggregation$confusionMatrix_
best_model_ranking_aggregation$coeffs_
best_model_ranking_aggregation$predictions_aggre
accuracy_val = best_model_voting_aggregation$accuracy_
precision_val = best_model_voting_aggregation$precision_
recall_val = best_model_voting_aggregation$recall_
f1_val <- best_model_voting_aggregation$f1_
# Création du dataframe sans la matrice de confusion
df_voting <- data.frame(
Algorithme = "Voting Aggregation",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_voting
accuracy_val = best_model_weighted_aggregation$accuracy_
precision_val = best_model_weighted_aggregation$precision_
recall_val = best_model_weighted_aggregation$recall_
f1_val <- best_model_weighted_aggregation$f1_
# Création du dataframe sans la matrice de confusion
df_weighted <- data.frame(
Algorithme = "weighted Aggregation",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_weighted
accuracy_val = best_model_newapproach_aggregation$accuracy_
precision_val = best_model_newapproach_aggregation$precision_
recall_val = best_model_newapproach_aggregation$recall_
f1_val <- best_model_newapproach_aggregation$f1_
# Création du dataframe sans la matrice de confusion
df_newapproach <- data.frame(
Algorithme = "New Approach",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_newapproach
accuracy_val = best_model_maximization_aggregation$accuracy_
precision_val = best_model_maximization_aggregation$precision_
recall_val = best_model_maximization_aggregation$recall_
f1_val <- best_model_maximization_aggregation$f1_
# Création du dataframe sans la matrice de confusion
df_maximization <- data.frame(
Algorithme = "maximization aggregation",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_maximization
accuracy_val = best_model_ranking_aggregation$accuracy_
precision_val =best_model_ranking_aggregation$precision_
recall_val = best_model_ranking_aggregation$recall_
f1_val <-best_model_ranking_aggregation$f1_
# Création du dataframe sans la matrice de confusion
df_ranking <- data.frame(
Algorithme = "ranking aggregation",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_ranking
accuracy_val <-  test_accuracy
precision_val <- test_precision
recall_val <- test_recall
f1_val <- f1_test
# Création du dataframe sans la matrice de confusion
df_rf <- data.frame(
Algorithme = "Sota_RF",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_rf
accuracy_val <-  accuracy_test1_rl
precision_val <- precision_test1_rl
recall_val <- recall_test1_rl
f1_val <- F1_test1_rl
# Création du dataframe sans la matrice de confusion
df_LR <- data.frame(
Algorithme = "Sota_LR",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_LR
accuracy_val <-  accuracy_test_svm
precision_val <- precision_test_svm
recall_val <- recall_test_svm
f1_val <- F1_test_svm
# Création du dataframe sans la matrice de confusion
df_SVM <- data.frame(
Algorithme = "Sota_SVM",
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = "Test",
Values = c(accuracy_val, precision_val, recall_val, f1_val)
)
df_SVM
# Combinez les dataframes en un seul dataframe
df_all <- rbind(df_ranking, df_maximization,df_newapproach, df_voting,df_weighted, df_rf,df_LR, df_SVM)
# Créer une liste des métriques à comparer
metrics_list <- c("Accuracy", "Precision", "Recall", "F1")
# Parcourir chaque métrique pour créer un plot unique
for (metric in metrics_list) {
# Filtrer les données par métrique
df_metric <- df_all[df_all$Metrics == metric, ]
# Créer le plot
plot <- ggplot(df_metric, aes(x = Algorithme, y = Values, fill = Algorithme)) +
geom_bar(stat = "identity") +
labs(title = paste("Comparison of", metric, "between the models"), y = metric, x = "Algorithms") +
theme_minimal() +
geom_text(aes(label = round(Values, 2), y = Values), vjust = -0.3, size = 3)
# Afficher le plot
print(plot)
}
library(ggplot2)
# Charger les données
df_voting <- data.frame(
Algorithme = c("Voting Aggregation", "Voting Aggregation", "Voting Aggregation", "Voting Aggregation"),
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = c("Test", "Test", "Test", "Test"),
Values = c(0.704, 0.7111, 0.704, 0.7078)
)
# Créer le graphique en nuage de points avec des limites spécifiques
ggplot(df_voting, aes(x = Metrics, y = Values, color = Metrics)) +
geom_point(size = 3) +
labs(title = "Performance Metrics Best Model for Ranking Aggregation",
x = "Metrics", y = "Values") +
theme_minimal() +
theme(legend.position = "none") +
geom_text(aes(label = round(Values, 3)), vjust = -0.5) +
scale_y_continuous(limits = c(0, 1)) +  # Ajuster l'échelle de l'axe y
scale_x_discrete()  # Utiliser une échelle discrète pour l'axe x
library(ggplot2)
# Charger les données
df_voting <- data.frame(
Algorithme = c("Voting Aggregation", "Voting Aggregation", "Voting Aggregation", "Voting Aggregation"),
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = c("Test", "Test", "Test", "Test"),
Values = c(0.704, 0.7111, 0.704, 0.7078)
)
# Créer le graphique en nuage de points avec des limites spécifiques
ggplot(df_voting, aes(x = Metrics, y = Values, color = Metrics)) +
geom_point(size = 3) +
labs(title = "Performance Metrics Best Model for Maximization Aggregation",
x = "Metrics", y = "Values") +
theme_minimal() +
theme(legend.position = "none") +
geom_text(aes(label = round(Values, 3)), vjust = -0.5) +
scale_y_continuous(limits = c(0, 1)) +  # Ajuster l'échelle de l'axe y
scale_x_discrete()  # Utiliser une échelle discrète pour l'axe x
library(ggplot2)
# Charger les données
df_voting <- data.frame(
Algorithme = c("Voting Aggregation", "Voting Aggregation", "Voting Aggregation", "Voting Aggregation"),
Metrics = c("Accuracy", "Precision", "Recall", "F1"),
Dataset = c("Test", "Test", "Test", "Test"),
Values = c(0.6969, 0.7032, 0.6969, 0.7001)
)
# Créer le graphique en nuage de points avec des limites spécifiques
ggplot(df_voting, aes(x = Metrics, y = Values, color = Metrics)) +
geom_point(size = 3) +
labs(title = "Performance Metrics Best Model for New Approach Aggregation",
x = "Metrics", y = "Values") +
theme_minimal() +
theme(legend.position = "none") +
geom_text(aes(label = round(Values, 3)), vjust = -0.5) +
scale_y_continuous(limits = c(0, 1)) +  # Ajuster l'échelle de l'axe y
scale_x_discrete()  # Utiliser une échelle discrète pour l'axe x
library(mcpredomics)
