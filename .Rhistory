print(g.after)  # Display the plot
fa <- makeFeatureAnnot_mc(pop = fbm,
X = X,
y = y,
clf = clf,
approch = "ovo")
plot_distribution <- plotFeatureModelCoeffs_mc(feat.model.coeffs = fa, y, approch = "ovo")
plots_abundance <- plotAbundanceByClass_mc(features = fa, X, y, approch = "ovo")
plots_prevalence <- plotPrevalence_mc(features = fa, X = X, y = y, approch = "ovo")
res_clf.dig <- digestmc(obj = terbeam_voting_unconstrained, penalty = 0.75/100, plot = TRUE)
clf <- regenerate_clf(clf, X, y, approch = "ovo")
# get the best model
best.model = res_clf.dig$best$model
# Visualize the model information (if needed)
printy_mc(best.model)
# Generate the plots
plots1 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ova")
plots2 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE, approch = "ova")
# Open a PDF device to save the grid of plots
pdf("model_visualization_plots.pdf", width = 12, height = 8)  # Adjust width and height as needed
# Extract the plots from each list and pass them to `grid.arrange`
grid.arrange(grobs = c(plots1, plots2), ncol = 4)
# Close the PDF device
dev.off()
# Confirmation of the save process
cat("The plots have been saved under the name 'model_visualization_plots.pdf'.\n")
tmp <- plotAUC_mc(scores = best.model$score_, y = y, percent = TRUE, approch = "ova"); rm(tmp)
tmp <- plotAUC_mc(scores = best.model$score_, y = y, percent = TRUE, approch = "ovo"); rm(tmp)
# Convert the model collection into a population of models scrambled by model size
pop <- modelCollectionToPopulation(terbeam_voting_unconstrained$classifier$models)
printy_mc(pop)
# Function to create boxplot for a given data frame
create_boxplot <- function(data, title) {
# Melt the dataframe for ggplot
data.melt <- melt(data, id.vars = c("accuracy_", "eval.sparsity"))
# Create ggplot
plot <- ggplot(data = data.melt, aes(y = accuracy_, x = eval.sparsity)) +
geom_boxplot(notch = FALSE, outlier.shape = NA, position = position_dodge(width = 0.9), alpha = 0.3) +
geom_point(aes(color = eval.sparsity), position = position_jitterdodge(dodge.width = 0.9), size = 1, alpha = 0.5) +
ylim(c(0, 1)) +
xlab("Model Parsimony") +
ggtitle(title) +
theme_bw() +
theme(legend.position = "bottom", legend.direction = "horizontal") +
guides(colour = "none")
return(plot)
}
# Convert the population to a data frame
pop.df <- populationToDataFrame_mc(pop)
# Plotting for the original population (single figure)
pop.dff <- as.data.frame(pop.df[[1]])  # Convert the first submodel to a data frame
g.before <- create_boxplot(pop.dff, title = "Original Population")
#print(g.before)  # Display the plot
# Select the best population models
fbm <- selectBestPopulation(pop)
#printy_mc(fbm)
# Convert the best population models to a data frame
fbm.df <- populationToDataFrame_mc(fbm)
# Plotting for the selected best models (single figure)
fbm.dff <- as.data.frame(fbm.df[[1]])  # Convert the first submodel to a data frame
g.after <- create_boxplot(fbm.dff, title = "FBM")
print(g.after)  # Display the plot
fa <- makeFeatureAnnot_mc(pop = fbm,
X = X,
y = y,
clf = clf,
approch = "ovo")
plot_distribution <- plotFeatureModelCoeffs_mc(feat.model.coeffs = fa, y, approch = "ovo")
plots_abundance <- plotAbundanceByClass_mc(features = fa, X, y, approch = "ovo")
plots_prevalence <- plotPrevalence_mc(features = fa, X = X, y = y, approch = "ovo")
feat1.import <- mergeMeltImportanceCV_mc(list.results = list(terBeam = terbeam_predomics_aggregation_ova_constrained),
filter.cv.prev = 0,
min.kfold.nb = FALSE,
learner.grep.pattern = "*",
#nb.top.features = 50,
feature.selection = fa,
scaled.importance = TRUE,
make.plot = TRUE,
cv.prevalence = FALSE)
feat2.import <- mergeMeltImportanceCV_mc(list.results = list(terBeam = terbeam_predomics_aggregation_ova_constrained),
filter.cv.prev = 0,
min.kfold.nb = FALSE,
learner.grep.pattern = "*",
nb.top.features = 148,
#feature.selection = rownames(fa$pop.noz),
scaled.importance = TRUE,
make.plot = TRUE,
cv.prevalence = FALSE)
feat1.import <- mergeMeltImportanceCV_mc(list.results = list(terBeam = terbeam_voting_unconstrained),
filter.cv.prev = 0,
min.kfold.nb = FALSE,
learner.grep.pattern = "*",
#nb.top.features = 50,
feature.selection = fa,
scaled.importance = TRUE,
make.plot = TRUE,
cv.prevalence = FALSE)
feat2.import <- mergeMeltImportanceCV_mc(list.results = list(terBeam = terbeam_voting_unconstrained),
filter.cv.prev = 0,
min.kfold.nb = FALSE,
learner.grep.pattern = "*",
nb.top.features = 148,
#feature.selection = rownames(fa$pop.noz),
scaled.importance = TRUE,
make.plot = TRUE,
cv.prevalence = FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
library(ggpubr)
library(dplyr)
library(tidyr)
library(tibble)
library(knitr)
library(kableExtra)
library(DT)
library(e1071)
library(glmnet)
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
# Filter null values
X_general <- mc.input$X[, colSums(mc.input$X) != 0]
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE)
set.seed(42)
y = as.vector(yvec)
X = X_general
# Determine the number of samples per class
nombre_echantillons_par_classe <- min(table(y))
# Function to balance classes and maintain order
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe, seed = 123) {
classes <- unique(y)
indices_equilibres <- integer(0)
for (classe in classes) {
indices_classe <- which(y == classe)
set.seed(seed)
indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
}
# Sort balanced indices to maintain original order
indices_equilibres <- sort(indices_equilibres)
return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}
# Get balanced data
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X
# Check distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)
# Split balanced data into 80% for training and 20% for testing
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[, indices_division, drop = FALSE]
X.test <- X_equilibre[, -indices_division, drop = FALSE]
table(y)
table(y.test)
dim(X)
dim(X.test)
clf <- terBeam_mc(sparsity = c(7,8,9),
max.nb.features = 1000,
seed = 1,
nCores = 1,
evalToFit = "accuracy_",
objective = "auc",
experiment.id = "terBeam_mc",
experiment.save = "nothing")
printy(clf)
load("terbeam_predomics_aggregation_ova_constrained.rda")
res_clf.dig <- digestmc(obj = terbeam_predomics_aggregation_ova_constrained, penalty = 0.75/100, plot = TRUE)
clf <- regenerate_clf(clf, X, y, approch = "ova")
# get the best model
best.model = res_clf.dig$best$model
# Visualize the model information (if needed)
printy_mc(best.model)
# Generate the plots
plots1 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ova")
plots2 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE, approch = "ova")
# Open a PDF device to save the grid of plots
pdf("model_visualization_plots.pdf", width = 12, height = 8)  # Adjust width and height as needed
# Extract the plots from each list and pass them to `grid.arrange`
grid.arrange(grobs = c(plots1, plots2), ncol = 4)
# Close the PDF device
dev.off()
# Confirmation of the save process
cat("The plots have been saved under the name 'model_visualization_plots.pdf'.\n")
tmp <- plotAUC_mc(scores = best.model$score_, y = y, percent = TRUE, approch = "ova"); rm(tmp)
# Convert the model collection into a population of models scrambled by model size
pop <- modelCollectionToPopulation(terbeam_predomics_aggregation_ova_constrained$classifier$models)
printy_mc(pop)
# Function to create boxplot for a given data frame
create_boxplot <- function(data, title) {
# Melt the dataframe for ggplot
data.melt <- melt(data, id.vars = c("accuracy_", "eval.sparsity"))
# Create ggplot
plot <- ggplot(data = data.melt, aes(y = accuracy_, x = eval.sparsity)) +
geom_boxplot(notch = FALSE, outlier.shape = NA, position = position_dodge(width = 0.9), alpha = 0.3) +
geom_point(aes(color = eval.sparsity), position = position_jitterdodge(dodge.width = 0.9), size = 1, alpha = 0.5) +
ylim(c(0, 1)) +
xlab("Model Parsimony") +
ggtitle(title) +
theme_bw() +
theme(legend.position = "bottom", legend.direction = "horizontal") +
guides(colour = "none")
return(plot)
}
# Convert the population to a data frame
pop.df <- populationToDataFrame_mc(pop)
# Plotting for the original population (single figure)
pop.dff <- as.data.frame(pop.df[[1]])  # Convert the first submodel to a data frame
g.before <- create_boxplot(pop.dff, title = "Original Population")
#print(g.before)  # Display the plot
# Select the best population models
fbm <- selectBestPopulation(pop)
#printy_mc(fbm)
# Convert the best population models to a data frame
fbm.df <- populationToDataFrame_mc(fbm)
# Plotting for the selected best models (single figure)
fbm.dff <- as.data.frame(fbm.df[[1]])  # Convert the first submodel to a data frame
g.after <- create_boxplot(fbm.dff, title = "FBM")
print(g.after)  # Display the plot
fa <- makeFeatureAnnot_mc(pop = fbm,
X = X,
y = y,
clf = clf,
approch = "ova")
plot_distribution <- plotFeatureModelCoeffs_mc(feat.model.coeffs = fa, y, approch = "ova")
plots_abundance <- plotAbundanceByClass_mc(features = fa, X, y, approch = "ova")
plots_prevalence <- plotPrevalence_mc(features = fa, X = X, y = y, approch = "ova")
feat1.import <- mergeMeltImportanceCV_mc(list.results = list(terBeam = terbeam_predomics_aggregation_ova_constrained),
filter.cv.prev = 0,
min.kfold.nb = FALSE,
learner.grep.pattern = "*",
#nb.top.features = 50,
feature.selection = fa,
scaled.importance = TRUE,
make.plot = TRUE,
cv.prevalence = FALSE)
feat2.import <- mergeMeltImportanceCV_mc(list.results = list(terBeam = terbeam_predomics_aggregation_ova_constrained),
filter.cv.prev = 0,
min.kfold.nb = FALSE,
learner.grep.pattern = "*",
nb.top.features = 148,
#feature.selection = rownames(fa$pop.noz),
scaled.importance = TRUE,
make.plot = TRUE,
cv.prevalence = FALSE)
rmarkdown::render("README.Rmd")
library(mcpredomics)
knitr::opts_chunk$set(echo = TRUE)
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
library(ggpubr)
library(dplyr)
library(tidyr)
library(tibble)
library(knitr)
library(kableExtra)
library(DT)
library(e1071)
library(glmnet)
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
library(ggpubr)
library(dplyr)
library(tidyr)
library(tibble)
library(knitr)
library(kableExtra)
library(DT)
library(e1071)
library(glmnet)
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
# Filter null values
X_general <- mc.input$X[, colSums(mc.input$X) != 0]
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE)
set.seed(42)
y = as.vector(yvec)
X = X_general
# Determine the number of samples per class
nombre_echantillons_par_classe <- min(table(y))
# Function to balance classes and maintain order
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe, seed = 123) {
classes <- unique(y)
indices_equilibres <- integer(0)
for (classe in classes) {
indices_classe <- which(y == classe)
set.seed(seed)
indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
}
# Sort balanced indices to maintain original order
indices_equilibres <- sort(indices_equilibres)
return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}
# Get balanced data
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X
# Check distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)
# Split balanced data into 80% for training and 20% for testing
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[, indices_division, drop = FALSE]
X.test <- X_equilibre[, -indices_division, drop = FALSE]
table(y)
table(y.test)
dim(X)
dim(X.test)
clf <- terBeam_mc(sparsity = c(7,8,9),
max.nb.features = 1000,
seed = 1,
nCores = 1,
evalToFit = "accuracy_",
objective = "auc",
experiment.id = "terBeam_mc",
experiment.save = "nothing")
printy(clf)
load("terbeam_predomics_aggregation_ova_constrained.rda")
??printy_mc
??runClassifier_mc
knitr::opts_chunk$set(echo = TRUE)
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
library(ggpubr)
library(dplyr)
library(tidyr)
library(tibble)
library(knitr)
library(kableExtra)
library(DT)
library(e1071)
library(glmnet)
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
library(ggpubr)
library(dplyr)
library(tidyr)
library(tibble)
library(knitr)
library(kableExtra)
library(DT)
library(e1071)
library(glmnet)
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
# Filter null values
X_general <- mc.input$X[, colSums(mc.input$X) != 0]
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE)
set.seed(42)
y = as.vector(yvec)
X = X_general
# Determine the number of samples per class
nombre_echantillons_par_classe <- min(table(y))
# Function to balance classes and maintain order
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe, seed = 123) {
classes <- unique(y)
indices_equilibres <- integer(0)
for (classe in classes) {
indices_classe <- which(y == classe)
set.seed(seed)
indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
}
# Sort balanced indices to maintain original order
indices_equilibres <- sort(indices_equilibres)
return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}
# Get balanced data
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X
# Check distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)
# Split balanced data into 80% for training and 20% for testing
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[, indices_division, drop = FALSE]
X.test <- X_equilibre[, -indices_division, drop = FALSE]
table(y)
table(y.test)
dim(X)
dim(X.test)
clf <- terBeam_mc(sparsity = c(7,8,9),
max.nb.features = 1000,
seed = 1,
nCores = 1,
evalToFit = "accuracy_",
objective = "auc",
experiment.id = "terBeam_mc",
experiment.save = "nothing")
printy(clf)
load("terbeam_predomics_aggregation_ova_constrained.rda")
res_clf.dig <- digestmc(obj = terbeam_predomics_aggregation_ova_constrained, penalty = 0.75/100, plot = TRUE)
clf <- regenerate_clf(clf, X, y, approch = "ova")
# get the best model
best.model = res_clf.dig$best$model
# Visualize the model information (if needed)
printy_mc(best.model)
# Generate the plots
plots1 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ova")
plots2 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE, approch = "ova")
# Open a PDF device to save the grid of plots
pdf("model_visualization_plots.pdf", width = 12, height = 8)  # Adjust width and height as needed
# Extract the plots from each list and pass them to `grid.arrange`
grid.arrange(grobs = c(plots1, plots2), ncol = 4)
# Close the PDF device
dev.off()
# Confirmation of the save process
cat("The plots have been saved under the name 'model_visualization_plots.pdf'.\n")
tmp <- plotAUC_mc(scores = best.model$score_, y = y, percent = TRUE, approch = "ova"); rm(tmp)
# Convert the model collection into a population of models scrambled by model size
pop <- modelCollectionToPopulation(terbeam_predomics_aggregation_ova_constrained$classifier$models)
printy_mc(pop)
# Function to create boxplot for a given data frame
create_boxplot <- function(data, title) {
# Melt the dataframe for ggplot
data.melt <- melt(data, id.vars = c("accuracy_", "eval.sparsity"))
# Create ggplot
plot <- ggplot(data = data.melt, aes(y = accuracy_, x = eval.sparsity)) +
geom_boxplot(notch = FALSE, outlier.shape = NA, position = position_dodge(width = 0.9), alpha = 0.3) +
geom_point(aes(color = eval.sparsity), position = position_jitterdodge(dodge.width = 0.9), size = 1, alpha = 0.5) +
ylim(c(0, 1)) +
xlab("Model Parsimony") +
ggtitle(title) +
theme_bw() +
theme(legend.position = "bottom", legend.direction = "horizontal") +
guides(colour = "none")
return(plot)
}
# Convert the population to a data frame
pop.df <- populationToDataFrame_mc(pop)
# Plotting for the original population (single figure)
pop.dff <- as.data.frame(pop.df[[1]])  # Convert the first submodel to a data frame
g.before <- create_boxplot(pop.dff, title = "Original Population")
#print(g.before)  # Display the plot
# Select the best population models
fbm <- selectBestPopulation(pop)
#printy_mc(fbm)
# Convert the best population models to a data frame
fbm.df <- populationToDataFrame_mc(fbm)
# Plotting for the selected best models (single figure)
fbm.dff <- as.data.frame(fbm.df[[1]])  # Convert the first submodel to a data frame
g.after <- create_boxplot(fbm.dff, title = "FBM")
print(g.after)  # Display the plot
fa <- makeFeatureAnnot_mc(pop = fbm,
X = X,
y = y,
clf = clf,
approch = "ova")
plot_distribution <- plotFeatureModelCoeffs_mc(feat.model.coeffs = fa, y, approch = "ova")
plots_abundance <- plotAbundanceByClass_mc(features = fa, X, y, approch = "ova")
plots_prevalence <- plotPrevalence_mc(features = fa, X = X, y = y, approch = "ova")
feat1.import <- mergeMeltImportanceCV_mc(list.results = list(terBeam = terbeam_predomics_aggregation_ova_constrained),
filter.cv.prev = 0,
min.kfold.nb = FALSE,
learner.grep.pattern = "*",
#nb.top.features = 50,
feature.selection = fa,
scaled.importance = TRUE,
make.plot = TRUE,
cv.prevalence = FALSE)
feat2.import <- mergeMeltImportanceCV_mc(list.results = list(terBeam = terbeam_predomics_aggregation_ova_constrained),
filter.cv.prev = 0,
min.kfold.nb = FALSE,
learner.grep.pattern = "*",
nb.top.features = 148,
#feature.selection = rownames(fa$pop.noz),
scaled.importance = TRUE,
make.plot = TRUE,
cv.prevalence = FALSE)
