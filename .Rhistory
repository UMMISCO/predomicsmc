missing_len <- length(score_temp) - length(y_temp)
y_random <- sample(y_temp, missing_len, replace = TRUE)
y_temp <- c(y_temp, y_random)
} else if (length(score_temp) < length(y_temp)) {
missing_len <- length(y_temp) - length(score_temp)
score_random <- sample(score_temp, missing_len, replace = TRUE)
score_temp <- c(score_temp, score_random)
}
# Save adjusted pairs
list_y[[i]] <- as.vector(y_temp)
list_scores[[i]] <- score_temp
}
} else {
stop("Invalid approach: choose 'ova' or 'ovo'")
}
# Loop to generate plots for each class or class combination
for (i in seq_along(list_y)) {
class_label <- if (approch == "ovo") {
comb <- combinations[[i]]
paste(comb[1], "vs", comb[2])  # Adjust to properly label each comparison
} else {
paste(classes[i], "vs ALL")
}
# Call the plotAUC function (this function needs to return a ggplot object)
tryCatch({
plot_item <- plotAUC(  # Assuming plotAUC is the function that generates a ggplot
score = list_scores[[i]],
y = list_y[[i]],
main = paste(main, class_label),
ci = ci,
percent = percent
)
# Ensure the generated plot is a ggplot object
if (!inherits(plot_item, "gg")) {
stop("The generated plot is not a ggplot object for class: ", class_label)
}
plot_list[[class_label]] <- plot_item
}, error = function(e) {
message("Error plotting for ", class_label, ": ", e$message)
})
}
# Arrange the plots in a grid
ncol <- length(list_y)
# Arrange the plots in a grid
ncol <- 4  # You can adjust this number based on how you want the plots displayed
arranged_plot <- gridExtra::grid.arrange(grobs = plot_list, ncol = ncol)  # Arrange in grid
# Display the arranged plots
print(arranged_plot)
# Return the arranged plot (optional)
return(arranged_plot)
}
approch = "OVA"
tmp <- plotAUC_mc(best.model$score_, y, percent = TRUE, approch = "ova"); rm(tmp)
library(gridExtra)
plotAUC_mc <- function(scores, y, approch = "ovo", main = "", ci = TRUE, percent = TRUE) {
# Identifier les classes uniques
classes <- unique(y)
list_y <- list()
list_scores <- list()
plot_list <- list()
# One-vs-One (OVO) approach
if (approch == "ovo") {
combinations <- combn(classes, 2, simplify = FALSE)  # Créer toutes les combinaisons de classes
k <- 1
for (comb in combinations) {
class_i <- comb[1]
class_j <- comb[2]
# Filtrer les échantillons des deux classes
indices <- which(y == class_i | y == class_j)
y_pair <- y[indices]
score_pair <- scores[[k]]
# Ajuster les tailles si nécessaire
if (length(y_pair) < length(score_pair)) {
missing_len <- length(score_pair) - length(y_pair)
y_random <- sample(y_pair, missing_len, replace = TRUE)
y_pair <- c(y_pair, y_random)
} else if (length(score_pair) < length(y_pair)) {
missing_len <- length(y_pair) - length(score_pair)
score_random <- sample(score_pair, missing_len, replace = TRUE)
score_pair <- c(score_pair, score_random)
}
# Vérification des types avant d'ajouter à la liste
if (!is.numeric(score_pair) || !is.character(y_pair)) {
stop("Scores must be numeric and labels must be character.")
}
# Sauvegarder les paires ajustées
list_y[[k]] <- as.vector(y_pair)
list_scores[[k]] <- score_pair
k <- k + 1
}
} else if (approch == "ova") {
for (i in seq_along(classes)) {
class_i <- classes[i]
# Créer les étiquettes binaires (classe vs All)
y_temp <- ifelse(y == class_i, as.character(class_i), "All")
score_temp <- scores[[i]]
# Ajuster les tailles si nécessaire
if (length(y_temp) < length(score_temp)) {
missing_len <- length(score_temp) - length(y_temp)
y_random <- sample(y_temp, missing_len, replace = TRUE)
y_temp <- c(y_temp, y_random)
} else if (length(score_temp) < length(y_temp)) {
missing_len <- length(y_temp) - length(score_temp)
score_random <- sample(score_temp, missing_len, replace = TRUE)
score_temp <- c(score_temp, score_random)
}
# Vérification des types avant d'ajouter à la liste
if (!is.numeric(score_temp) || !is.character(y_temp)) {
stop("Scores must be numeric and labels must be character.")
}
# Sauvegarder les paires ajustées
list_y[[i]] <- as.vector(y_temp)
list_scores[[i]] <- score_temp
}
} else {
stop("Invalid approach: choose 'ova' or 'ovo'")
}
# Boucle pour générer des graphes pour chaque classe ou combinaison de classes
for (i in seq_along(list_y)) {
class_label <- if (approch == "ovo") {
comb <- combinations[[i]]
paste(comb[1], "vs", comb[2])  # Ajustement pour étiqueter correctement chaque comparaison
} else {
paste(classes[i], "vs ALL")
}
# Appel de la fonction plotAUC pour chaque sous-modèle
tryCatch({
plot_list[[class_label]] <- plotAUC(
score = list_scores[[i]],
y = list_y[[i]],
main = paste(main, class_label),
ci = ci,
percent = percent
)
}, error = function(e) {
message("Error plotting: ", e$message)
})
}
# Afficher les plots dans une grille
grid.arrange(grobs = plot_list, ncol = 2)  # Changez ncol selon vos besoins
return(plot_list)
}
approch = "OVA"
tmp <- plotAUC_mc(best.model$score_, y, percent = TRUE, approch = "ova"); rm(tmp)
approch = "OVA"
tmp <- plotAUC_mc(best.model$score_, y, percent = TRUE, approch = "ova"); rm(tmp)
tmp <- plotAUC_mc(best.model$score_, y, percent = TRUE, approch = "ova"); rm(tmp)
plotAUC_mc <- function(scores, y, approch = "ovo", main = "", ci = TRUE, percent = TRUE) {
# Identifier les classes uniques
classes <- unique(y)
list_y <- list()
list_scores <- list()
plot_list <- list()
# One-vs-One (OVO) approach
if (approch == "ovo") {
combinations <- combn(classes, 2, simplify = FALSE)  # Créer toutes les combinaisons de classes
k <- 1
for (comb in combinations) {
class_i <- comb[1]
class_j <- comb[2]
# Filtrer les échantillons des deux classes
indices <- which(y == class_i | y == class_j)
y_pair <- y[indices]
score_pair <- scores[[k]]
# Ajuster les tailles si nécessaire
if (length(y_pair) < length(score_pair)) {
missing_len <- length(score_pair) - length(y_pair)
y_random <- sample(y_pair, missing_len, replace = TRUE)
y_pair <- c(y_pair, y_random)
} else if (length(score_pair) < length(y_pair)) {
missing_len <- length(y_pair) - length(score_pair)
score_random <- sample(score_pair, missing_len, replace = TRUE)
score_pair <- c(score_pair, score_random)
}
# Vérification des types avant d'ajouter à la liste
if (!is.numeric(score_pair) || !is.character(y_pair)) {
stop("Scores must be numeric and labels must be character.")
}
# Sauvegarder les paires ajustées
list_y[[k]] <- as.vector(y_pair)
list_scores[[k]] <- score_pair
k <- k + 1
}
# One-vs-All (OVA) approach
} else if (approch == "ova") {
for (i in seq_along(classes)) {
class_i <- classes[i]
# Créer les étiquettes binaires (classe vs All)
y_temp <- ifelse(y == class_i, as.character(class_i), "All")
score_temp <- scores[[i]]
# Ajuster les tailles si nécessaire
if (length(y_temp) < length(score_temp)) {
missing_len <- length(score_temp) - length(y_temp)
y_random <- sample(y_temp, missing_len, replace = TRUE)
y_temp <- c(y_temp, y_random)
} else if (length(score_temp) < length(y_temp)) {
missing_len <- length(y_temp) - length(score_temp)
score_random <- sample(score_temp, missing_len, replace = TRUE)
score_temp <- c(score_temp, score_random)
}
# Vérification des types avant d'ajouter à la liste
if (!is.numeric(score_temp) || !is.character(y_temp)) {
stop("Scores must be numeric and labels must be character.")
}
# Sauvegarder les paires ajustées
list_y[[i]] <- as.vector(y_temp)
list_scores[[i]] <- score_temp
}
} else {
stop("Invalid approach: choose 'ova' or 'ovo'")
}
# Boucle pour générer des graphes pour chaque classe ou combinaison de classes
for (i in seq_along(list_y)) {
class_label <- if (approch == "ovo") {
comb <- combinations[[i]]
paste(comb[1], "vs", comb[2])  # Ajustement pour étiqueter correctement chaque comparaison
} else {
paste(classes[i], "vs ALL")
}
# Appel de la fonction plotAUC pour chaque sous-modèle
tryCatch({
plot_list[[class_label]] <- plotAUC(
score = list_scores[[i]],
y = list_y[[i]],
main = paste(main, class_label),
ci = ci,
percent = percent
)
}, error = function(e) {
message("Error plotting: ", e$message)
})
}
return(plot_list)
}
tmp <- plotAUC_mc(best.model$score_, y, percent = TRUE, approch = "ova"); rm(tmp)
best.model <- fbm[[1]]
tmp <- plotAUC_mc(best.model$score_, y, percent = TRUE, approch = "ova"); rm(tmp)
# get the best model
printy_mc(best.model)
plots1 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ova")
plots2 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE, approch = "ova")
# Extract the plots from each list and pass them to `grid.arrange
grid.arrange(grobs = c(plots1, plots2), ncol = 4)
# get the best model
printy_mc(best.model)
plots1 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ova")
plots2 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE, approch = "ova")
# Extract the plots from each list and pass them to `grid.arrange
grid.arrange(grobs = c(plots1, plots2), ncol = 4)
# get the best model
# Visualize the model information (if needed)
printy_mc(best.model)
# Generate the plots
plots1 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ova")
plots2 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE, approch = "ova")
# Open a PDF device to save the grid of plots
pdf("model_visualization_plots.pdf", width = 12, height = 8)  # Adjust width and height as needed
# Extract the plots from each list and pass them to `grid.arrange`
grid.arrange(grobs = c(plots1, plots2), ncol = 4)
# Close the PDF device
dev.off()
# Confirmation of the save process
cat("The plots have been saved under the name 'model_visualization_plots.pdf'.\n")
knitr::opts_chunk$set(echo = TRUE)
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
library(ggpubr)
library(dplyr)
library(tidyr)
library(tibble)
library(knitr)
library(kableExtra)
library(DT)
library(e1071)
library(glmnet)
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Create an index vector for data partitioning
X_general <- X_general[rowSums(X_general)!=0,]; dim(X_general) # filter out variables with only zero values
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE); dim(X_general)
set.seed(42)
y = as.vector(yvec_trie)
X = X_general
# Number of desired samples in each class
nombre_echantillons_par_classe <- min(table(y))
# Function to balance the classes
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe,seed =123) {
classes <- unique(y)
indices_equilibres <- integer(0)
for (classe in classes) {
indices_classe <- which(y == classe)
set.seed(seed)
indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
}
return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X
# Verify the distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)
# Split yvec_trie into 80% train and 20% test
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[,indices_division]
X.test <- X_equilibre[,-indices_division]
table(y)
table(y.test)
dim(X)
dim(X.test)
clf <- terBeam_mc(sparsity = c(2,3,4,5,6,7,8,9,10),
max.nb.features = 1000,
seed = 1,
nCores = 1,
evalToFit = "accuracy_",
objective = "auc",
experiment.id = "terBeam_mc",
experiment.save = "nothing")
printy(clf)
load("Unique_Predomics_aggregation_ova.rda")
load("Unique_Predomics_aggregation_ovo.rda")
load("Unique_votingAggregation.rda")
load("Unique_maximizationAggregation.rda")
load("Unique_rankingAggregation.rda")
load("Unique_weightedAggregation.rda")
load("res_clf.rda")
load("sota_ml.rda")
load("Final_combined_results_with_terbeam.rda")
# Select only the relevant columns for generalization
combined_generalization <- Final_combined_results_with_terbeam %>%
select(Methods, Accuracy.generalization, Precision.generalization, Recall.generalization, F1.generalization)
# Convert the columns to numeric, handling potential conversions
combined_generalization <- combined_generalization %>%
mutate(across(Accuracy.generalization:F1.generalization, ~ as.numeric(as.character(.))))
# Create a new column for sorting methods, prioritizing those starting with 'Terbeam'
combined_generalization <- combined_generalization %>%
mutate(Order = ifelse(grepl("^Terbeam", Methods), 0, 1)) %>%
arrange(Order, Methods)  # Sort by Order first, then Methods
# Reshape the data into long format for easier visualization
combined_long_generalization <- combined_generalization %>%
gather(key = "Metric", value = "Value", -Methods, -Order) %>%
mutate(Type = "Generalization") %>%
drop_na(Value)
# Calculate the standard deviation for each method and each metric
ecart_type_results_generalization <- combined_long_generalization %>%
group_by(Methods, Metric) %>%
summarise(EcartType = sd(Value, na.rm = TRUE), .groups = 'drop')
# Display the standard deviation results
print(ecart_type_results_generalization)
# Create the boxplot with facets for each metric
p_generalization <- ggplot(combined_long_generalization, aes(x = reorder(Methods, Order), y = Value, fill = Methods)) +
geom_boxplot() +
facet_wrap(~ Metric, scales = "free_y", nrow = 1) +  # Arrange horizontally in a single row
labs(title = "Generalization Metrics Boxplots by Methods",
x = "Methods",
y = "Values") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.5),  # Rotate method names vertically
panel.border = element_rect(colour = "black", fill = NA, size = 1),  # Encapsulate each facet
strip.background = element_rect(colour = "black", fill = "lightgray"),  # Outline facet titles
legend.position = "bottom",  # Position the legend at the bottom
legend.justification = "left",  # Justify the legend to the left
legend.title = element_blank()) +  # Remove the title of the legend
guides(fill = guide_legend(nrow = 2, byrow = TRUE)) +  # Legend in two rows
scale_fill_brewer(palette = "Set3")  # Palette of colors for the methods
# Save the plot as a PDF
ggsave("generalization_metrics_boxplots.pdf", plot = p_generalization, width = 12, height = 6)
# Confirmation of the save process
cat("The plot has been saved under the name 'generalization_metrics_boxplots.pdf'.\n")
# Select only the relevant columns for generalization
combined_generalization <- Final_combined_results_with_terbeam %>%
select(Methods, Accuracy.generalization, Precision.generalization, Recall.generalization, F1.generalization)
# Select only the relevant columns for empirical data
combined_empirique <- Final_combined_results_with_terbeam %>%
select(Methods, Accuracy.empirique, Precision.empirique, Recall.empirique, F1.empirique)
# Convert the columns to numeric, handling potential conversions
combined_generalization <- combined_generalization %>%
mutate(across(Accuracy.generalization:F1.generalization, ~ as.numeric(as.character(.))))
combined_empirique <- combined_empirique %>%
mutate(across(Accuracy.empirique:F1.empirique, ~ as.numeric(as.character(.))))
# Reshape the data into long format for easier visualization
combined_long_generalization <- combined_generalization %>%
gather(key = "Metric", value = "Value", -Methods) %>%
mutate(Type = "Generalization") %>%
separate(Metric, into = c("MetricName", "TypeSuffix"), sep = "\\.") %>%
drop_na(Value)
combined_long_empirique <- combined_empirique %>%
gather(key = "Metric", value = "Value", -Methods) %>%
mutate(Type = "Empirical") %>%
separate(Metric, into = c("MetricName", "TypeSuffix"), sep = "\\.") %>%
drop_na(Value)
# Combine both datasets
combined_long <- bind_rows(combined_long_generalization, combined_long_empirique)
# Adjust the order of the methods to prioritize those starting with "Terbeam"
combined_long <- combined_long %>%
mutate(Methods = factor(Methods, levels = c(grep("^Terbeam", unique(Methods), value = TRUE),
setdiff(unique(Methods), grep("^Terbeam", unique(Methods), value = TRUE))) ))
# Create the boxplot with facets for each metric
p_combined <- ggplot(combined_long, aes(x = Methods, y = Value, fill = Type)) +
geom_boxplot(position = position_dodge()) +
facet_wrap(~ MetricName, scales = "free_y", nrow = 1) +  # Facet by metric name
labs(title = "Empirical vs Generalization Metrics by Methods",
x = "Methods",
y = "Values") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, hjust = 0.5, vjust = 0.5),
panel.border = element_rect(colour = "black", fill = NA, size = 1),
strip.background = element_rect(colour = "black", fill = "lightgray"),
legend.position = "bottom",
legend.justification = "left",
legend.title = element_blank()) +
guides(fill = guide_legend(nrow = 1, byrow = TRUE)) +
scale_fill_brewer(palette = "Set1")  # Distinctive palette to differentiate types
# Save the plot as a PDF
ggsave("combined_metrics_boxplots_with_facets.pdf", plot = p_combined, width = 12, height = 6)
# Confirmation of the save process
cat("The plot has been saved under the name 'combined_metrics_boxplots_with_facets.pdf'.\n")
# Convert the model collection into a population of models scrambled by model size
pop <- modelCollectionToPopulation(res_clf$classifier$models)
printy_mc(pop)
# Function to create boxplot for a given data frame
create_boxplot <- function(data, title) {
# Melt the dataframe for ggplot
data.melt <- melt(data, id.vars = c("accuracy_", "eval.sparsity"))
# Create ggplot
plot <- ggplot(data = data.melt, aes(y = accuracy_, x = eval.sparsity)) +
geom_boxplot(notch = FALSE, outlier.shape = NA, position = position_dodge(width = 0.9), alpha = 0.3) +
geom_point(aes(color = eval.sparsity), position = position_jitterdodge(dodge.width = 0.9), size = 1, alpha = 0.5) +
ylim(c(0, 1)) +
xlab("Model Parsimony") +
ggtitle(title) +
theme_bw() +
theme(legend.position = "bottom", legend.direction = "horizontal") +
guides(colour = "none")
return(plot)
}
# Convert the population to a data frame
pop.df <- populationToDataFrame_mc(pop)
# Plotting for the original population (single figure)
pop.dff <- as.data.frame(pop.df[[1]])  # Convert the first submodel to a data frame
g.before <- create_boxplot(pop.dff, title = "Original Population")
#print(g.before)  # Display the plot
# Select the best population models
fbm <- selectBestPopulation(pop)
#printy_mc(fbm)
# Convert the best population models to a data frame
fbm.df <- populationToDataFrame_mc(fbm)
# Plotting for the selected best models (single figure)
fbm.dff <- as.data.frame(fbm.df[[1]])  # Convert the first submodel to a data frame
g.after <- create_boxplot(fbm.dff, title = "FBM")
print(g.after)  # Display the plot
fa <- makeFeatureAnnot_mc(pop = fbm,
X = X,
y = y,
clf = clf,
approch = "ova")
plot_distribution <- plotFeatureModelCoeffs_mc(feat.model.coeffs = fa, y, approch = "OVA")
plots_abundance <- plotAbundanceByClass_mc(features = fa, X, y, approch = "OVA")
plots_prevalence <- plotPrevalence_mc(features = fa, X = X, y = y, approch = "OVA")
clf <- regenerate_clf(clf, X, y, approch = "ova")
best.model <- fbm[[1]]
tmp <- plotAUC_mc(best.model$score_, y, percent = TRUE, approch = "ova"); rm(tmp)
# get the best model
# Visualize the model information (if needed)
printy_mc(best.model)
# Generate the plots
plots1 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ova")
plots2 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE, approch = "ova")
# Open a PDF device to save the grid of plots
pdf("model_visualization_plots.pdf", width = 12, height = 8)  # Adjust width and height as needed
# Extract the plots from each list and pass them to `grid.arrange`
grid.arrange(grobs = c(plots1, plots2), ncol = 4)
# Close the PDF device
dev.off()
# Confirmation of the save process
cat("The plots have been saved under the name 'model_visualization_plots.pdf'.\n")
# Generate the plot
plot_distribution <- plotFeatureModelCoeffs_mc(feat.model.coeffs = fa, y, approch = "OVA")
# Open a PDF device for saving
pdf("plot_feature_model_coeffs.pdf", width = 12, height = 8)  # Adjust the width and height as needed
# Print the plot directly into the PDF
print(plot_distribution)
# Close the PDF device
dev.off()
# Confirmation of the save process
cat("The plot has been saved under the name 'plot_feature_model_coeffs.pdf'.\n")
# Generate the plot
plot_distribution <- plotFeatureModelCoeffs_mc(feat.model.coeffs = fa, y, approch = "OVA")
# Save the plot as a PDF
ggsave("plot_distribution.pdf", plot = p_generalization, width = 12, height = 6)
# Confirmation of the save process
cat("The plot has been saved under the name 'plot_distribution.pdf'.\n")
# Generate the plot
plot_distribution <- plotFeatureModelCoeffs_mc(feat.model.coeffs = fa, y, approch = "OVA")
# Save the plot as a PDF
ggsave("plot_distribution.pdf", plot = plot_distribution, width = 12, height = 6)
# Confirmation of the save process
cat("The plot has been saved under the name 'plot_distribution.pdf'.\n")
plot_distribution <- plotFeatureModelCoeffs_mc(feat.model.coeffs = fa, y, approch = "OVA")
