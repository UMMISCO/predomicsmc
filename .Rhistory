# Données fournies
Multi_weighted <- Multi_weightedAggregation$crossVal$scores$mean.acc
Multi_maximization <- Multi_maximizationAggregation$crossVal$scores$mean.acc
Multi_Predomics_Aggre <- Multi_Predomics_Aggregation_ova$crossVal$scores$mean.acc
Multi_ranking <- Multi_rankingAggregation$crossVal$scores$mean.acc
Multi_voting <- Multi_votingAggregation$crossVal$scores$mean.acc
Multi_Predomics_Aggre_ovo <- Multi_Predomics_aggregation_ovo$crossVal$scores$mean.acc
# Ajouter la colonne 'k'
Multi_weighted$k <- rownames(Multi_weighted)
Multi_maximization$k <- rownames(Multi_maximization)
Multi_Predomics_Aggre$k <- rownames(Multi_Predomics_Aggre)
Multi_ranking$k <- rownames(Multi_ranking)
Multi_voting$k <- rownames(Multi_voting)
Multi_Predomics_Aggre_ovo$k <- rownames(Multi_Predomics_Aggre_ovo)
# Ajouter une colonne 'algorithm' pour chaque dataframe
Multi_weighted$algorithm <- "Multi_weightedAggregation"
Multi_maximization$algorithm <- "Multi_maximizationAggregation"
Multi_Predomics_Aggre$algorithm <- "Multi_Predomics_Aggregation_ova"
Multi_voting$algorithm <- "Multi_votingAggregation"
Multi_ranking$algorithm <- "Multi_rankingAggregation"
Multi_Predomics_Aggre_ovo$algorithm <- "Multi_Predomics_Aggregation_ovo"
# Réordonner les colonnes pour avoir "k" et "algorithm" en premier
Multi_weighted <- Multi_weighted[, c("k", "empirical", "generalization", "algorithm")]
Multi_maximization <- Multi_maximization[, c("k", "empirical", "generalization", "algorithm")]
Multi_Predomics_Aggre <- Multi_Predomics_Aggre[, c("k", "empirical", "generalization", "algorithm")]
Multi_voting <- Multi_voting[, c("k", "empirical", "generalization", "algorithm")]
Multi_ranking <- Multi_ranking[, c("k", "empirical", "generalization", "algorithm")]
Multi_Predomics_Aggre_ovo <- Multi_Predomics_Aggre_ovo[, c("k", "empirical", "generalization", "algorithm")]
# Combiner les dataframes
combined_df <- rbind(Multi_weighted, Multi_maximization,
Multi_Predomics_Aggre, Multi_ranking,
Multi_voting,Multi_Predomics_Aggre_ovo)
# Supprimer les lignes contenant des valeurs NaN
combined_df <- na.omit(combined_df)
# Transformer les données pour ggplot2
data_melted <- melt(combined_df, id.vars = c("k", "algorithm"), variable.name = "type", value.name = "accuracy")
# Plotting
ggplot(data_melted, aes(x = k, y = accuracy, color = type)) +
geom_point(size = 3) +
geom_line(aes(group = interaction(algorithm, type), linetype = type)) +
geom_text(aes(label = round(accuracy, 3)), vjust = -0.5, size = 3) +  # Ajouter les valeurs à côté des points
facet_wrap(~ algorithm, ncol = 2) +  # Afficher deux graphiques par ligne
theme_minimal() +
labs(title = "Mean accuracy: empirical vs. generalization for cross-validation using the specific variables of each combination.",
x = "Valeur de k",
y = "Accuracy",
color = "Type") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
scale_color_manual(values = c("empirical" = "#377EB8", "generalization" = "#E41A1C"))
# Fonction pour extraire, transformer et calculer les statistiques
process_algorithm <- function(data, algorithm_name) {
df <- as.data.frame(data)
df$k <- rownames(df)
df_long <- df %>%
pivot_longer(cols = -k, names_to = "fold", values_to = "accuracy")
df_stats <- df_long %>%
group_by(k) %>%
summarise(
mean_accuracy = mean(accuracy, na.rm = TRUE),
sd_accuracy = sd(accuracy, na.rm = TRUE),
algorithm = algorithm_name,
.groups = 'drop'
)
return(df_stats)
}
# Traiter chaque algorithme
df_weighted <- process_algorithm(Multi_weightedAggregation$crossVal$scores$generalization.acc, "Weighted voting")
df_maximization <- process_algorithm(Multi_maximizationAggregation$crossVal$scores$generalization.acc, "Maximization")
df_Predomics_Aggre <-process_algorithm(Multi_Predomics_Aggregation_ova$crossVal$scores$generalization.acc, "Predomics_Aggregation_ova")
df_ranking <- process_algorithm(Multi_rankingAggregation$crossVal$scores$generalization.acc, "Ranking")
df_voting <- process_algorithm(Multi_votingAggregation$crossVal$scores$generalization.acc, "Voting")
# Fusionner les résultats
combined_df <- bind_rows(df_weighted, df_maximization, df_Predomics_Aggre, df_ranking, df_voting)
# Supprimer les lignes où k est k_1
combined_df <- combined_df %>% filter(k != "k_1")
# Afficher le tableau
kable(combined_df, format = "html",
caption = "Tableau des performances des algorithmes avec moyennes et écarts types des précisions de généralisation") %>%
kable_styling(full_width = TRUE)
# Fonction pour extraire, transformer et calculer les statistiques
process_algorithm <- function(data, algorithm_name) {
df <- as.data.frame(data)
df$k <- rownames(df)
df_long <- df %>%
pivot_longer(cols = -k, names_to = "fold", values_to = "accuracy")
df_stats <- df_long %>%
group_by(k) %>%
summarise(
mean_accuracy = mean(accuracy, na.rm = TRUE),
sd_accuracy = sd(accuracy, na.rm = TRUE),
algorithm = algorithm_name,
.groups = 'drop'
)
return(df_stats)
}
# Traiter chaque algorithme
df_weighted <- process_algorithm(Multi_weightedAggregation$crossVal$scores$generalization.acc, "Weighted voting")
df_maximization <- process_algorithm(Multi_maximizationAggregation$crossVal$scores$generalization.acc, "Maximization")
df_Predomics_Aggre <-process_algorithm(Multi_Predomics_Aggregation_ova$crossVal$scores$generalization.acc, "Predomics_Aggregation_ova")
df_ranking <- process_algorithm(Multi_rankingAggregation$crossVal$scores$generalization.acc, "Ranking")
df_voting <- process_algorithm(Multi_votingAggregation$crossVal$scores$generalization.acc, "Voting")
# Fusionner les résultats
combined_df <- bind_rows(df_weighted, df_maximization, df_Predomics_Aggre, df_ranking, df_voting)
# Supprimer les lignes où k est k_1
combined_df <- combined_df %>% filter(k != "k_1")
# Afficher le tableau
kable(combined_df, format = "html",
caption = "Tableau des performances des algorithmes avec moyennes et écarts types des précisions de généralisation") %>%
kable_styling(full_width = TRUE)
knitr::opts_chunk$set(echo = TRUE)
# Rassembler les données pour chaque métrique
accuracy_data <- data.frame(
Model = c("K_2", "K_3", "k_4"),
Metric = "Accuracy",
Value = c(TestmaximizationAggregation_testModels[[1]]$accuracy_,
TestmaximizationAggregation_testModels[[2]]$accuracy_,
TestmaximizationAggregation_testModels[[3]]$accuracy_)
)
knitr::opts_chunk$set(echo = TRUE)
# Fonction pour extraire les données et calculer les écarts types
extract_and_process <- function(data, metric) {
# Convertir en dataframe et retirer les NA
df <- as.data.frame(data[[metric]])
# Ajouter les colonnes k et algorithm
df <- tibble::rownames_to_column(df, "k") %>%
pivot_longer(-k, names_to = "fold", values_to = "accuracy") %>%
group_by(k) %>%
summarise(mean_accuracy = mean(accuracy, na.rm = TRUE),
sd_accuracy = sd(accuracy, na.rm = TRUE),
.groups = 'drop')
return(df)
}
# Extraire les données et calculer les écarts types pour chaque algorithme
df_weighted <- extract_and_process(Unique_weightedAggregation$crossVal$scores, "mean.acc")
df_weighted$algorithm <- "Weighted voting"
df_maximization <- extract_and_process(Unique_maximizationAggregation$crossVal$scores, "mean.acc")
df_maximization$algorithm <- "Maximization"
df_Predomics_Aggre <- extract_and_process(Unique_Predomics_aggregation_ova$crossVal$scores, "mean.acc")
df_Predomics_Aggre$algorithm <- "Predomics_Aggregation_ova"
df_ranking <- extract_and_process(Unique_rankingAggregation$crossVal$scores, "mean.acc")
df_ranking$algorithm <- "Ranking"
df_voting <- extract_and_process(Unique_votingAggregation$crossVal$scores, "mean.acc")
df_voting$algorithm <- "Voting"
# Fusionner toutes les données
combined_df <- bind_rows(df_weighted, df_maximization, df_Predomics_Aggre, df_ranking, df_voting)
# Créer le graphique avec les barres d'erreur
ggplot(combined_df, aes(x = k, y = mean_accuracy, color = algorithm)) +
geom_point(size = 3) +
geom_line(aes(group = algorithm, linetype = algorithm)) +
geom_errorbar(aes(ymin = mean_accuracy - sd_accuracy, ymax = mean_accuracy + sd_accuracy), width = 0.2) +
geom_text(aes(label = round(mean_accuracy, 3)), vjust = -0.5, size = 3) +  # Ajouter les valeurs à côté des points
facet_wrap(~ algorithm, ncol = 2) +  # Afficher deux graphiques par ligne
theme_minimal() +
labs(title = "Mean Accuracy with Standard Deviation for Cross-Validation",
x = "Valeur de k",
y = "Mean Accuracy",
color = "Algorithm") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
scale_color_manual(values = c("Weighted voting" = "#377EB8",
"Maximization" = "#E41A1C",
"Predomics_Aggregation_ova" = "#4DAF4A",
"Ranking" = "#FF7F00",
"Voting" = "#6A3D9A"))
knitr::opts_chunk$set(echo = TRUE)
extract_and_process <- function(data, metric) {
# Convertir en dataframe et retirer les NA
df <- as.data.frame(data[[metric]])
# Ajouter les colonnes k et algorithm
df <- tibble::rownames_to_column(df, "k") %>%
pivot_longer(-k, names_to = "fold", values_to = "accuracy") %>%
group_by(k) %>%
summarise(mean_accuracy = mean(accuracy, na.rm = TRUE),
sd_accuracy = sd(accuracy, na.rm = TRUE),
.groups = 'drop')
return(df)
}
# Extraire et calculer les statistiques pour chaque algorithme
df_weighted <- extract_and_process(Unique_weightedAggregation$crossVal$scores, "mean.acc")
df_weighted$algorithm <- "Weighted voting"
df_maximization <- extract_and_process(Unique_maximizationAggregation$crossVal$scores, "mean.acc")
df_maximization$algorithm <- "Maximization"
df_Predomics_Aggre <- extract_and_process(Unique_Predomics_aggregation_ova$crossVal$scores, "mean.acc")
df_Predomics_Aggre$algorithm <- "Predomics_Aggregation_ova"
df_ranking <- extract_and_process(Unique_rankingAggregation$crossVal$scores, "mean.acc")
df_ranking$algorithm <- "Ranking"
df_voting <- extract_and_process(Unique_votingAggregation$crossVal$scores, "mean.acc")
df_voting$algorithm <- "Voting"
# Fusionner toutes les données
combined_df <- bind_rows(df_weighted, df_maximization, df_Predomics_Aggre, df_ranking, df_voting)
# Nettoyer les données en supprimant les lignes avec NaN ou NA pour mean_accuracy
cleaned_df <- combined_df %>%
filter(!is.nan(mean_accuracy) & !is.na(mean_accuracy))
# Créer le graphique avec les barres d'erreur
ggplot(cleaned_df, aes(x = k, y = mean_accuracy, color = algorithm)) +
geom_point(size = 3) +
geom_line(aes(group = algorithm, linetype = algorithm)) +
geom_errorbar(aes(ymin = mean_accuracy - sd_accuracy, ymax = mean_accuracy + sd_accuracy),
width = 0.5,  # Largeur des barres d'erreur
size = 0.8) +  # Épaisseur des barres d'erreur
geom_text(aes(label = round(mean_accuracy, 3)), vjust = -0.5, size = 3) +  # Ajouter les valeurs à côté des points
facet_wrap(~ algorithm, ncol = 2) +  # Afficher deux graphiques par ligne
theme_minimal() +
labs(title = "Mean Accuracy with Standard Deviation for Cross-Validation",
x = "Valeur de k",
y = "Mean Accuracy",
color = "Algorithm") +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
scale_color_manual(values = c("Weighted voting" = "#377EB8",
"Maximization" = "#E41A1C",
"Predomics_Aggregation_ova" = "#4DAF4A",
"Ranking" = "#FF7F00",
"Voting" = "#6A3D9A"))
knitr::opts_chunk$set(echo = TRUE)
# Extrait les données de généralisation
generalization_acc <- Multi_Predomics_aggregation_ovo$crossVal$scores$generalization.acc
# Convertir les données en dataframe en conservant les noms de lignes comme une colonne
df <- as.data.frame(generalization_acc)
df$k <- rownames(df)
# Réorganiser les données pour le calcul
df_long <- df %>%
pivot_longer(cols = -k, names_to = "fold", values_to = "accuracy")
# Calculer la moyenne et l'écart type en ignorant les NA
df_stats <- df_long %>%
group_by(k) %>%
summarise(
mean_accuracy = mean(accuracy, na.rm = TRUE),
sd_accuracy = sd(accuracy, na.rm = TRUE),
.groups = 'drop'
)
# Afficher le résultat
print(df_stats)
# Fonction pour extraire, transformer et calculer les statistiques
process_algorithm <- function(data, algorithm_name) {
df <- as.data.frame(data)
df$k <- rownames(df)
df_long <- df %>%
pivot_longer(cols = -k, names_to = "fold", values_to = "accuracy")
df_stats <- df_long %>%
group_by(k) %>%
summarise(
mean_accuracy = mean(accuracy, na.rm = TRUE),
sd_accuracy = sd(accuracy, na.rm = TRUE),
algorithm = algorithm_name,
.groups = 'drop'
)
return(df_stats)
}
# Traiter chaque algorithme
df_weighted <- process_algorithm(Multi_weightedAggregation$crossVal$scores$generalization.acc, "Weighted voting")
df_maximization <- process_algorithm(Multi_maximizationAggregation$crossVal$scores$generalization.acc, "Maximization")
df_Predomics_Aggre <-process_algorithm(Multi_Predomics_Aggregation_ova$crossVal$scores$generalization.acc, "Predomics_Aggregation_ova")
df_ranking <- process_algorithm(Multi_rankingAggregation$crossVal$scores$generalization.acc, "Ranking")
df_voting <- process_algorithm(Multi_votingAggregation$crossVal$scores$generalization.acc, "Voting")
# Fusionner les résultats
combined_df <- bind_rows(df_weighted, df_maximization, df_Predomics_Aggre, df_ranking, df_voting)
# Supprimer les lignes où k est k_1
combined_df <- combined_df %>% filter(k != "k_1")
# Afficher le tableau
kable(combined_df, format = "html",
caption = "Tableau des performances des algorithmes avec moyennes et écarts types des précisions de généralisation") %>%
kable_styling(full_width = TRUE)
knitr::opts_chunk$set(echo = TRUE)
# Fixer une graine aléatoire pour assurer la reproductibilité
set.seed(123)
# Préparation des données
X_train <- t(X)
y_train <- as.factor(y)
X_test <- t(X.test)
y_test <- as.factor(y.test)
# Définir la validation croisée à 10 plis
train_control <- trainControl(method = "cv", number = 10, savePredictions = TRUE)
# Entraîner le modèle Random Forest avec validation croisée à 10 plis
rf_model <- train(x = X_train, y = y_train, method = "rf", trControl = train_control)
# Prédictions sur les données d'entraînement
train_pred <- predict(rf_model, X_train)
train_metrics <- confusionMatrix(train_pred, y_train)
# Calcul de l'AUC pour les données d'entraînement
roc_train <- multiclass.roc(y_train, as.numeric(train_pred))
auc_train <- auc(roc_train)
# Affichage des métriques pour les données d'entraînement
print(train_metrics)
print(auc_train)
# Prédictions sur les données de test
test_pred <- predict(rf_model, X_test)
test_metrics <- confusionMatrix(test_pred, y_test)
# Calcul de l'AUC pour les données de test
roc_test <- multiclass.roc(y_test, as.numeric(test_pred))
auc_test <- auc(roc_test)
# Affichage des métriques pour les données de test
print(test_metrics)
print(auc_test)
# Calcul des métriques pour l'entraînement et le test
train_accuracy <- train_metrics$overall["Accuracy"]
train_precision <- mean(train_metrics$byClass[,"Precision"], na.rm = TRUE)
train_recall <- mean(train_metrics$byClass[,"Recall"], na.rm = TRUE)
f1_train <- 2 * (train_precision * train_recall) / (train_precision + train_recall)
test_accuracy <- test_metrics$overall["Accuracy"]
test_precision <- mean(test_metrics$byClass[,"Precision"], na.rm = TRUE)
test_recall <- mean(test_metrics$byClass[,"Recall"], na.rm = TRUE)
f1_test <- 2 * (test_precision * test_recall) / (test_precision + test_recall)
# Calcul de l'écart type des métriques des 10 folds de la validation croisée
accuracy_sd <- sd(rf_model$resample$Accuracy)
precision_sd <- sd(rf_model$resample$Precision, na.rm = TRUE)
recall_sd <- sd(rf_model$resample$Recall, na.rm = TRUE)
f1_sd <- sd(rf_model$resample$F1, na.rm = TRUE)
# Affichage de l'écart type des métriques
print(paste("Ecart type de l'Accuracy:", accuracy_sd))
print(paste("Ecart type de la Précision:", precision_sd))
print(paste("Ecart type du Rappel:", recall_sd))
print(paste("Ecart type du F1 Score:", f1_sd))
# Création des dataframes pour les métriques d'entraînement et de test
train_metrics_df <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(train_accuracy, train_precision, f1_train, train_recall),
Dataset = "Training"
)
test_metrics_df <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(test_accuracy, test_precision, f1_test, test_recall),
Dataset = "Test"
)
# Combiner les métriques d'entraînement et de test
all_metrics <- rbind(train_metrics_df, test_metrics_df)
# Création du graphique comparant les métriques d'entraînement et de test
plot <- ggplot(all_metrics, aes(x = Metric, y = Value, fill = Dataset)) +
geom_bar(stat = "identity", position = "dodge") +
geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
labs(title = "Comparison of test and training metrics - Random Forest",
x = "Metrics", y = "Values") +
theme_minimal()
# Affichage du graphique
print(plot)
# Fixer une graine aléatoire pour assurer la reproductibilité
set.seed(123)  # Choisissez n'importe quel nombre
# Préparation des données
df <- as.data.frame(t(X))
df$y <- factor(y)
# Vérifiez la répartition des classes
class_distribution <- table(df$y)
print(class_distribution)
# Filtrer les classes avec suffisamment d'observations (par exemple, au moins 5)
min_observations <- 5
valid_classes <- names(class_distribution[class_distribution >= min_observations])
df_filtered <- df[df$y %in% valid_classes, ]
df_filtered$y <- factor(df_filtered$y)
# Vérification après filtrage
print(table(df_filtered$y))
# Répartition des données pour la validation croisée manuelle
set.seed(123)  # Pour la reproductibilité
folds <- createFolds(df_filtered$y, k = 10, list = TRUE)
# Initialiser un vecteur pour stocker les précisions de chaque fold
fold_accuracies <- numeric(length(folds))
# Calculer l'accuracy pour chaque fold
for (i in seq_along(folds)) {
# Indices des données de validation pour ce fold
val_indices <- folds[[i]]
# Données d'entraînement pour ce fold
train_data <- df_filtered[-val_indices, ]
# Données de validation pour ce fold
val_data <- df_filtered[val_indices, ]
# Entraînement du modèle sur les données d'entraînement du fold
model_fold <- glmnet(as.matrix(train_data[, -ncol(train_data)]), train_data$y, family = "multinomial", alpha = 1)
# Prédictions pour les données de validation du fold
fold_pred <- predict(model_fold, newx = as.matrix(val_data[, -ncol(val_data)]), type = "class", s = model_fold$lambda.min)
# Convertir les prédictions en facteur avec les mêmes niveaux que les données originales
fold_pred <- factor(fold_pred, levels = levels(df_filtered$y))
# Vérité terrain pour ce fold
fold_truth <- val_data$y
# Calculer l'accuracy pour ce fold
fold_accuracies[i] <- mean(fold_pred == fold_truth)
}
# Calcul de la moyenne des accuracies
mean_accuracy <- mean(fold_accuracies, na.rm = TRUE)
# Calcul de l'écart type des accuracies
sd_accuracy <- sd(fold_accuracies, na.rm = TRUE)
# Afficher les résultats de validation croisée
print(paste("Cross-Validation Mean Accuracy:", round(mean_accuracy, 3)))
print(paste("Cross-Validation Accuracy Standard Deviation:", round(sd_accuracy, 3)))
# Création du modèle final sur l'ensemble des données d'entraînement
best_model_rl <- cv.glmnet(as.matrix(df_filtered[, -ncol(df_filtered)]), df_filtered$y, family = "multinomial", alpha = 1, nfolds = 10)
# Prédictions sur les données d'entraînement complètes
train_predictions <- predict(best_model_rl, newx = as.matrix(df_filtered[, -ncol(df_filtered)]), s = best_model_rl$lambda.min, type = "class")
train_predictions <- factor(train_predictions, levels = levels(df_filtered$y))
# Création de la matrice de confusion pour les données d'entraînement
conf_matrix_train_rl <- confusionMatrix(train_predictions, df_filtered$y)
# Calcul des métriques pour les données d'entraînement
accuracy_train_rl <- conf_matrix_train_rl$overall[["Accuracy"]]
precision_train_rl <- mean(conf_matrix_train_rl$byClass[ , "Precision"], na.rm = TRUE)
recall_train_rl <- mean(conf_matrix_train_rl$byClass[ , "Sensitivity"], na.rm = TRUE)
AUC_train_rl <- mean(conf_matrix_train_rl$byClass[ , "Balanced Accuracy"], na.rm = TRUE)
F1_train_rl <- 2 * (precision_train_rl * recall_train_rl) / (precision_train_rl + recall_train_rl)
# Affichage des métriques pour les données d'entraînement
print(paste("Train Accuracy:", accuracy_train_rl))
print(paste("Train Precision:", precision_train_rl))
print(paste("Train Recall (Sensitivity):", recall_train_rl))
print(paste("Train AUC (Balanced Accuracy):", AUC_train_rl))
print(paste("Train F1 Score:", F1_train_rl))
# Prédictions sur les données de test
predictions_test1_rl <- predict(best_model_rl, newx = as.matrix(X_test), s = best_model_rl$lambda.min, type = "class")
predictions_test1_rl <- factor(predictions_test1_rl, levels = levels(df_filtered$y))
# Création de la matrice de confusion pour les données de test
conf_matrix_test_rl <- confusionMatrix(predictions_test1_rl, y_test)
# Calcul des métriques pour les données de test
accuracy_test1_rl <- conf_matrix_test_rl$overall[["Accuracy"]]
precision_test1_rl <- mean(conf_matrix_test_rl$byClass[ , "Precision"], na.rm = TRUE)
recall_test1_rl <- mean(conf_matrix_test_rl$byClass[ , "Sensitivity"], na.rm = TRUE)
AUC_test1_rl <- mean(conf_matrix_test_rl$byClass[ , "Balanced Accuracy"], na.rm = TRUE)
F1_test1_rl <- 2 * (precision_test1_rl * recall_test1_rl) / (precision_test1_rl + recall_test1_rl)
# Création du data frame pour les métriques d'entraînement
train_metrics_rl <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_train_rl, precision_train_rl, F1_train_rl, recall_train_rl),
Dataset = "Training"
)
# Création du data frame pour les métriques de test
test_metrics_rl <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_test1_rl, precision_test1_rl, F1_test1_rl, recall_test1_rl),
Dataset = "Test"
)
# Combinaison des métriques d'entraînement et de test
all_metrics_rl <- rbind(test_metrics_rl, train_metrics_rl)
# Création du graphique comparant les métriques de test et d'entraînement
plot <- ggplot(all_metrics_rl, aes(x = Metric, y = Value, fill = Dataset)) +
geom_bar(stat = "identity", position = "dodge") +
geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
labs(title = "Comparison of Test and Training Metrics - Logistic Regression",
x = "Metrics", y = "Values") +
theme_minimal()
# Affichage du graphique
print(plot)
# Préparer les données pour la validation croisée
set.seed(123)  # Pour la reproductibilité
folds <- createFolds(y_train, k = 10, list = TRUE, returnTrain = TRUE)
# Initialiser des vecteurs pour stocker les métriques
fold_accuracies <- numeric(length(folds))
fold_precisions <- matrix(0, nrow = length(folds), ncol = length(levels(y_train)))
fold_recalls <- matrix(0, nrow = length(folds), ncol = length(levels(y_train)))
fold_f1_scores <- matrix(0, nrow = length(folds), ncol = length(levels(y_train)))
# Calculer les métriques pour chaque fold
for (i in seq_along(folds)) {
train_indices <- folds[[i]]
val_indices <- which(!1:nrow(X_train) %in% train_indices)
train_data <- data.frame(x = X_train[train_indices, ], y = y_train[train_indices])
val_data <- data.frame(x = X_train[val_indices, ], y = y_train[val_indices])
# Entraînement du modèle SVM pour ce fold
svm_model_fold <- train(x = train_data[, -ncol(train_data)], y = train_data$y, method = "svmRadial", trControl = trainControl(method = "none"))
# Prédictions pour les données de validation
predictions_val <- predict(svm_model_fold, newdata = val_data[, -ncol(val_data)])
# Calculer les métriques pour ce fold
conf_matrix <- confusionMatrix(predictions_val, val_data$y)
fold_accuracies[i] <- conf_matrix$overall['Accuracy']
# Assurez-vous d'utiliser les bons noms de colonnes
if (!is.null(conf_matrix$byClass)) {
precision_values <- conf_matrix$byClass[, 'Pos Pred Value']
recall_values <- conf_matrix$byClass[, 'Sensitivity']
# S'assurer que les dimensions sont correctes
if (length(precision_values) == ncol(fold_precisions) && length(recall_values) == ncol(fold_recalls)) {
fold_precisions[i, 1:length(precision_values)] <- precision_values
fold_recalls[i, 1:length(recall_values)] <- recall_values
fold_f1_scores[i, 1:length(precision_values)] <- 2 * (precision_values * recall_values) / (precision_values + recall_values)
}
}
}
# Calcul de la moyenne et de l'écart type pour les metrics
mean_accuracy <- mean(fold_accuracies, na.rm = TRUE)
sd_accuracy <- sd(fold_accuracies, na.rm = TRUE)
# Entraînement du modèle SVM avec validation croisée complète
svm_model <- train(x = X_train, y = y_train, method = "svmRadial", trControl = trainControl(method = "cv", number = 10))
# Prédictions avec le modèle SVM
svm_predictions_train <- predict(svm_model, X_train)
svm_predictions_test <- predict(svm_model, X_test)
# Calculer les métriques pour les ensembles d'entraînement et de test
conf_matrix_train_svm <- confusionMatrix(data = svm_predictions_train, reference = y_train)
conf_matrix_test_svm <- confusionMatrix(data = svm_predictions_test, reference = y_test)
accuracy_train_svm <- conf_matrix_train_svm$overall['Accuracy']
accuracy_test_svm <- conf_matrix_test_svm$overall['Accuracy']
precision_train_svm <- mean(conf_matrix_train_svm$byClass[, 'Pos Pred Value'], na.rm = TRUE)
precision_test_svm <- mean(conf_matrix_test_svm$byClass[, 'Pos Pred Value'], na.rm = TRUE)
recall_train_svm <- mean(conf_matrix_train_svm$byClass[, 'Sensitivity'], na.rm = TRUE)
recall_test_svm <- mean(conf_matrix_test_svm$byClass[, 'Sensitivity'], na.rm = TRUE)
F1_train_svm <- mean(2 * (precision_train_svm * recall_train_svm) / (precision_train_svm + recall_train_svm), na.rm = TRUE)
F1_test_svm <- mean(2 * (precision_test_svm * recall_test_svm) / (precision_test_svm + recall_test_svm), na.rm = TRUE)
# Calculer l'AUC
auc_train_svm <- roc(y_train, as.numeric(svm_predictions_train))$auc
auc_test_svm <- roc(y_test, as.numeric(svm_predictions_test))$auc
# Créer un data frame pour les métriques d'entraînement
train_metrics_svm <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_train_svm, precision_train_svm, F1_train_svm, recall_train_svm),
Dataset = "Training"
)
# Créer un data frame pour les métriques de test
test_metrics_svm <- data.frame(
Metric = c("Accuracy", "Precision", "F1", "Recall"),
Value = c(accuracy_test_svm, precision_test_svm, F1_test_svm, recall_test_svm),
Dataset = "Test"
)
# Combiner les métriques d'entraînement et de test
all_metrics_svm <- rbind(test_metrics_svm, train_metrics_svm)
# Créer le graphique comparant les métriques de test et d'entraînement
plot <- ggplot(all_metrics_svm, aes(x = Metric, y = Value, fill = Dataset)) +
geom_bar(stat = "identity", position = "dodge") +
geom_text(aes(label = round(Value, 3)), position = position_dodge(width = 1), vjust = -0.5) +
labs(title = "Comparison of Test and Training Metrics - SVM",
x = "Metrics", y = "Values") +
theme_minimal()
# Afficher le graphique
print(plot)
# Afficher les résultats de validation croisée
print(paste("Cross-Validation Mean Accuracy:", round(mean_accuracy, 3)))
print(paste("Cross-Validation Accuracy Standard Deviation:", round(sd_accuracy, 3)))
