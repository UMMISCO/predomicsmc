listXmin <- clf$data$X.min
listXmax <- clf$data$X.max
listy <- clf$data$y
list_y <- list() #  List of different combinations of y
list_X <- list() #  List of different combinations of X
combi <- generate_combinations_with_factors(y, X, approch = approch)
list_y <- combi$list_y
list_X <- combi$list_X
for (i in 1:length(list_X)) {
clf$coeffs_ <- listcoeffs[[i]]
clf$data$X <- listX[[i]]
clf$data$X.min <- listXmin[[i]]
clf$data$X.max <- listXmax[[i]]
clf$data$y <- listy[[i]]
list_evolved_pop[[i]] <- evolve(X = list_X[[i]], y = list_y[[i]], clf, pop, seed = NULL)
}
evolved_pop <- list_evolved_pop
return(evolved_pop)
}
library(mcpredomics)
knitr::opts_chunk$set(echo = TRUE)
# Package multi class predomics
library(mcpredomics)
# Package predomics
library(predomics)
# Visualization library for creating complex plots
library(ggplot2)
# Arranges multiple ggplot objects on a single page
library(gridExtra)
# ROC curve analysis and AUC calculation
library(pROC)
# Reshaping and melting data frames
library(reshape2)
# Implementation of the Random Forest algorithm for classification and regression
library(randomForest)
# Comprehensive library for classification and regression training
library(caret)
# Various R programming tools and functions, including data manipulation
library(gtools)
# Adding statistical comparisons and publication-ready visualizations
library(ggpubr)
# Data manipulation and transformation (part of the tidyverse)
library(dplyr)
# Tidying messy data by gathering and spreading
library(tidyr)
# Enhanced data frames with row names as a column (tibble format)
library(tibble)
# Dynamic report generation and displaying results in tables
library(knitr)
# Creating aesthetically pleasing and customizable HTML tables
library(kableExtra)
# Interactive tables for data visualization and exploration
library(DT)
# Functions for statistical learning, including SVM and Naive Bayes
library(e1071)
# Lasso and ridge regression via generalized linear models
library(glmnet)
# Reading data from files (including CSV and text files)
library(readr)
# String manipulation and regular expression functions
library(stringr)
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
# Filter null values
X_general <- mc.input$X[, colSums(mc.input$X) != 0]
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE)
set.seed(42)
y = as.vector(yvec)
X = X_general
# Determine the number of samples per class
nombre_echantillons_par_classe <- min(table(y))
# Function to balance classes and maintain order
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe, seed = 123) {
classes <- unique(y)
indices_equilibres <- integer(0)
for (classe in classes) {
indices_classe <- which(y == classe)
set.seed(seed)
indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
}
# Sort balanced indices to maintain original order
indices_equilibres <- sort(indices_equilibres)
return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}
# Get balanced data
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X
# Check distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)
# Split balanced data into 80% for training and 20% for testing
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[, indices_division, drop = FALSE]
X.test <- X_equilibre[, -indices_division, drop = FALSE]
table(y)
dim(X)
clf <- terBeam_mc(sparsity = c(10),
max.nb.features = 1000,
seed = 1,
nCores = 1,
evalToFit = "accuracy_",
objective = "auc",
experiment.id = "terBeam_mc",
experiment.save = "nothing")
printy(clf)
load("~/Documents/multiclasse_predomics/mcpredomics/Analysis_Final_Results_DF_05_2025.rda")
colnames(Analysis_Final_Results_DF_05_2025)
table(Analysis_Final_Results_DF_05_2025$Methods)
table(Analysis_Final_Results_DF_05_2025$Dataset)
table(Analysis_Final_Results_DF_05_2025$Set)
table(Analysis_Final_Results_DF_05_2025$Constraint_factor)
library(ggplot2)
library(dplyr)
# Moyennes et écarts-types par méthode
Analysis_Final_Results_DF_05_2025 %>%
group_by(Methods) %>%
summarise(Accuracy_mean = mean(Accuracy),
Accuracy_sd = sd(Accuracy),
F1_mean = mean(F1),
F1_sd = sd(F1)) %>%
arrange(desc(Accuracy_mean))
# Filtrage pour ne garder que les résultats du jeu de test
Analysis_Final_Results_DF_05_2025 %>%
filter(Set == "Test") %>%                 # On garde uniquement les lignes "Test"
group_by(Methods) %>%
summarise(
Accuracy_mean = mean(Accuracy),         # Moyenne de l'accuracy
Accuracy_sd   = sd(Accuracy),           # Écart-type de l'accuracy
F1_mean       = mean(F1),               # Moyenne du F1-score (optionnel ici)
F1_sd         = sd(F1)                  # Écart-type du F1-score (optionnel ici)
) %>%
arrange(desc(Accuracy_mean))              # Classement décroissant par accuracy
library(dplyr)
# Calcul des moyennes et écarts-types par méthode et dataset pour les résultats de test uniquement
results_summary <- Analysis_Final_Results_DF_05_2025 %>%
filter(Set == "Test") %>%                            # Filtrer uniquement le jeu de test
group_by(Dataset, Methods) %>%                       # Grouper par dataset et méthode
summarise(
Accuracy_mean = mean(Accuracy),
Accuracy_sd   = sd(Accuracy),
F1_mean       = mean(F1),
F1_sd         = sd(F1),
.groups = "drop"
) %>%
arrange(Dataset, desc(Accuracy_mean))                # Trier par dataset et accuracy décroissante
# Afficher les résultats
print(results_summary)
Analysis_Final_Results_DF_05_2025$Constraint_factor
library(dplyr)
# Résumé des métriques pour Set == "Test", groupé par Dataset, Methods et Constraint_factor
results_summary_constraints <- Analysis_Final_Results_DF_05_2025 %>%
filter(Set == "Test") %>%                              # On garde uniquement les résultats du jeu de test
group_by(Dataset, Methods, Constraint_factor) %>%      # On groupe selon Dataset, Méthode et Facteur de Contrainte
summarise(
Accuracy_mean = mean(Accuracy),
Accuracy_sd   = sd(Accuracy),
F1_mean       = mean(F1),
F1_sd         = sd(F1),
.groups = "drop"
) %>%
arrange(Dataset, Constraint_factor, desc(Accuracy_mean))  # Tri des résultats
# Afficher le tableau résumé
print(results_summary_constraints)
results_summary_constraints
results_summary_constraints
runit = TRUE
if(runit)
{
res_clf <- fit_mc(X = X, y = y, clf = clf,approch="ovo", cross.validate = TRUE,aggregation_ = "voting", nfolds = 10, constraint_factor = "full_constrained");
save(res_clf , clf, file ="res_clf .rda", compression_level = 9)
}
runit = TRUE
if(runit)
{
res_clf <- fit_mc(X = X, y = y, clf = clf,approch="ovo", cross.validate = TRUE,aggregation_ = "voting", nfolds = 10, constraint_factor = "fully_constrained");
save(res_clf , clf, file ="res_clf .rda", compression_level = 9)
}
res_clf$crossVal$scores$empirical.acc
res_clf$crossVal$scores$generalization.acc
res_clf$crossVal$scores$mean.acc
Analysis_Final_Results_DF_05_2025$Accuracy
# Convert the model collection into a population of models scrambled by model size
pop <- modelCollectionToPopulation(res_clf$classifier$models)
printy_mc(pop)
# Function to create boxplot for a given data frame
create_boxplot <- function(data, title) {
# Melt the dataframe for ggplot
data.melt <- melt(data, id.vars = c("accuracy_", "eval.sparsity"))
# Create ggplot
plot <- ggplot(data = data.melt, aes(y = accuracy_, x = eval.sparsity)) +
geom_boxplot(notch = FALSE, outlier.shape = NA, position = position_dodge(width = 0.9), alpha = 0.3) +
geom_point(aes(color = eval.sparsity), position = position_jitterdodge(dodge.width = 0.9), size = 1, alpha = 0.5) +
ylim(c(0, 1)) +
xlab("Model Parsimony") +
ggtitle(title) +
theme_bw() +
theme(legend.position = "bottom", legend.direction = "horizontal") +
guides(colour = "none")
return(plot)
}
# Convert the population to a data frame
pop.df <- populationToDataFrame_mc(pop)
# Plotting for the original population (single figure)
pop.dff <- as.data.frame(pop.df[[1]])  # Convert the first submodel to a data frame
g.before <- create_boxplot(pop.dff, title = "Original Population")
#print(g.before)  # Display the plot
# Select the best population models
fbm <- selectBestPopulation(pop)
#printy_mc(fbm)
# Convert the best population models to a data frame
fbm.df <- populationToDataFrame_mc(fbm)
# Plotting for the selected best models (single figure)
fbm.dff <- as.data.frame(fbm.df[[1]])  # Convert the first submodel to a data frame
g.after <- create_boxplot(fbm.dff, title = "FBM")
print(g.after)  # Display the plot
fa <- makeFeatureAnnot_mc(pop = fbm,
X = X,
y = y,
clf = clf,
approch = "ovo")
plot_distribution <- plotFeatureModelCoeffs_mc(feat.model.coeffs = fa, y, approch = "ovo")
plots_abundance <- plotAbundanceByClass_mc(features = fa, X, y, approch = "ovo")
plots_prevalence <- plotPrevalence_mc(features = fa, X = X, y = y, approch = "ovo")
feat1.import <- mergeMeltImportanceCV_mc(list.results = list(terBeam = res_clf),
filter.cv.prev = 0,
min.kfold.nb = FALSE,
learner.grep.pattern = "*",
#nb.top.features = 50,
feature.selection = fa,
scaled.importance = TRUE,
make.plot = TRUE,
cv.prevalence = FALSE)
feat2.import <- mergeMeltImportanceCV_mc(list.results = list(terBeam = res_clf),
filter.cv.prev = 0,
min.kfold.nb = FALSE,
learner.grep.pattern = "*",
nb.top.features = 148,
#feature.selection = rownames(fa$pop.noz),
scaled.importance = TRUE,
make.plot = TRUE,
cv.prevalence = FALSE)
# get the best model
best.model = fbm[[1]]
# Visualize the model information (if needed)
printy_mc(best.model)
# Generate the plots
plots1 <- plotModel_mcc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ovo")
# get the best model
best.model = fbm[[1]]
# Visualize the model information (if needed)
printy_mc(best.model)
# Generate the plots
plots1 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, approch = "ovo")
plots2 <- plotModel_mc(best.model, X, y, sort.features = FALSE, feature.name = TRUE, importance = TRUE, approch = "ovo")
# Open a PDF device to save the grid of plots
pdf("model_visualization_plots2.pdf", width = 18, height = 12)  # Adjust width and height as needed
# Extract the plots from each list and pass them to `grid.arrange`
grid.arrange(grobs = c(plots1, plots2), ncol = 6)
# Close the PDF device
dev.off()
# Confirmation of the save process
cat("The plots have been saved under the name 'model_visualization_plots2.pdf'.\n")
tmp <- plotAUC_mc(scores = best.model$score_, y = y, percent = TRUE, approch = "ovo"); rm(tmp)
clf <- terga1_mc(nCores = 1,
seed = 1,
plot = TRUE
)
printy(clf)
runit = TRUE
if(runit)
{
res_clf2 <- fit_mc(X = X, y = y, clf = clf,approch="ovo", cross.validate = TRUE,aggregation_ = "voting", nfolds = 10, constraint_factor = "fully_constrained");
save(res_clf2 , clf, file ="res_clf2.rda", compression_level = 9)
}
res_clf2$crossVal$scores$generalization.acc
res_clf2$crossVal$scores$mean.acc
res_clf2$crossVal$scores$empirical.acc
load("~/Documents/multiclasse_predomics/mcpredomics/Analysis_Final_Results_DF_05_2025.rda")
colnames(Analysis_Final_Results_DF_05_2025)
table(Analysis_Final_Results_DF_05_2025$Dataset)
table(Analysis_Final_Results_DF_05_2025$Constraint_factor)
table(Analysis_Final_Results_DF_05_2025$Approach)
table(Analysis_Final_Results_DF_05_2025$Methods)
table(Analysis_Final_Results_DF_05_2025$Set)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
# Filtrer les données pour le set Test
df_test <- subset(Analysis_Final_Results_DF_05_2025, Set == "Test")
# Plot avec boxplot + points des folds pour visualiser la variation
ggplot(df_test, aes(x = Methods, y = Accuracy, fill = Dataset)) +
geom_boxplot(alpha = 0.6, outlier.shape = NA) +  # boxplot sans outliers
geom_jitter(aes(color = Constraint_factor), width = 0.2, size = 1.5, alpha = 0.8) + # points dispersés
facet_wrap(~ Dataset) + # séparer par dataset (optionnel, à retirer si on garde fill = Dataset)
theme_bw() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +  # rotation des labels méthodes
labs(
title = "Distribution de l'Accuracy en test selon les méthodes et datasets",
x = "Méthode",
y = "Accuracy",
fill = "Dataset",
color = "Constraint Factor"
)
library(ggplot2)
df_test <- subset(Analysis_Final_Results_DF_05_2025, Set == "Test")
ggplot(df_test, aes(x = Methods, y = Accuracy, fill = Dataset)) +
geom_boxplot(alpha = 0.6, outlier.shape = NA, position = position_dodge(width = 0.8)) +
geom_jitter(aes(color = Dataset), width = 0.2, size = 1.5, alpha = 0.8, position = position_dodge(width = 0.8)) +
facet_wrap(~ Constraint_factor, scales = "free_x") +   # un plot par contrainte
theme_bw() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(
title = "Accuracy en test selon les méthodes, datasets et facteurs de contrainte",
x = "Méthode",
y = "Accuracy",
fill = "Dataset",
color = "Dataset"
)
library(ggplot2)
df_test <- subset(Analysis_Final_Results_DF_05_2025, Set == "Test")
ggplot(df_test, aes(x = Methods, y = Accuracy, fill = Dataset)) +
geom_boxplot(alpha = 0.6, outlier.shape = NA, position = position_dodge(width = 0.8)) +
geom_jitter(aes(color = Dataset),
position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.8),
size = 1.5, alpha = 0.8) +
facet_wrap(~ Constraint_factor, scales = "free_x") +
theme_bw() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(
title = "Accuracy en test selon les méthodes, datasets et facteurs de contrainte",
x = "Méthode",
y = "Accuracy",
fill = "Dataset",
color = "Dataset"
)
library(ggplot2)
df_test <- subset(Analysis_Final_Results_DF_05_2025, Set == "Test")
ggplot(df_test, aes(x = Methods, y = Accuracy, fill = Dataset)) +
geom_boxplot(alpha = 0.6, outlier.shape = NA, position = position_dodge(width = 0.8)) +
geom_jitter(aes(color = Dataset),
position = position_jitterdodge(jitter.width = 0.2, dodge.width = 0.8),
size = 1.5, alpha = 0.8) +
facet_grid(Approach ~ Constraint_factor, scales = "free_x", space = "free_x") +
theme_bw() +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
strip.text.y = element_text(angle = 0) # pour mieux lire les facettes verticales
) +
labs(
title = "Accuracy en test selon les méthodes, datasets, approche et contraintes",
x = "Méthode",
y = "Accuracy",
fill = "Dataset",
color = "Dataset"
)
head(
subset(
Analysis_Final_Results_DF_05_2025,
Constraint_factor == "Full_Constrained" &
Approach == "Terga1 Predomics" &
Dataset == "Balanced Enterotype"
),
10
)
# Sous-ensemble des données ciblées
df_sub <- subset(
Analysis_Final_Results_DF_05_2025,
Constraint_factor == "Full_Constrained" &
Approach == "Terga1 Predomics" &
Dataset == "Balanced Enterotype"
)
# Sélectionner uniquement les colonnes numériques
numeric_cols <- sapply(df_sub, is.numeric)
# Calculer la moyenne par colonne numérique
means <- sapply(df_sub[, numeric_cols], mean, na.rm = TRUE)
# Calculer l'écart-type par colonne numérique
sds <- sapply(df_sub[, numeric_cols], sd, na.rm = TRUE)
# Combiner dans un data.frame pour mieux voir
summary_stats <- data.frame(
Variable = names(means),
Mean = means,
Std_Dev = sds
)
print(summary_stats)
library(dplyr)
library(gridExtra)   # Pour afficher tableau dans PDF
library(grid)
library(ggplot2)
# Filtrer les données test
df_test <- Analysis_Final_Results_DF_05_2025 %>% filter(Set == "Test")
# Calculer mean et sd groupé
summary_stats <- df_test %>%
group_by(Dataset, Constraint_factor, Approach, Methods) %>%
summarise(
Mean_Accuracy = mean(Accuracy, na.rm = TRUE),
SD_Accuracy = sd(Accuracy, na.rm = TRUE),
Mean_Recall = mean(Recall, na.rm = TRUE),
SD_Recall = sd(Recall, na.rm = TRUE),
Mean_Precision = mean(Precision, na.rm = TRUE),
SD_Precision = sd(Precision, na.rm = TRUE),
Mean_F1 = mean(F1, na.rm = TRUE),
SD_F1 = sd(F1, na.rm = TRUE),
.groups = "drop"
)
# Afficher un aperçu
print(head(summary_stats))
# Préparer une table pour gridExtra (facultatif, mais joli pour PDF)
table_grob <- tableGrob(summary_stats)
# Exporter en PDF
pdf("Summary_Stats_Test_Set.pdf", width = 14, height = 8)
grid.newpage()
grid.draw(table_grob)
dev.off()
library(dplyr)
library(gridExtra)
df_test <- Analysis_Final_Results_DF_05_2025 %>% filter(Set == "Test")
# Liste des méthodes uniques
methods_list <- unique(df_test$Methods)
for (method in methods_list) {
cat("\n--- Méthode :", method, "---\n")
# Filtrer par méthode
df_sub <- df_test %>% filter(Methods == method)
# Calcul stats groupées par Dataset, Constraint_factor, Approach
summary_stats <- df_sub %>%
group_by(Dataset, Constraint_factor, Approach) %>%
summarise(
Mean_Accuracy = mean(Accuracy, na.rm = TRUE),
SD_Accuracy = sd(Accuracy, na.rm = TRUE),
Mean_Recall = mean(Recall, na.rm = TRUE),
SD_Recall = sd(Recall, na.rm = TRUE),
Mean_Precision = mean(Precision, na.rm = TRUE),
SD_Precision = sd(Precision, na.rm = TRUE),
Mean_F1 = mean(F1, na.rm = TRUE),
SD_F1 = sd(F1, na.rm = TRUE),
.groups = "drop"
)
print(summary_stats)
# Affichage tableau avec gridExtra
grid.table(summary_stats)
# Pause pour visualiser chaque tableau
readline(prompt = "Appuyez sur Entrée pour continuer à la méthode suivante...")
}
df_test <- Analysis_Final_Results_DF_05_2025 %>% filter(Set == "Test")
summary_stats <- df_test %>%
group_by(Methods, Dataset, Approach, Constraint_factor) %>%
summarise(
Mean_Accuracy = mean(Accuracy, na.rm = TRUE),
SD_Accuracy = sd(Accuracy, na.rm = TRUE),
Mean_Recall = mean(Recall, na.rm = TRUE),
SD_Recall = sd(Recall, na.rm = TRUE),
Mean_Precision = mean(Precision, na.rm = TRUE),
SD_Precision = sd(Precision, na.rm = TRUE),
Mean_F1 = mean(F1, na.rm = TRUE),
SD_F1 = sd(F1, na.rm = TRUE),
.groups = "drop"
)
print(head(summary_stats, 10))
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
# Filter null values
X_general <- mc.input$X[, colSums(mc.input$X) != 0]
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE)
set.seed(42)
y = as.vector(yvec)
X = X_general
# Determine the number of samples per class
nombre_echantillons_par_classe <- min(table(y))
# Function to balance classes and maintain order
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe, seed = 123) {
classes <- unique(y)
indices_equilibres <- integer(0)
for (classe in classes) {
indices_classe <- which(y == classe)
set.seed(seed)
indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
}
# Sort balanced indices to maintain original order
indices_equilibres <- sort(indices_equilibres)
return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}
# Get balanced data
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X
# Check distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)
# Split balanced data into 80% for training and 20% for testing
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[, indices_division, drop = FALSE]
X.test <- X_equilibre[, -indices_division, drop = FALSE]
table(y)
dim(X)
clf <- terga1_mc(nCores = 1,
seed = 1,
plot = TRUE
)
printy(clf)
runit = TRUE
if(runit)
{
res_clf3 <- fit_mc(X = X, y = y, clf = clf,approch="ovo", cross.validate = TRUE,aggregation_ = "voting", nfolds = 10, constraint_factor = "semi_constrained");
save(res_clf3 , clf, file ="res_clf3.rda", compression_level = 9)
}
res_clf3$crossVal$scores$generalization.acc
res_clf3$crossVal$scores$mean.acc
