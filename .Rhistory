levels = levels(df.melt$parsimony)[order(as.numeric(gsub("k_","",levels(df.melt$parsimony))))]
)
g.cor.gen.cv <- ggplot(data = df.melt, aes(y = value, x = parsimony)) +
geom_point(aes(color = variable), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
geom_boxplot(notch = FALSE, outlier.colour = NA, position = position_dodge(width=0.9), alpha = 0.3) +
ylab("cor_") +
xlab("Model parsimony") +
ggtitle("Testing performance (CV)") +
ylim(ylim) +
theme_bw() +
#geom_hline(yintercept = unique(maj.class), col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
}else
{
g.cor.emp.cv <- g.empty
g.cor.gen.cv <- g.empty
}
# RHO Empirical
v <- res$best$scores$cor_
df <- data.frame(value = v, parsimony = names(v))
df$parsimony <- factor(df$parsimony,
levels = levels(df$parsimony)[order(as.numeric(gsub("k_","",levels(df$parsimony))))]
)
g.cor.emp <- ggplot(data = df, aes(x = parsimony, y = value, group = 1)) +
geom_line(aes(color = "gray")) +
geom_point(size = 2, alpha = 1) +
scale_color_manual(values = "gray") +
ylab("cor_") +
xlab("Model parsimony") +
ggtitle("Emprical performance") +
labs(subtitle = paste("L:",obj$classifier$learner,"|F:",signif(res$best$model$cor_,4),"|k:",length(res$best$model$indices_[[1]]), sep="")) +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = mean(maj.class), col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
# RSQ Empirical (R squared)
v <- res$best$scores$rsq_
df <- data.frame(value = v, parsimony = names(v))
df$parsimony <- factor(df$parsimony,
levels = levels(df$parsimony)[order(as.numeric(gsub("k_","",levels(df$parsimony))))]
)
g.rsq.emp <- ggplot(data = df, aes(x = parsimony, y = value, group = 1)) +
geom_line(aes(color = "gray")) +
geom_point(size = 2, alpha = 1) +
scale_color_manual(values = "gray") +
ylab("rsq_") +
xlab("Model parsimony") +
ggtitle("Emprical performance") +
labs(subtitle = paste("L:",obj$classifier$learner,"|F:",signif(res$best$model$rsq_,4),"|k:",length(res$best$model$indices_[[1]]), sep="")) +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = mean(maj.class), col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
# SER Empirical (Standar error of the regression)
v <- res$best$scores$ser_
df <- data.frame(value = v, parsimony = names(v))
df$parsimony <- factor(df$parsimony,
levels = levels(df$parsimony)[order(as.numeric(gsub("k_","",levels(df$parsimony))))]
)
g.ser.emp <- ggplot(data = df, aes(x = parsimony, y = value, group = 1)) +
geom_line(aes(color = "gray")) +
geom_point(size = 2, alpha = 1) +
scale_color_manual(values = "gray") +
ylab("ser_") +
xlab("Model parsimony") +
ggtitle("Emprical performance") +
labs(subtitle = paste("L:",obj$classifier$learner,"|F:",signif(res$best$model$ser_,4),"|k:",length(res$best$model$indices_[[1]]), sep="")) +
#ylim(ylim) +
theme_bw() +
geom_hline(yintercept = mean(maj.class), col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
grid.arrange(g.cor.emp.cv, g.cor.gen.cv,
# empricial
g.cor.emp, g.rsq.emp,
g.ser.emp,
ncol = 2)
}else # if classification
{
if(crossval)
{
# make an empty plot in case it does not work
g.empty <- ggplot(data.frame()) + geom_point() + xlim(0, 10) + ylim(ylim) +
theme_bw() + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) +
ylab("") +
xlab("Model parismony sparse") +
ggtitle("") +
geom_hline(aes(yintercept=1), lty=2, col="lightgray") +
geom_hline(aes(yintercept=0.5), lty=2, col="lightgray")
#-----------------------------------------------------
# ACCURACY
#-----------------------------------------------------
# overall training accuracy learner results
dat <- res$cv$scores$empirical.acc
if(omit.na)
{
dat <- dat[rowSums(!is.na(dat))!=0,]
}
df <- data.frame(parsimony = rownames(dat),dat)
df.melt <- melt(df, id.vars = "parsimony")
df.melt$parsimony <- as.factor(df.melt$parsimony)
df.melt$parsimony <- factor(df.melt$parsimony,
levels = levels(df.melt$parsimony)[order(as.numeric(gsub("k_","",levels(df.melt$parsimony))))]
)
g.accuracy.emp.cv <- ggplot(data = df.melt, aes(y = value, x = parsimony)) +
geom_point(aes(color = variable), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
geom_boxplot(notch = FALSE, outlier.colour = NA, position = position_dodge(width=0.9), alpha = 0.3) +
ylab("accuracy_") +
xlab("Model parsimony") +
ggtitle("Training performance (CV)") +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = unique(maj.class), col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
# overall testing accuracy learner results
dat <- res$cv$scores$generalization.acc
if(omit.na)
{
dat <- dat[rowSums(!is.na(dat))!=0,]
}
df <- data.frame(parsimony = rownames(dat),dat)
df.melt <- melt(df, id.vars = "parsimony")
df.melt$parsimony <- as.factor(df.melt$parsimony)
df.melt$parsimony <- factor(df.melt$parsimony,
levels = levels(df.melt$parsimony)[order(as.numeric(gsub("k_","",levels(df.melt$parsimony))))]
)
g.accuracy.gen.cv <- ggplot(data = df.melt, aes(y = value, x = parsimony)) +
geom_point(aes(color = variable), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
geom_boxplot(notch = FALSE, outlier.colour = NA, position = position_dodge(width=0.9), alpha = 0.3) +
ylab("accuracy_") +
xlab("Model parsimony") +
ggtitle("Testing performance (CV)") +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = unique(maj.class), col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
#-----------------------------------------------------
# AUC
#-----------------------------------------------------
# overall training accuracy learner results
dat <- res$cv$scores$empirical.auc
if(omit.na)
{
dat <- dat[rowSums(!is.na(dat))!=0,]
}
df <- data.frame(parsimony = rownames(dat),dat)
df.melt <- melt(df, id.vars = "parsimony")
df.melt$parsimony <- as.factor(df.melt$parsimony)
df.melt$parsimony <- factor(df.melt$parsimony,
levels = levels(df.melt$parsimony)[order(as.numeric(gsub("k_","",levels(df.melt$parsimony))))]
)
g.auc.emp.cv <- ggplot(data = df.melt, aes(y = value, x = parsimony)) +
geom_point(aes(color = variable), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
geom_boxplot(notch = FALSE, outlier.colour = NA, position = position_dodge(width=0.9), alpha = 0.3) +
ylab("auc_") +
xlab("Model parsimony") +
ggtitle("Training performance (CV)") +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = 0.5, col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
# overall testing accuracy learner results
dat <- res$cv$scores$generalization.auc
if(omit.na)
{
dat <- dat[rowSums(!is.na(dat))!=0,]
}
df <- data.frame(parsimony = rownames(dat),dat)
df.melt <- melt(df, id.vars = "parsimony")
df.melt$parsimony <- as.factor(df.melt$parsimony)
df.melt$parsimony <- factor(df.melt$parsimony,
levels = levels(df.melt$parsimony)[order(as.numeric(gsub("k_","",levels(df.melt$parsimony))))]
)
g.auc.gen.cv <- ggplot(data = df.melt, aes(y = value, x = parsimony)) +
geom_point(aes(color = variable), position=position_jitterdodge(dodge.width=0.9), size = 1, alpha = 0.5) +
geom_boxplot(notch = FALSE, outlier.colour = NA, position = position_dodge(width=0.9), alpha = 0.3) +
ylab("auc_") +
xlab("Model parsimony") +
ggtitle("Testing performance (CV)") +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = 0.5, col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
}else
{
g.accuracy.emp.cv <- g.empty
g.accuracy.gen.cv <- g.empty
g.auc.emp.cv <- g.empty
g.auc.gen.cv <- g.empty
}
if(all(is.na(unlist(res$best$scores))))
{
g.accuracy.emp <- g.empty
g.auc.emp <- g.empty
g.recall.emp <- g.empty
g.precision.emp <- g.empty
}else
{
# ACCURACY Empirical
v <- res$best$scores$accuracy_
df <- data.frame(value = v, parsimony = names(v))
df$parsimony <- as.factor(df$parsimony)
df$parsimony <- factor(df$parsimony,
levels = levels(df$parsimony)[order(as.numeric(gsub("k_","",levels(df$parsimony))))]
)
g.accuracy.emp <- ggplot(data = df, aes(x = parsimony, y = value, group = 1)) +
geom_line(aes(color = "gray")) +
geom_point(size = 2, alpha = 1) +
scale_color_manual(values = "gray") +
ylab("accuracy_") +
xlab("Model parsimony") +
ggtitle("Emprical performance") +
labs(subtitle = paste("L:",obj$classifier$learner,"|F:",signif(res$best$model$accuracy_,4),"|k:",length(res$best$model$indices_[[1]]), sep="")) +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = mean(maj.class), col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
# AUC Empirical
v <- res$best$scores$auc_
df <- data.frame(value = v, parsimony = names(v))
df$parsimony <- as.factor(df$parsimony)
df$parsimony <- factor(df$parsimony,
levels = levels(df$parsimony)[order(as.numeric(gsub("k_","",levels(df$parsimony))))]
)
g.auc.emp <- ggplot(data = df, aes(x = parsimony, y = value, group = 1)) +
geom_line(aes(color = "gray")) +
geom_point(size = 2, alpha = 1) +
scale_color_manual(values = "gray") +
ylab("auc_") +
xlab("Model parsimony") +
ggtitle("Emprical performance") +
labs(subtitle = paste("L:",obj$classifier$learner,"|F:",signif(res$best$model$auc_,4),"|k:",length(res$best$model$indices_[[1]]), sep="")) +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = mean(maj.class), col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
# RECALL Empirical
v <- res$best$scores$recall_
df <- data.frame(value = v, parsimony = names(v))
df$parsimony <- as.factor(df$parsimony)
df$parsimony <- factor(df$parsimony,
levels = levels(df$parsimony)[order(as.numeric(gsub("k_","",levels(df$parsimony))))]
)
g.recall.emp <- ggplot(data = df, aes(x = parsimony, y = value, group = 1)) +
geom_line(aes(color = "gray")) +
geom_point(size = 2, alpha = 1) +
scale_color_manual(values = "gray") +
ylab("recall_") +
xlab("Model parsimony") +
ggtitle("Emprical performance") +
labs(subtitle = paste("L:",obj$classifier$learner,"|F:",signif(res$best$model$recall_,4),"|k:",length(res$best$model$indices_[[1]]), sep="")) +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = 0.5, col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
# PRECISION Empirical
v <- res$best$scores$precision_
df <- data.frame(value = v, parsimony = names(v))
df$parsimony <- as.factor(df$parsimony)
df$parsimony <- factor(df$parsimony,
levels = levels(df$parsimony)[order(as.numeric(gsub("k_","",levels(df$parsimony))))]
)
g.precision.emp <- ggplot(data = df, aes(x = parsimony, y = value, group = 1)) +
geom_line(aes(color = "gray")) +
geom_point(size = 2, alpha = 1) +
scale_color_manual(values = "gray") +
ylab("precision_") +
xlab("Model parsimony") +
ggtitle("Emprical performance") +
labs(subtitle = paste("L:",obj$classifier$learner,"|F:",signif(res$best$model$precision_,4),"|k:",length(res$best$model$indices_[[1]]), sep="")) +
ylim(ylim) +
theme_bw() +
geom_hline(yintercept = 0.5, col = "gray", linetype = "dashed") +
theme(legend.position="bottom", legend.direction="horizontal") +
guides(colour = "none")
} # end existing scores
grid.arrange(g.accuracy.emp.cv, g.accuracy.gen.cv,
g.auc.emp.cv, g.auc.gen.cv,
# empricial
g.accuracy.emp, g.auc.emp,
g.recall.emp, g.precision.emp,
ncol = 2)
} # end classification
}
# return digested results
return(res)
}
#' @title votingova
#' @description Function to aggregate one-versus-one or one-versus-all predictions using majority voting.
#' If there's a tie, a class is randomly selected. If no class is predicted, one is also randomly chosen among all possible.
#' @param mod List of submodels containing predictions and metadata.
#' @param y Vector of class labels (true labels or full set of possible classes).
#' @return A list representing the aggregated model object.
#' @export
votingova <- function(mod, y) {
predictions_list <- lapply(mod, function(x) x$predictions)  # Extract predictions
all_classes <- unique(as.character(y))
num_samples <- length(predictions_list[[1]])
aggregated_vector <- character(num_samples)
set.seed(123)  # Fix seed for reproducibility
for (i in 1:num_samples) {
votes <- table(sapply(predictions_list, `[`, i))
# Remove votes for "ALL" (OVA: means not predicted)
votes <- votes[names(votes) != "ALL"]
if (length(votes) == 0) {
# No class predicted at all: choose randomly from all classes
aggregated_vector[i] <- sample(all_classes, 1)
} else {
max_vote <- max(votes)
top_classes <- names(votes)[votes == max_vote]
if (length(top_classes) == 1) {
aggregated_vector[i] <- top_classes
} else {
aggregated_vector[i] <- sample(top_classes, 1)
}
}
}
# Reconstruct the model
model <- list()
model$learner <- mod[[1]]$learner
model$language <- mod[[1]]$language
model$objective <- mod[[1]]$objective
model$indices_ <- lapply(mod, function(x) x$indices_)
model$names_ <- lapply(mod, function(x) x$names_)
model$coeffs_ <- lapply(mod, function(x) x$coeffs_)
model$fit_ <- lapply(mod, function(x) x$fit_)
model$unpenalized_fit_ <- lapply(mod, function(x) x$unpenalized_fit_)
model$auc_ <- lapply(mod, function(x) x$auc_)
model$accuracy_ <- lapply(mod, function(x) x$accuracy_)
model$cor_ <- NA
model$aic_ <- NA
model$list_intercept_ <- lapply(mod, function(x) x$intercept_)
model$intercept_ <- mean(sapply(mod, function(x) x$intercept_))
model$eval.sparsity <- mod[[1]]$eval.sparsity
model$precision_ <- lapply(mod, function(x) x$precision_)
model$recall_ <- lapply(mod, function(x) x$recall_)
model$f1_ <- lapply(mod, function(x) x$f1_)
model$sign_ <- lapply(mod, function(x) x$sign_)
model$rsq_ <- lapply(mod, function(x) x$rsq_)
model$ser_ <- lapply(mod, function(x) x$ser_)
model$score_ <- lapply(mod, function(x) x$score_)
model$predictions <- predictions_list
model$scores_predictions <- lapply(mod, function(x) x$scores_predictions)
model$pos_score_ <- lapply(mod, function(x) x$pos_score_)
model$neg_score_ <- lapply(mod, function(x) x$neg_score_)
model$confusionMatrix_ <- lapply(mod, function(x) x$confusionMatrix_)
model$predictions_aggre <- aggregated_vector
model$method <- "votingova"
model$approach <- "ova"  # <-- Change ici si besoin
return(model)
}
#' @title Majority Voting with Tie-Breaking (OVA)
#' @description Aggregates predictions from one-versus-all (OVA) classifiers using majority voting.
#' In case of a tie between classes (equal number of votes), the class whose associated OVA model
#' provides the highest prediction score for the current sample is selected.
#' If no model predicts a class (all vote "ALL"), a class is selected randomly from the full class set.
#'
#' @param mod A list of submodels (OVA classifiers), each containing predictions and prediction scores.
#' @param y A vector of true labels or the complete set of possible classes.
#'
#' @return A list representing the aggregated model, including the final predicted classes.
#' @export
Majority_Voting_with_Tie_Breaking_ova <- function(mod, y) {
predictions_list <- lapply(mod, function(x) x$predictions)
scores_list <- lapply(mod, function(x) x$scores_predictions)
all_classes <- unique(as.character(y))
num_samples <- length(predictions_list[[1]])
aggregated_vector <- character(num_samples)
set.seed(123)  # Ensure reproducibility
for (i in 1:num_samples) {
votes <- table(sapply(predictions_list, `[`, i))
votes <- votes[names(votes) != "ALL"]
if (length(votes) == 0) {
aggregated_vector[i] <- sample(all_classes, 1)
} else {
max_vote <- max(votes)
top_classes <- names(votes)[votes == max_vote]
if (length(top_classes) == 1) {
aggregated_vector[i] <- top_classes
} else {
# Search for the submodel with highest score among tied classes
best_score <- -Inf
best_class <- NA
for (j in seq_along(mod)) {
pred_class <- predictions_list[[j]][i]
score <- scores_list[[j]][i]
if (pred_class %in% top_classes && score > best_score) {
best_score <- score
best_class <- pred_class
}
}
aggregated_vector[i] <- best_class
}
}
}
# Assemble the final model object
model <- list()
model$learner <- mod[[1]]$learner
model$language <- mod[[1]]$language
model$objective <- mod[[1]]$objective
model$indices_ <- lapply(mod, function(x) x$indices_)
model$names_ <- lapply(mod, function(x) x$names_)
model$coeffs_ <- lapply(mod, function(x) x$coeffs_)
model$fit_ <- lapply(mod, function(x) x$fit_)
model$unpenalized_fit_ <- lapply(mod, function(x) x$unpenalized_fit_)
model$auc_ <- lapply(mod, function(x) x$auc_)
model$accuracy_ <- lapply(mod, function(x) x$accuracy_)
model$cor_ <- NA
model$aic_ <- NA
model$list_intercept_ <- lapply(mod, function(x) x$intercept_)
model$intercept_ <- mean(sapply(mod, function(x) x$intercept_))
model$eval.sparsity <- mod[[1]]$eval.sparsity
model$precision_ <- lapply(mod, function(x) x$precision_)
model$recall_ <- lapply(mod, function(x) x$recall_)
model$f1_ <- lapply(mod, function(x) x$f1_)
model$sign_ <- lapply(mod, function(x) x$sign_)
model$rsq_ <- lapply(mod, function(x) x$rsq_)
model$ser_ <- lapply(mod, function(x) x$ser_)
model$score_ <- lapply(mod, function(x) x$score_)
model$predictions <- predictions_list
model$scores_predictions <- scores_list
model$pos_score_ <- lapply(mod, function(x) x$pos_score_)
model$neg_score_ <- lapply(mod, function(x) x$neg_score_)
model$confusionMatrix_ <- lapply(mod, function(x) x$confusionMatrix_)
model$predictions_aggre <- aggregated_vector
model$method <- "Majority_Voting_with_Tie_Breaking_ova"
model$approach <- "ova"
return(model)
}
knitr::opts_chunk$set(echo = TRUE)
# Package multi class predomics
library(mcpredomics)
# Package predomics
library(predomics)
# Visualization library for creating complex plots
library(ggplot2)
# Arranges multiple ggplot objects on a single page
library(gridExtra)
# ROC curve analysis and AUC calculation
library(pROC)
# Reshaping and melting data frames
library(reshape2)
# Implementation of the Random Forest algorithm for classification and regression
library(randomForest)
# Comprehensive library for classification and regression training
library(caret)
# Various R programming tools and functions, including data manipulation
library(gtools)
# Adding statistical comparisons and publication-ready visualizations
library(ggpubr)
# Data manipulation and transformation (part of the tidyverse)
library(dplyr)
# Tidying messy data by gathering and spreading
library(tidyr)
# Enhanced data frames with row names as a column (tibble format)
library(tibble)
# Dynamic report generation and displaying results in tables
library(knitr)
# Creating aesthetically pleasing and customizable HTML tables
library(kableExtra)
# Interactive tables for data visualization and exploration
library(DT)
# Functions for statistical learning, including SVM and Naive Bayes
library(e1071)
# Lasso and ridge regression via generalized linear models
library(glmnet)
# Reading data from files (including CSV and text files)
library(readr)
# String manipulation and regular expression functions
library(stringr)
# Load the file
load("../data/predomics.inputs.ExperimentHubMulticlass.Rda")
lapply(predomics.inputs, function(x){summary(colSums(x[["X"]]))})
# Extract data
data1 <- predomics.inputs$FengQ_2015
df <- data1$y.df
y <- df$study_condition
X <- data1$X
table(y)
dim(X)
clf <- terBeam_mc(sparsity = c(2,3,4,5,6,7,8,9,10),
max.nb.features = 1000,
seed = 1,
nCores = 1,
evalToFit = "accuracy_",
objective = "auc",
experiment.id = "terBeam_mc",
experiment.save = "nothing")
printy(clf)
runit = TRUE
if(runit)
{
Exa <- fit_mc(X = X, y = y, clf = clf,approch="ovo", cross.validate = TRUE,aggregation_ = "Majority_Voting_with_Tie_Breaking", nfolds = 10, constraint_factor = "semi_constrained");
#save(res_clf_terga1_TD2_voting_ova_unconst  , clf, file ="res_clf_terga1_TD2_voting_ova_unconst.rda", compression_level = 9)
}
Exa$crossVal$scores$mean.acc
#runit = TRUE
#if(runit)
#{
#Exa <- fit_mc(X = X, y = y, clf = clf,approch="ovo", cross.validate = TRUE,aggregation_ = "Majority_Voting_with_Tie_Breaking", nfolds = 10, constraint_factor = "semi_constrained");
save(Exa  , clf, file ="Exa.rda", compression_level = 9)
#}
