X.nona <- X
}
feature.cor     <- filterfeaturesK(data = t(apply(X.nona, 1, rank)), # for speedup
trait = rank(y.nona),             # for speedup
k = max.nb.features,
type = "pearson",                 # for speedup
sort = TRUE,
verbose = clf$params$verbose,
return.data = FALSE) # to avoid having to recompute this all the time
}else # classification
{
nClasse <- unique(y)
feature.cor   <- list()
list_y <- list()
list_X <- list()
if (approch == "ovo") {
k <- 1
for (i in 1:(length(nClasse)-1)) {
for (j in (i+1):length(nClasse)) {
class_i <- nClasse[i]
class_j <- nClasse[j]
indices <- which(y == class_i | y == class_j)
y_pair <- y[indices]
X_pair <- X[, indices]
list_y[[k]] <- as.vector(y_pair)
list_X[[k]] <- X_pair
k <- k + 1
}
}
} else {
for (i in 1:length(nClasse)) {
class_i <- nClasse[i]
y_temp <- ifelse(y == class_i, as.character(class_i), "All")
# Organiser les indices pour que la classe "Rest" soit toujours la première
rest_indices <- which(y_temp == "All")
new_order <- c(rest_indices, setdiff(1:length(y_temp), rest_indices))
list_y[[i]] <- as.vector(y_temp[new_order])
list_X[[i]] <- X[, new_order]
}
}
for (i in 1:(length(list_y))) {
feature.cor[[i]]     <- filterfeaturesK(data = list_X[[i]],
trait = list_y[[i]],
k = max.nb.features,
type = "wilcoxon",
sort = TRUE,
verbose = clf$params$verbose,
return.data = FALSE) # to avoid having to recompute this all the time
}
}
if(!is.null(path))
{
save(feature.cor, file = path.feature.cor)
cat("... Correlation file saved\n")
}
}
cat("... Storing data in the classifier object for speedup\n")
clf$feature.cor <- feature.cor # add them to the clf
if(all(is.na(clf$feature.cor[[1]]$p)))
{
warning("runClassifier: does not seem to have produced a pvalue")
}
# store the initial order and indexes
clf$data          <- list()
list_y <- list()
list_X <- list()
list_features <- list()
y <- as.vector(y)
nClasse <- unique(y)
if (approch == "ovo") {
k <- 1
for (i in 1:(length(nClasse)-1)) {
for (j in (i+1):length(nClasse)) {
class_i <- nClasse[i]
class_j <- nClasse[j]
indices <- which(y == class_i | y == class_j)
y_pair <- y[indices]
X_pair <- X[, indices]
list_y[[k]] <- as.vector(y_pair)
list_X[[k]] <- X_pair
k <- k + 1
}
}
} else {
for (i in 1:length(nClasse)) {
class_i <- nClasse[i]
y_temp <- ifelse(y == class_i, as.character(class_i), "All")
# Organiser les indices pour que la classe "Rest" soit toujours la première
rest_indices <- which(y_temp == "All")
new_order <- c(rest_indices, setdiff(1:length(y_temp), rest_indices))
list_y[[i]] <- as.vector(y_temp[new_order])
list_X[[i]] <- X[, new_order]
}
}
clf$data$features <- list_features
names(clf$data$features) <- clf$data$features
list_XX <- list()
list_min <- list()
list_max <- list()
for (i in 1:(length(list_X))) {
Xi <- list_X[[i]][rownames(clf$feature.cor[[i]])[1:max.nb.features],]
mino <- min(Xi, na.rm=TRUE)
maxo <- max(Xi, na.rm=TRUE)
list_XX[[i]] <- Xi
list_min[[i]] <-  mino
list_max[[i]] <-  maxo
}
clf$data$X        <- list_XX
clf$data$X.min    <- list_min
clf$data$X.max    <- list_max
clf$data$y        <- list_y
# compute the coefficients once for all to improve performance
cat("... Computing ternary coefficients for speedup\n")
coeffs          <- getSign_ovo(X = X, y = y, clf = clf, approch = approch, parallel.local = FALSE)
clf$coeffs_     <- coeffs # add them to the clf
# check sparsity not to be larger than variables in X
if(any(is.na(clf$params$sparsity > nrow(X))))
{
# adding the maximum number of featuers
clf$params$sparsity <- c(clf$params$sparsity, nrow(X))
}
# mark NAs the bigger ones
clf$params$sparsity[clf$params$sparsity > nrow(X)] <- NA
# delete them
clf$params$sparsity <- clf$params$sparsity[!is.na(clf$params$sparsity)]
# set the seed while sanity checking
if(!(any(clf$params$seed=="NULL")))
{
# convert from list to a vector
if(is.list(clf$params$seed))
{
clf$params$seed <- unlist(clf$params$seed)
if(any(class(clf$params$seed)!="numeric"))
{
stop("fit: convertion of seed from list to numeric vector failed.")
}
}
# we can have multiple k-folds per experiment if the seed is vectors of seeds
if(length(clf$params$seed)==1)
{
set.seed(clf$params$seed)
cat("... One seed found, setting by default\n")
}else
{
if(length(clf$params$seed)==0)
{
stop("fit: the seed should have at least one value.")
}
# if we are here this means that it everything is as expected seed is a
# vector of numeric values.
set.seed(clf$params$seed[1])
cat("... Multiple seeds found, setting the default\n")
}
}
# check and set the number of folds
if(!is.null(lfolds))
{
cat("... Custom folds are provided\n")
if(!is.list(lfolds))
{
lfolds = NULL
}else #if lfolds exists
{
nfolds = length(lfolds)
}
}
if(nfolds == 1)
{
cat("... The number of folds is set to 1. In this case I'm deactivating the cross-validation process\n")
cross.validate = FALSE
}
# set the parallelize.folds parameter. If no crossval than it is deactivated
clf$params$parallelize.folds <- parallelize.folds & cross.validate & clf$params$parallel
# add a parallel.local parameter if we wish to speed out some local steps
clf$params$parallel.local <- FALSE
# START CLUSTER if parallel computing set the cluster
if(clf$params$parallel)
{
cat(paste("... Running the classifier", clf$learner,"in parallel mode with ",clf$params$nCores + 1,"CPUs\n"))
# adding another core for the whole dataset. If it is taken into account
# during the launch this will be a sleeping thread so no harm, if not it
# will allow to run faster as we won't forget to increment it
registerDoSNOW(clf$params$cluster <- makeCluster(clf$params$nCores + 1, type = "SOCK", outfile = log.file))
if(!clf$params$parallelize.folds)  # if folds are not parallelized
{
clf$params$parallel.local <- TRUE # switch the local parallel to TRUE
}
}else
{
cat(paste("... Running the classifier", clf$learner,"with a single CPU\n"))
}
# for all the predomics learners
if(!isLearnerSota(clf)) # if runing the BTR algorithms
{
# set the epsilon
if(!is.null(clf$params$epsilon))
{
if(clf$params$epsilon=="NULL")
{
#clf$params$epsilon        <- .Machine$double.xmin
clf$params$epsilon        <- 0
if(clf$params$verbose) cat("... Setting epsilon for predomics learners\n")
}
}
}
# save the data step by step to be able to resume
if(clf$experiment$save != "nothing")
{
if(clf$params$verbose) cat("... Saving experiments\n")
fileNames                 <- gsub(" ", "_", clf$experiment$id)
dir.create(fileNames)
setwd(fileNames)
saveResults(X, paste("X", fileNames, "yml", sep = "."))
saveResults(y, paste("Y", fileNames, "yml", sep = "."))
experiment <- list()
experiment$desc           <- clf$experiment$description
experiment$params         <- list(clf=list(learner = clf$learner, params = clf$params, experiment = clf$experiment))
if(clf$experiment$save == "full")
{
if((clf$params$popSaveFile=="NULL"))
{
clf$params$popSaveFile <- fileNames
}
}
}
switch(clf$learner,
terda=
{
# Here we handle also the sota.glmnet as well since this is a derivate of terda
cat('... terda fitting based on Linear programming relaxation ...\n')
},
terga1=
{
cat('... First version of terga fitting based on Genetic Algorithm heuristics ...\n')
},
terga1_mc=
{
cat('... First version of terga fitting based on Genetic Algorithm heuristics ...\n')
},
terga2=
{
cat('... Second and faster version of terga fitting based on Genetic Algorithm heuristics ...\n')
},
terBeam=
{
cat('... terbeam fitting based on Exhaustive Heuristic beam search ...\n')
},
terBeam_mc=
{
cat('... terbeam fitting based on Exhaustive Heuristic beam search ...\n')
},
metal=
{
cat('... model fitting based on aggregating different Heuristics ...\n')
},
sota.svm=
{
cat('... SOTA: state of the art SVM fitting ...\n')
},
sota.rf=
{
cat('... SOTA: state of the art Ranfom Forest fitting ...\n')
},
sota.rf_ovo =
{
cat('... SOTA: state of the art Ranfom Forest fitting MC ...\n')
},
{
warning('This method does not exist !')
}
)
# get the best model
best.model_terga1_mcovo <- res_clf.dig_terga1ova$best$model
#printy(best.model)
#grid.arrange(plotModel_ovo(best.model, X=X, y=y, sort.features = FALSE, feature.name = TRUE),
#plotModel_ovo(best.model, X=X, y=y, sort.features = FALSE, feature.name = TRUE, importance = TRUE),ncol=2)
best.model.test_terga1ovo <- evaluateModel_ovo(mod = best.model_terga1_mcovo, X = X.test, y = y.test, clf = clf, eval.all = TRUE, force.re.evaluation = TRUE, mode = "test", approch="ovo")
best.model.test_terga1ovo
model_test_terga1 <- list()
model_test_terga1$accuracy_ =best.model.test_terga1ovo$accuracy_
model_test_terga1$auc_ = best.model.test_terga1ovo$auc_
model_test_terga1$recall_ = best.model.test_terga1ovo$recall_
model_test_terga1$precision_ = best.model.test_terga1ovo$precision_
# Sélectionner uniquement les métriques nécessaires
model_test_terga1 <- list(
accuracy_ = model_test_terga1$accuracy_,
auc_ = model_test_terga1$auc_,
recall_ = model_test_terga1$recall_,
precision_ = model_test_terga1$precision_
)
# Convertir la liste en un data frame
df <- data.frame(
Metric = names(model_test_terga1),
Value = unlist(model_test_terga1)
)
# Modifier la couleur pour chaque métrique
df$Color <- c("red", "green", "blue", "orange")
# Créer le graphique de barres avec ggplot2
library(ggplot2)
plot <- ggplot(df, aes(x = Metric, y = Value, fill = Color)) +
geom_bar(stat = "identity", alpha = 0.7) +
geom_text(aes(label = round(Value, 3)), vjust = -0.3) +
ylim(0, 1) +
labs(title = "Performance of model test terbeam one versus one",
x = "Metrics",
y = "Values") +
theme_minimal()
# Afficher le graphique
print(plot)
approch = "ova"
runit = TRUE
if(runit)
{
res_clf <- fit_OVO(X = X, y = y, clf = clf,approch="ova", cross.validate = TRUE, nfolds = 1); # class(res_clf)
# save results
save(res_clf, clf, file = "res_clf.rda", compression_level = 9)
}
# ... Database X is not a matrix! Converting ...
# ... Classification mode, computing factor(y) for speedup and robustness
# ... Loading feature correlation for speedup
# ... Correlation file loaded
# ... Storing data in the classifier object for speedup
# ... Computing ternary coefficients for speedup
# ... One seed found, setting by default
# ... Running the classifier terga2 with a single CPU
# ... Second and faster version of terga fitting based on Genetic Algorithm heuristics ...
# ... Cross validation mode
# ... Starting cross validation not in parallel
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ... Learning process is finished succesfuly
# ... Thank you for using Predomics. Don't forget to digest the results now.
#
# [1] "experiment" "predomics"
res_clf.dig_terga1ova <- digest(obj = res_clf, penalty = 0.75/100, plot = TRUE)
# get the best model
best.model_terga1_mcovo <- res_clf.dig_terga1ova$best$model
#printy(best.model)
#grid.arrange(plotModel_ovo(best.model, X=X, y=y, sort.features = FALSE, feature.name = TRUE),
#plotModel_ovo(best.model, X=X, y=y, sort.features = FALSE, feature.name = TRUE, importance = TRUE),ncol=2)
best.model.test_terga1ovo <- evaluateModel_ovo(mod = best.model_terga1_mcovo, X = X.test, y = y.test, clf = clf, eval.all = TRUE, force.re.evaluation = TRUE, mode = "test", approch="ova")
best.model.test_terga1ovo
model_test_terga1 <- list()
model_test_terga1$accuracy_ =best.model.test_terga1ovo$accuracy_
model_test_terga1$auc_ = best.model.test_terga1ovo$auc_
model_test_terga1$recall_ = best.model.test_terga1ovo$recall_
model_test_terga1$precision_ = best.model.test_terga1ovo$precision_
model_test_terga1 <- list()
model_test_terga1$accuracy_ =best.model.test_terga1ovo$accuracy_
model_test_terga1$auc_ = best.model.test_terga1ovo$auc_
model_test_terga1$recall_ = best.model.test_terga1ovo$recall_
model_test_terga1$precision_ = best.model.test_terga1ovo$precision_
# Sélectionner uniquement les métriques nécessaires
model_test_terga1 <- list(
accuracy_ = model_test_terga1$accuracy_,
auc_ = model_test_terga1$auc_,
recall_ = model_test_terga1$recall_,
precision_ = model_test_terga1$precision_
)
# Convertir la liste en un data frame
df <- data.frame(
Metric = names(model_test_terga1),
Value = unlist(model_test_terga1)
)
# Modifier la couleur pour chaque métrique
df$Color <- c("red", "green", "blue", "orange")
# Créer le graphique de barres avec ggplot2
library(ggplot2)
plot <- ggplot(df, aes(x = Metric, y = Value, fill = Color)) +
geom_bar(stat = "identity", alpha = 0.7) +
geom_text(aes(label = round(Value, 3)), vjust = -0.3) +
ylim(0, 1) +
labs(title = "Performance of model test terbeam all versus all",
x = "Metrics",
y = "Values") +
theme_minimal()
# Afficher le graphique
print(plot)
# Sélectionner uniquement les métriques nécessaires
model_test_terga1 <- list(
accuracy_ = model_test_terga1$accuracy_,
auc_ = model_test_terga1$auc_,
recall_ = model_test_terga1$recall_,
precision_ = model_test_terga1$precision_
)
# Convertir la liste en un data frame
df <- data.frame(
Metric = names(model_test_terga1),
Value = unlist(model_test_terga1)
)
# Modifier la couleur pour chaque métrique
df$Color <- c("red", "green", "blue", "orange")
# Créer le graphique de barres avec ggplot2
library(ggplot2)
plot <- ggplot(df, aes(x = Metric, y = Value, fill = Color)) +
geom_bar(stat = "identity", alpha = 0.7) +
geom_text(aes(label = round(Value, 3)), vjust = -0.3) +
ylim(0, 1) +
labs(title = "Performance of model test terbeam one versus all",
x = "Metrics",
y = "Values") +
theme_minimal()
# Afficher le graphique
print(plot)
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
#recover the vector y
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Créer un vecteur d'indices pour la division des données
set.seed(42)  # Vous pouvez définir une graine pour la reproductibilité
indices_division <- createDataPartition(yvec_trie, p = 0.8, list = FALSE)
# Diviser yvec_trie en 80% train et 20% test
y <- as.vector(yvec_trie[indices_division])
y.test <- as.vector(yvec_trie[-indices_division])
X <- X_general[,indices_division]
X.test <- X_general[,-indices_division]
# Vérifier la répartition dans chaque ensemble
table(y)
table(y.test)
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
#recover the vector y
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Créer un vecteur d'indices pour la division des données
set.seed(42)  # Vous pouvez définir une graine pour la reproductibilité
indices_division <- createDataPartition(yvec_trie, p = 0.8, list = FALSE)
# Diviser yvec_trie en 80% train et 20% test
y <- as.vector(yvec_trie[indices_division])
y.test <- as.vector(yvec_trie[-indices_division])
X <- X_general[,indices_division]
X.test <- X_general[,-indices_division]
# Vérifier la répartition dans chaque ensemble
table(y)
table(y.test)
dim(X)
dim(X.test)
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Créer un vecteur d'indices pour la division des données
set.seed(42)  # Vous pouvez définir une graine pour la reproductibilité
#indices_division <- createDataPartition(yvec_trie, p = 0.8, list = FALSE)
# Diviser yvec_trie en 80% train et 20% test
#y <- as.vector(yvec_trie[indices_division])
#y.test <- as.vector(yvec_trie[-indices_division])
#X <- X_general[,indices_division]
#X.test <- X_general[,-indices_division]
# Vérifier la répartition dans chaque ensemble
#table(y)
#table(y.test)
y = as.vector(yvec_trie)
X = X_general
# Nombre d'échantillons désiré dans chaque classe
nombre_echantillons_par_classe <- min(table(y))
# Fonction pour équilibrer les classes
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe) {
classes <- unique(y)
indices_equilibres <- integer(0)
for (classe in classes) {
indices_classe <- which(y == classe)
indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
}
return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}
# Équilibrer les classes dans l'ensemble d'entraînement
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
# Utiliser les données équilibrées
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X
# Vérifier la répartition après équilibrage
#table(y_equilibre)
set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)
# Diviser yvec_trie en 80% train et 20% test
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[,indices_division]
X.test <- X_equilibre[,-indices_division]
# Vérifier la répartition dans chaque ensemble
table(y)
table(y.test)
dim(X)
dim(X.test)
best.model_terga1_mcovo
best.model_terga1_mcovo
clf$data$y
lapply(clf$data$y,table)
lapply(lapply(clf$data$y,table), names)
unlist(lapply(lapply(lapply(clf$data$y,table), names), paste))
lapply(lapply(lapply(clf$data$y,table), names), paste, collapse = "_")
unlist(lapply(lapply(lapply(clf$data$y,table), names), paste, collapse = "_"))
