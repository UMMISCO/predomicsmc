}
# create the empty object structure that will be returned
res.crossval                                <- list()
res.crossval$nfold                          <- list()
res.crossval$k                              <- list()
res.crossval$scores                         <- list()
res.crossval$scores$empirical.auc           <- as.data.frame(matrix(nrow=max(clf$params$sparsity), ncol=(nfolds-1)))
rownames(res.crossval$scores$empirical.auc) <- c(paste("k",c(1:max(clf$params$sparsity)), sep="_"))
colnames(res.crossval$scores$empirical.auc) <- paste("fold",1:(nfolds-1),sep="_") #-1 since the whole won't be taken into account
# add others using the same model
res.crossval$scores$generalization.auc      <- res.crossval$scores$empirical.auc # auc
# accuracy
res.crossval$scores$empirical.acc           <- res.crossval$scores$empirical.auc # accuracy
res.crossval$scores$generalization.acc      <- res.crossval$scores$empirical.auc # accuracy
# recall
res.crossval$scores$empirical.rec           <- res.crossval$scores$empirical.auc # recall
res.crossval$scores$generalization.rec      <- res.crossval$scores$empirical.auc # recall
# precision
res.crossval$scores$empirical.prc           <- res.crossval$scores$empirical.auc # precision
res.crossval$scores$generalization.prc      <- res.crossval$scores$empirical.auc # precision
# f1-score
res.crossval$scores$empirical.f1s           <- res.crossval$scores$empirical.auc # f1-score
res.crossval$scores$generalization.f1s      <- res.crossval$scores$empirical.auc # f1-score
# correlation
res.crossval$scores$empirical.cor           <- res.crossval$scores$empirical.auc # cor
res.crossval$scores$generalization.cor      <- res.crossval$scores$empirical.auc # cor
# for each fold compute the best models
if(clf$params$parallelize.folds)
{
cat("... Starting CV in parallel\n")
# execute each crossVal in //
res.all <- foreach(i = 1:nfolds) %dorng%
{
# prepare the datasets
if(all(is.na(lfolds[[i]])))
{
# Here we are in the whole dataset
# training dataset
x_train = X
y_train = y
# # testing dataset, needed for checking extreme cases
x_test = X
y_test = y
}else
{
# Here we are in the whole dataset
# training dataset
x_train = X[,-lfolds[[i]]]
y_train = y[-lfolds[[i]]]
# # testing dataset needed for checking extreme cases
if(length(lfolds[[i]]) == 1) # for leave one out
{
x_test = as.matrix(X[,lfolds[[i]]])
}else
{
x_test = X[,lfolds[[i]]]
}
y_test = y[lfolds[[i]]]
} # end else other folds
# omit some particular cases
if(any(table(y_train)==0) | length(table(y_train))==1) # to take into account the leve one out case
{
NULL
} else
{
# TODO: test saveing all the k-folds during execution
if(!(clf$params$popSaveFile=="NULL"))
{
#dirName <- paste(clf$params$popSaveFile,paste("crossVal.fold", i, sep = ""),sep="/")
dirName                         <- paste("crossVal.fold", i, sep = "")
dir.create(dirName)
setwd(dirName)
}
# Launch the classifier in the train dataset and digest results
clf$params$current_seed <- clf$params$seed[1] + i # set the current seed
if(clf$params$debug)
{
cat("=> DBG: before runclassifier\n")
runClassifier_ovo(X = x_train, y =  y_train, clf = clf, x_test = x_test, y_test = y_test)
}else
{
try(
{
runClassifier_ovo(X = x_train, y =  y_train, clf = clf, x_test = x_test, y_test = y_test)
}, silent = TRUE
)
}
}
} # end of folds loop (foreach)
}else # no parallel
{
# execute each crossval in serial
cat("... Starting cross validation not in parallel\n")
res.all <- list()
for (i in 1:nfolds)
{
if (clf$params$verbose) {cat("===> k-fold\t",names(lfolds)[i],"\n")}
# prepare the datasets
if(all(is.na(lfolds[[i]])))
{
# Here we are in the whole dataset
# training dataset
x_train = X
y_train = y
# # testing dataset, needed for checking extreme cases
x_test = X
y_test = y
}else
{
# Here we are in the whole dataset
# training dataset
x_train = X[,-lfolds[[i]]]
y_train = y[-lfolds[[i]]]
# # testing dataset needed for checking extreme cases
if(length(lfolds[[i]]) == 1) # for leave one out
{
x_test = as.matrix(X[,lfolds[[i]]])
}else
{
x_test = X[,lfolds[[i]]]
}
y_test = y[lfolds[[i]]]
} # end other folds
# omit some particular cases
if(any(table(y_train)==0) | length(table(y_train))==1) # to take into account the leve one out case
{
warning("runCrossval: only one level in the class impossible to compute fitness")
next
}
# TODO: test saveing all the k-folds during execution
if(!(clf$params$popSaveFile=="NULL"))
{
#dirName <- paste(clf$params$popSaveFile,paste("crossVal.fold", i, sep = ""),sep="/")
dirName                         <- paste("crossVal.fold", i, sep = "")
dir.create(dirName)
setwd(dirName)
}
# Launch the classifier in the train dataset and digest results
clf$params$current_seed           <- clf$params$seed[1] + i
if(clf$params$debug)
{
res.all[[i]]                    <- runClassifier_ovo(X = x_train, y =  y_train, clf = clf, x_test = x_test, y_test = y_test)
}else
{
res.all[[i]]                    <- try(
{
runClassifier_ovo(X = x_train, y =  y_train, clf = clf, x_test = x_test, y_test = y_test)
}, silent = FALSE
) # end try
} # end else debug
} # end of folds loop (for)
} # end else parallelize.folds
if(clf$params$verbose) cat("... Cross validation finished\n")
# store the empirical result separately
res.crossval$whole <- res.all[[1]]
# omit it from the empirical results as they will be extracted separately and
res.all <- res.all[-1]
# also clean the lfolds object
lfolds  <- lfolds[-1]
# results for FEATURE IMPORTANCE
# the MDA for each fold
mda.all <- as.data.frame(matrix(NA, nrow = nrow(X), ncol = length(res.all)))
rownames(mda.all) <- rownames(X); colnames(mda.all) <- names(res.all)
# create also results for the standard deviation and the prevalence in the folds
pda.all <- sda.all <- mda.all
# DISPATCH the results in the custom output structure
for(i in 1:length(res.all))
{
# prepare the datasets
# training dataset
x_train = X[,-lfolds[[i]]]
y_train = y[-lfolds[[i]]]
# testing dataset
if(length(lfolds[[i]]) == 1) # for leave one out
{
x_test = as.matrix(X[,lfolds[[i]]])
}else
{
x_test = X[,lfolds[[i]]]
}
y_test = y[lfolds[[i]]]
res_train <- res.all[[i]]
if(is.list(res_train)) # if result object exist
{
if(is.null(res_train$models)) # and is a model collection
{
if(!isModelCollection(res_train))
{
res_train.digest              <- NULL
}else
{
# digest
res_train.digest              <- digestModelCollection(obj = res_train, X = x_train, clf = clf)
}
}else # is a crossval result
{
res_train.digest                <- digestModelCollection(obj = res_train$models, X = x_train, clf = clf)
} # end if/else models exist
}else # return nothing
{
res_train.digest                  <- NULL
}
# if the results could be digested
if(!is.null(res_train.digest))
{
# for all the best models of each k-sparse (create empty matrix) for auc
res.crossval$k$auc                <- as.data.frame(matrix(nrow=max(clf$params$sparsity), ncol=2))
rownames(res.crossval$k$auc)      <- c(paste("k",c(1:max(clf$params$sparsity)), sep="_"))
colnames(res.crossval$k$auc)      <- c("empirical","generalization")
# add another table for accuracy
res.crossval$k$acc                <- res.crossval$k$auc
# recall
res.crossval$k$rec                <- res.crossval$k$auc
# precision
res.crossval$k$prc                <- res.crossval$k$auc
# f1-score
res.crossval$k$f1s                <- res.crossval$k$auc
# add another table for correlation
res.crossval$k$cor                <- res.crossval$k$auc
# for all k-sparse BEST models
for(k in 1:length(res_train.digest$best_models))
{
k_sparse.name     <- names(res_train.digest$best_models)[k]
mod               <- res_train.digest$best_models[[k_sparse.name]]
# update the final indexes as the input X
mod               <- updateObjectIndex(obj = mod, features = rownames(X))
#mod.train         <- evaluateModel(mod = mod, X=x_train, y=y_train, clf=clf, eval.all = TRUE, force.re.evaluation = TRUE, mode='train')
mod.train         <- mod # since this is the same as computed above
mod.test          <- evaluateModel_ovo(mod = mod, X=x_test, y=y_test, clf=clf, eval.all = TRUE, force.re.evaluation = TRUE, mode='test')
if(!is.null(mod.train) & !is.null(mod.test))
{
# Empirical fitting score
res.crossval$scores$empirical.auc[k_sparse.name,i]            <- mod.train$auc_
res.crossval$scores$empirical.acc[k_sparse.name,i]            <- mod.train$accuracy_
res.crossval$scores$empirical.rec[k_sparse.name,i]            <- mod.train$recall_
res.crossval$scores$empirical.prc[k_sparse.name,i]            <- mod.train$precision_
res.crossval$scores$empirical.f1s[k_sparse.name,i]            <- mod.train$f1_
res.crossval$scores$empirical.cor[k_sparse.name,i]            <- mod.train$cor_
# Generalization fitting score
res.crossval$scores$generalization.auc[k_sparse.name,i]       <- mod.test$auc_
res.crossval$scores$generalization.acc[k_sparse.name,i]       <- mod.test$accuracy_
res.crossval$scores$generalization.rec[k_sparse.name,i]       <- mod.test$recall_
res.crossval$scores$generalization.prc[k_sparse.name,i]       <- mod.test$precision_
res.crossval$scores$generalization.f1s[k_sparse.name,i]       <- mod.test$f1_
res.crossval$scores$generalization.cor[k_sparse.name,i]       <- mod.test$cor_
# store by k
# AUC
res.crossval$k$auc[k_sparse.name,"empirical"]                 <- mod.train$auc_
res.crossval$k$auc[k_sparse.name,"generalization"]            <- mod.test$auc_
# Accuracy
res.crossval$k$acc[k_sparse.name,"empirical"]                 <- mod.train$accuracy_
res.crossval$k$acc[k_sparse.name,"generalization"]            <- mod.test$accuracy_
# Recall
res.crossval$k$rec[k_sparse.name,"empirical"]                 <- mod.train$recall_
res.crossval$k$rec[k_sparse.name,"generalization"]            <- mod.test$recall_
# Precision
res.crossval$k$prc[k_sparse.name,"empirical"]                 <- mod.train$precision_
res.crossval$k$prc[k_sparse.name,"generalization"]            <- mod.test$precision_
# F1-Score
res.crossval$k$f1s[k_sparse.name,"empirical"]                 <- mod.train$f1_
res.crossval$k$f1s[k_sparse.name,"generalization"]            <- mod.test$f1_
# Regression
res.crossval$k$cor[k_sparse.name,"empirical"]                 <- mod.train$cor_
res.crossval$k$cor[k_sparse.name,"generalization"]            <- mod.test$cor_
} # if training and testing results exist
} # end of k_sparse loop
# if saving move one level up
if(!(clf$params$popSaveFile=="NULL"))
{
setwd("..")
}
} # end if null digest
# Compute FEATURE IMPORTANCE for each classifier in GENERALIZATION
if(clf$params$compute.importance & !isLearnerSota(clf))
{
# we compute the feature importance and BTR languages and algorithms
if(isClf(res.all[[i]]))
{
# if results are valid
if(!is.null(res.all[[i]]$fip))
{
# if object exist
if(!is.null(res.all[[i]]$fip$mda))
{
mda.all[res.all[[i]]$fip$feat.catalogue,i] <- res.all[[i]]$fip$mda
}
# if object exist
if(!is.null(res.all[[i]]$fip$sda))
{
sda.all[res.all[[i]]$fip$feat.catalogue,i] <- res.all[[i]]$fip$sda
}
# if object exist
if(!is.null(res.all[[i]]$fip$pda))
{
pda.all[res.all[[i]]$fip$feat.catalogue,i] <- res.all[[i]]$fip$pda
}
}
}else
{
# print out information (might be errors)
print(res.all[[i]])
next
}
} # end importance
# Do we need to return everything back ?
if(return.all)
{
res.crossval$nfold[[i]]             <- list(results = res_train,
resultsDigest = res_train.digest)
}
} # end for (dispatching results)
if(clf$params$verbose) cat("... All results from cross validation are dispatched\n")
# reorder results function
reorderByRownamesNumeric <- function(mat)
{
ind <- as.numeric(gsub("k_","",rownames(mat)))
mat <- mat[order(ind),]
}
# reorder results
res.crossval$scores$empirical.auc       <- reorderByRownamesNumeric(res.crossval$scores$empirical.auc)
res.crossval$scores$empirical.acc       <- reorderByRownamesNumeric(res.crossval$scores$empirical.acc)
res.crossval$scores$empirical.rec       <- reorderByRownamesNumeric(res.crossval$scores$empirical.rec)
res.crossval$scores$empirical.prc       <- reorderByRownamesNumeric(res.crossval$scores$empirical.prc)
res.crossval$scores$empirical.f1s       <- reorderByRownamesNumeric(res.crossval$scores$empirical.f1s)
res.crossval$scores$empirical.cor       <- reorderByRownamesNumeric(res.crossval$scores$empirical.cor)
res.crossval$scores$generalization.auc  <- reorderByRownamesNumeric(res.crossval$scores$generalization.auc)
res.crossval$scores$generalization.acc  <- reorderByRownamesNumeric(res.crossval$scores$generalization.acc)
res.crossval$scores$generalization.rec  <- reorderByRownamesNumeric(res.crossval$scores$generalization.rec)
res.crossval$scores$generalization.prc  <- reorderByRownamesNumeric(res.crossval$scores$generalization.prc)
res.crossval$scores$generalization.f1s  <- reorderByRownamesNumeric(res.crossval$scores$generalization.f1s)
res.crossval$scores$generalization.cor  <- reorderByRownamesNumeric(res.crossval$scores$generalization.cor)
# auc
if(!is.null(dim(res.crossval$scores$empirical.auc)))
{
res.crossval$scores$mean.auc            <- data.frame(cbind(rowMeans(res.crossval$scores$empirical.auc, na.rm = TRUE),
rowMeans(res.crossval$scores$generalization.auc, na.rm = TRUE)))
colnames(res.crossval$scores$mean.auc)  <- c("empirical","generalization")
}
# accuracy
if(!is.null(dim(res.crossval$scores$empirical.acc)))
{
res.crossval$scores$mean.acc            <- data.frame(cbind(rowMeans(res.crossval$scores$empirical.acc, na.rm = TRUE),
rowMeans(res.crossval$scores$generalization.acc, na.rm = TRUE)))
colnames(res.crossval$scores$mean.acc)  <- c("empirical","generalization")
}
# recall
if(!is.null(dim(res.crossval$scores$empirical.rec)))
{
res.crossval$scores$mean.rec            <- data.frame(cbind(rowMeans(res.crossval$scores$empirical.rec, na.rm = TRUE),
rowMeans(res.crossval$scores$generalization.rec, na.rm = TRUE)))
colnames(res.crossval$scores$mean.rec)  <- c("empirical","generalization")
}
# precision
if(!is.null(dim(res.crossval$scores$empirical.prc)))
{
res.crossval$scores$mean.prc            <- data.frame(cbind(rowMeans(res.crossval$scores$empirical.prc, na.rm = TRUE),
rowMeans(res.crossval$scores$generalization.prc, na.rm = TRUE)))
colnames(res.crossval$scores$mean.prc)  <- c("empirical","generalization")
}
# f1-score
if(!is.null(dim(res.crossval$scores$empirical.f1s)))
{
res.crossval$scores$mean.f1s            <- data.frame(cbind(rowMeans(res.crossval$scores$empirical.f1s, na.rm = TRUE),
rowMeans(res.crossval$scores$generalization.f1s, na.rm = TRUE)))
colnames(res.crossval$scores$mean.f1s)  <- c("empirical","generalization")
}
# correlation
if(!is.null(dim(res.crossval$scores$empirical.cor)))
{
res.crossval$scores$mean.cor            <- data.frame(cbind(rowMeans(res.crossval$scores$empirical.cor, na.rm = TRUE),
rowMeans(res.crossval$scores$generalization.cor, na.rm = TRUE)))
colnames(res.crossval$scores$mean.cor)  <- c("empirical","generalization")
}
# adding results for feature importance
if(clf$params$compute.importance & !isLearnerSota(clf))
{
res.crossval$fip <- list(mda = mda.all,
sda = sda.all,
pda = pda.all,
fpf = rowSums(!is.na(mda.all)) # feature prevalenc in folds
)
}
return(res.crossval)
}
knitr::opts_chunk$set(echo = TRUE)
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest())
# load the data
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
#recover the vector y
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
#Divide the dataset into training and testing
proportion_test <- 0.2
taille_test <- round(proportion_test * ncol(mc.input$X))
indices_test <- sample(1:ncol(mc.input$X), taille_test)
X <- mc.input$X[,-indices_test]
X.test <- mc.input$X[,indices_test]
y <- as.vector(yvec[-indices_test])
y.test <- as.vector(yvec[indices_test])
clf <- terga1_ovo(nCores = 1,
seed = 1,
plot = TRUE
)
printy(clf) # print the object for more information
isClf(clf)  # test whether the object is a classifier
class(clf)  # the class of the classifier object
runit = TRUE
if(runit)
{
res_clf <- fit_OVO(X = X, y = y, clf = clf, cross.validate = TRUE, nfolds = 1); # class(res_clf)
# save results
save(res_clf, clf, file = "res_clf.rda", compression_level = 9)
}
# ... Database X is not a matrix! Converting ...
# ... Classification mode, computing factor(y) for speedup and robustness
# ... Loading feature correlation for speedup
# ... Correlation file loaded
# ... Storing data in the classifier object for speedup
# ... Computing ternary coefficients for speedup
# ... One seed found, setting by default
# ... Running the classifier terga2 with a single CPU
# ... Second and faster version of terga fitting based on Genetic Algorithm heuristics ...
# ... Cross validation mode
# ... Starting cross validation not in parallel
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ....................................................................................................
# ... Learning process is finished succesfuly
# ... Thank you for using Predomics. Don't forget to digest the results now.
#
# [1] "experiment" "predomics"
if(!runit)
{
load("res_clf.rda")
}
res_clf.dig <- digest(obj = res_clf, penalty = 0.75/100, plot = TRUE)
# get the best model
best.model <- res_clf.dig$best$model
printy(best.model)
grid.arrange(plotModel_ovo(best.model, X=X, y=y, sort.features = FALSE, feature.name = TRUE),
plotModel_ovo(best.model, X=X, y=y, sort.features = FALSE, feature.name = TRUE, importance = TRUE),ncol=2)
best.model.test <- evaluateModel_ovo(mod = best.model, X = X.test, y = y.test, clf = clf, eval.all = TRUE, force.re.evaluation = TRUE, mode = "test")
printy(best.model.test)
# get the best model
best.model <- res_clf.dig$best$model
printy(best.model)
grid.arrange(plotModel_ovo(best.model, X=X, y=y, sort.features = FALSE, feature.name = TRUE),
plotModel_ovo(best.model, X=X, y=y, sort.features = FALSE, feature.name = TRUE, importance = TRUE),ncol=2)
best.model.test <- evaluateModel_ovo(mod = best.model, X = X.test, y = y.test, clf = clf, eval.all = TRUE, force.re.evaluation = TRUE, mode = "test")
printy(best.model.test)
nClasse <- unique(y)
list_y <- list()
list_X <- list()
k <- 1
for (i in 1:(length(nClasse)-1)) {
for (j in (i+1):length(nClasse)) {
class_i <- nClasse[i]
class_j <- nClasse[j]
indices <- which(y == class_i | y == class_j)
y_pair <- y[indices]
X_pair <- X[,indices]
list_y[[k]] <- y_pair
list_X[[k]] <- X_pair
k <- k + 1
}
}
nClasse <- unique(y.test)
list_y.test <- list()
list_X.test <- list()
k <- 1
for (i in 1:(length(nClasse)-1)) {
for (j in (i+1):length(nClasse)) {
class_i <- nClasse[i]
class_j <- nClasse[j]
indicess <- which(y.test == class_i | y.test == class_j)
y_paire <- y[indicess]
X_paire <- X[,indicess]
list_y.test[[k]] <- y_paire
list_X.test[[k]] <- X_paire
k <- k + 1
}
}
# we recover the first output to apply the plot
X <- list_X[[1]]
y <- list_y[[1]]
X.test <- list_X.test[[1]]
y.test <- list_y.test[[1]]
tmp <- plotAUC(best.model$score_, y, percent = TRUE); rm(tmp)
# create the roc objects
rocobj.train <- roc(y ~ best.model$score_)
rocobj.test <- roc(y.test ~ best.model.test$score_)
# make the plot
ggroc(list(train = rocobj.train, test = rocobj.test))
best.model
best.model.test <- evaluateModel_ovo(mod = best.model, X = X.test, y = y.test, clf = clf, eval.all = TRUE, force.re.evaluation = TRUE, mode = "test")
printy(best.model.test)
best.model.test
