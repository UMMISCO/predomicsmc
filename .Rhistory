{
warning('This method does not exist! Try one of these: short, long or str')
}
)
list_res[[km]] <- res
}
res <- list()
res <- list_res
return(res)
}
#' Prints a population of model objects as text.
#'
#' @description Prints a population of model objects as text
#' @param obj: a population of models to plot
#' @param method: if "digested" a short sumary (one line) will be printed, otherwise the method will contain the
#' specific way to print a model through the printModel() routine
#' @param score: which score to show in the fit (default:fit_)
#' @param indent: a string (default:'tab---') that will precede each element of the object.
#' @export
printPopulation_mc <- function(obj, method = "short", score = "fit_", indent="")
{
# sanity check
if(!isPopulation(obj))
{
return(NULL)
warning("printPopulation: the object to print is not a valid Population of models")
}
switch(method,
digested={
spar <- populationGet_X("eval.sparsity")(obj)
pop.name <- unique(spar)
if(length(pop.name)==1)
{
attribute.name <- paste0("k_",pop.name)
}else
{
attribute.name <- "k_mixed"
}
ptdf <- populationToDataFrame(obj) # convert model population to a dataframe for easy access
#attribute.name <- ""
attribute.value <- paste(length(obj), "models ...",
paste(paste(ptdf$learner, ptdf$language, signif(ptdf$fit_,2), ptdf$eval.sparsity, sep="_")[1:min(5,length(obj))],collapse = "; "))
cat(paste(indent,attribute.name,": ",attribute.value, "\n",sep=""))
},
short =,
str =,
long ={
for(i in 1:length(obj))
{
mod <- obj[[i]]
print(paste(i, printModel_mc(mod = mod, method = method, score = score), sep=":"))
}
},
{
print('printPopulation: please provide a valid method (digested/short/long/str)')
}
)
}
#' Prints as text the detail on a given ModelCollection object
#'
#' @description This function prints a ModelCollection object. For each k_sparsity it will show some detail of
#' the maximum first models
#' @param obj: a ModelCollection object
#' @param indent: a string (default:'tab---') that will precede each element of the object for the "long" method.
#' @param method: the output method (default:long) will print for each k_sparsity a short information of the population of models,
#' while the short method will output the number of models for each k_sparsity
#' @return NULL if the object is not a valid ModelCollection.
#' @export
printModelCollection_mc <- function(obj, indent = "\t--- ", method = "long")
{
if(!isModelCollection(obj))
{
return(NULL)
warning("printModelCollection: the object to print is not a valid experiment.")
}
switch(method,
short={
paste(names(obj), unlist(lapply(obj,length)), sep=": ")
},
long={
# for each k-sparsity show the number of models and some information on the first ones.
for(i in 1:length(obj))
{
printPopulation_mc(obj = obj[[i]], method = "digested", indent = indent)
}
},
{
print('printModelCollection: please provide a valid method (short/long)')
}
)
}
#' Prints as text the detail on a given Experiment object
#'
#' @description This function prints a summary of an Experiment object.
#' @param obj: an Experiment object
#' @param indent: a string (default:'tab---') that will precede each element of the object.
#' @return NULL if the object is not a valid Experiment
#' @export
printExperiment_mc <- function(obj, indent = "\t--- ")
{
if(!isExperiment(obj))
{
return(NULL)
warning("printExperiment: the object to print is not a valid experiment.")
}
cat("========== Experiment ==========\n")
for(i in 1:length(obj$classifier$experiment))
{
attribute.name <- names(obj$classifier$experiment)[i]
attribute.value <- obj$classifier$experiment[[i]]
cat(paste(indent,attribute.name,": ",attribute.value, "\n",sep=""))
}
if(!is.null(obj$crossVal))
{
cat("========== Cross validation ==========\n")
cat(paste(indent, "total folds",": ",ncol(obj$crossVal$scores$empirical.auc), "\n",sep=""))
cat(paste(indent, "folds",": ",ncol(obj$crossVal$scores$empirical.auc)/length(obj$classifier$params$seed), "\n",sep=""))
cat(paste(indent, "times",": ",length(obj$classifier$params$seed), "\n",sep=""))
cat(paste(indent, "seeds",": ",paste(obj$classifier$params$seed, collapse = ", "), "\n",sep=""))
}else
{
cat("========== Cross validation ==========\n")
cat(paste(indent, "total folds",": ",0, "\n",sep=""))
cat(paste(indent, "folds",": ",0, "\n",sep=""))
cat(paste(indent, "times",": ",length(obj$classifier$params$seed), "\n",sep=""))
cat(paste(indent, "seeds",": ",paste(obj$classifier$params$seed, collapse = ", "), "\n",sep=""))
}
# Detailed learner options
if(!is.null(obj$classifier$params))
{
cat("========== Learner ==========\n")
cat(paste(indent,"learner: ", obj$classifier$learner, "\n",sep=""))
for(i in 1:length(obj$classifier$params))
{
attribute.name <- names(obj$classifier$params)[i]
attribute.value <- obj$classifier$params[[i]]
if(length(attribute.value) > 1) # for sparsity for instance
{
if(!is.list(attribute.value))
{
cat(paste(indent, attribute.name,": ",paste(attribute.value, collapse = ","), "\n",sep=""))
}
}else
{
if(is.function(attribute.value)) # In case of functions
{
cat(paste(indent, attribute.name,": function","\n",sep=""))
}else
{
cat(paste(indent, attribute.name,": ",attribute.value, "\n",sep=""))
}
}
}
if(obj$classifier$learner == "metal")
{
nclf <- length(obj$classifier$params$list.clfs)-1
for(i in 1:nclf)
{
cat(paste(indent, obj$classifier$params$list.clfs[[i]]$experiment$id,"\n",sep = ""))
}
}
}
# Detailed learner options
if(isModelCollection(obj$classifier$models))
{
cat("========== Model Collection ==========\n")
printModelCollection_mc(obj$classifier$models)
}
}
#' Prints as text the detail on a given Classifier object
#'
#' @description This function prints a summary of a Classifier object.
#' @param obj: a Classifier object
#' @param indent: a string (default:'tab---') that will precede each element of the object.
#' @return NULL if the object is not a valid Classifier
#' @export
printClassifier_mc <- function(obj, indent="\t--- ")
{
# sanity check
if(!isClf(obj))
{
return(NULL)
warning("printClassifier: the object to print is not a valid Classifier")
}
# Global experiment information
if(!is.null(obj$experiment))
{
cat("========== Experiment ==========\n")
for(i in 1:length(obj$experiment)){
attribute.name <- names(obj$experiment)[i]
attribute.value <- obj$experiment[[i]]
cat(paste(indent,attribute.name,": ",attribute.value, "\n",sep=""))
}
}
# Detailed learner options
if(!is.null(obj$params))
{
cat("========== Learner ==========\n")
cat(paste(indent,"learner: ", obj$learner, "\n",sep=""))
for(i in 1:length(obj$params))
{
attribute.name <- names(obj$params)[i]
attribute.value <- obj$params[[i]]
if(length(attribute.value)>1) # for sparsity for instance
{
if(!is.list(attribute.value))
{
cat(paste(indent, attribute.name,": ",paste(attribute.value, collapse = ","), "\n",sep=""))
}
}else
{
if(is.function(attribute.value)) # In case of functions
{
cat(paste(indent, attribute.name,": function","\n",sep=""))
}else
{
cat(paste(indent, attribute.name,": ",attribute.value, "\n",sep=""))
}
}
}
if(obj$learner=="metal")
{
nclf <- length(obj$params$list.clfs)-1
for(i in 1:nclf)
{
cat(paste(indent, obj$params$list.clfs[[i]]$experiment$id,"\n",sep = ""))
}
}
}
# Detailed learner options
if(isModelCollection(obj$models))
{
cat("========== Model Collection ==========\n")
printModelCollection_mc(obj$models)
}
}
#' Prints as text the detail on a given object from the predomics package.
#'
#' @description This function will summarize any of the predomics package objects such as can be an Experiment,
#' a Model, a Population of models or a ModelCollection
#' @param obj: an object from the predomics object
#' @return NULL
#' @export
printy_mc <- function(obj)
{
type = NA
if(isModel(obj))
{
type <- "model"
}
if(isPopulation(obj))
{
type <- "population"
}
if(isClf(obj))
{
type <- "classifier"
}
if(isExperiment(obj))
{
type <- "experiment"
}
if(isModelCollection(obj))
{
type <- "model.collection"
}
switch(type,
model={
print(paste("Summary of Model object"))
printModel_mc(mod = obj, method = "long")
},
population={
print(paste("Summary of a population of models with",length(obj),"models"))
printPopulation_mc(obj = obj[1:min(5,length(obj))], method = "long")
if(length(obj) > 5) print("...")
},
model.collection={
print(paste("Summary of a ModelCollection object with",length(obj),"populations of models"))
printModelCollection_mc(obj = obj, method = "long")
},
experiment={
print(paste("Summary of Experiment object"))
printExperiment_mc(obj)
},
classifier={
print(paste("Summary of Classifier object"))
printClassifier_mc(obj)
},
{
print('printy: please provide valid predomics model')
}
)
}
printy_mc(mod)
library(mcpredomics)
knitr::opts_chunk$set(echo = TRUE)
printy_mc(mod)
pop = res_clf_ovo$classifier$models
Population_weighted_Aggregation <- evaluatePopulation_aggregation(pop = pop, y = y.test, X = X.test, force.re.evaluation = TRUE, clf,  aggregation = "weightedAggregation")
data <- data.frame(
Metric = rep(c("Accuracy", "Precision", "Recall", "F1"), each = 4),
Value = c(
extract_metric_values(Population_weighted_Aggregation, "accuracy_") * 100,
extract_metric_values(Population_weighted_Aggregation, "precision_") * 100,
extract_metric_values(Population_weighted_Aggregation, "recall_") * 100,
extract_metric_values(Population_weighted_Aggregation, "f1_") * 100
),
Method = rep(c("K_2", "K_3", "K_4", "K_5"), times = 4)
)
colors <- c("Accuracy" = "#1f77b4", "Precision" = "#ff7f0e", "Recall" = "#2ca02c", "F1" = "#d62728")
ggplot(data, aes(x = Method, y = Value, fill = Metric)) +
geom_boxplot(color = "black", outlier.color = "black") +
scale_fill_manual(values = colors) +  # Utiliser la palette de couleurs définie
facet_wrap(~Metric, scales = "free_y") +
labs(x = "Metrics", y = "Value", title = "Metrics by sparsity") +
ylim(0, 100) +  # Définir la limite de l'axe y de 0 à 100
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA),
legend.title = element_blank(),
legend.position = "bottom")
best_model_weighted_aggregation = Population_weighted_Aggregation[[4]][[1]]
best_model_weighted_aggregation$accuracy_
best_model_weighted_aggregation$confusionMatrix_
best_model_weighted_aggregation$coeffs_
best_model_weighted_aggregation$predictions_aggre
printy_mc(mod = best_model_weighted_aggregation)
printy_mc(best_model_weighted_aggregation)
load("res_clf_ova.rda")
bests_models_parsimony_ova <- digestmc(obj = res_clf_ova, penalty = 0.75/100, plot = TRUE)
pop = res_clf_ova$classifier$models
Population_newApproach_Aggregation <- evaluatePopulation_aggregation(pop = pop, y = y.test, X = X.test, force.re.evaluation = TRUE, clf,  aggregation = "NewApproach")
data <- data.frame(
Metric = rep(c("Accuracy", "Precision", "Recall", "F1"), each = 4),
Value = c(
extract_metric_values(Population_newApproach_Aggregation, "accuracy_") * 100,
extract_metric_values(Population_newApproach_Aggregation, "precision_") * 100,
extract_metric_values(Population_newApproach_Aggregation, "recall_") * 100,
extract_metric_values(Population_newApproach_Aggregation, "f1_") * 100
),
Method = rep(c("K_2", "K_3", "K_4", "K_5"), times = 4)
)
colors <- c("Accuracy" = "#1f77b4", "Precision" = "#ff7f0e", "Recall" = "#2ca02c", "F1" = "#d62728")
ggplot(data, aes(x = Method, y = Value, fill = Metric)) +
geom_boxplot(color = "black", outlier.color = "black") +
scale_fill_manual(values = colors) +  # Utiliser la palette de couleurs définie
facet_wrap(~Metric, scales = "free_y") +
labs(x = "Metrics", y = "Value", title = "Metrics by sparsity") +
ylim(0, 100) +  # Définir la limite de l'axe y de 0 à 100
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA),
legend.title = element_blank(),
legend.position = "bottom")
best_model_newapproach_aggregation = Population_newApproach_Aggregation[[4]][[1]]
best_model_newapproach_aggregation$accuracy_
best_model_newapproach_aggregation$confusionMatrix_
best_model_newapproach_aggregation$coeffs_
best_model_newapproach_aggregation$predictions_aggre
printy_mc(best_model_newapproach_aggregation)
pop = res_clf_ova$classifier$models
Population_maximization_Aggregation <- evaluatePopulation_aggregation(pop = pop, y = y.test, X = X.test, force.re.evaluation = TRUE, clf,  aggregation = "maximizationAggregation")
data <- data.frame(
Metric = rep(c("Accuracy", "Precision", "Recall", "F1"), each = 4),
Value = c(
extract_metric_values(Population_maximization_Aggregation, "accuracy_") * 100,
extract_metric_values(Population_maximization_Aggregation, "precision_") * 100,
extract_metric_values(Population_maximization_Aggregation, "recall_") * 100,
extract_metric_values(Population_maximization_Aggregation, "f1_") * 100
),
Method = rep(c("K_2", "K_3", "K_4", "K_5"), times = 4)
)
colors <- c("Accuracy" = "#1f77b4", "Precision" = "#ff7f0e", "Recall" = "#2ca02c", "F1" = "#d62728")
ggplot(data, aes(x = Method, y = Value, fill = Metric)) +
geom_boxplot(color = "black", outlier.color = "black") +
scale_fill_manual(values = colors) +  # Utiliser la palette de couleurs définie
facet_wrap(~Metric, scales = "free_y") +
labs(x = "Metrics", y = "Value", title = "Metrics by sparsity") +
ylim(0, 100) +  # Définir la limite de l'axe y de 0 à 100
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA),
legend.title = element_blank(),
legend.position = "bottom")
best_model_maximization_aggregation = Population_maximization_Aggregation[[4]][[1]]
best_model_maximization_aggregation$accuracy_
best_model_maximization_aggregation$confusionMatrix_
best_model_maximization_aggregation$coeffs_
best_model_maximization_aggregation$predictions_aggre
printy_mc(best_model_maximization_aggregation)
knitr::opts_chunk$set(echo = TRUE)
load("res_clf_ovo.rda")
bests_models_parsimony_ovo <- digestmc(obj = res_clf_ovo, penalty = 0.75/100, plot = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(mcpredomics)
library(predomics)
library(ggplot2)
library(gridExtra)
library(pROC)
library(reshape2)
library(randomForest)
library(caret)
library(gtools)
chemin_du_dataset <- system.file("data", "mc.input.Rda", package = "mcpredomics")
load(chemin_du_dataset)
set.seed(123)
yvec <- mc.input$y$Enterotype[match(colnames(mc.input$X), rownames(mc.input$y))]
indices_tries <- order(yvec)
yvec_trie <- yvec[indices_tries]
X_general  <- mc.input$X[,indices_tries]
# Create an index vector for data partitioning
X_general <- X_general[rowSums(X_general)!=0,]; dim(X_general) # filter out variables with only zero values
X_general <- filterNoSignal(X = X_general, side = 1, threshold = "auto", verbose = FALSE); dim(X_general)
set.seed(42)
y = as.vector(yvec_trie)
X = X_general
# Number of desired samples in each class
nombre_echantillons_par_classe <- min(table(y))
# Function to balance the classes
equilibrer_classes <- function(y, X, nombre_echantillons_par_classe,seed =123) {
classes <- unique(y)
indices_equilibres <- integer(0)
for (classe in classes) {
indices_classe <- which(y == classe)
set.seed(seed)
indices_equilibres <- c(indices_equilibres, sample(indices_classe, nombre_echantillons_par_classe))
}
return(list(y = y[indices_equilibres], X = X[, indices_equilibres]))
}
donnees_equilibrees <- equilibrer_classes(y, X, nombre_echantillons_par_classe)
y_equilibre <- donnees_equilibrees$y
X_equilibre <- donnees_equilibrees$X
# Verify the distribution after balancing
set.seed(42)
indices_division <- createDataPartition(y_equilibre, p = 0.8, list = FALSE)
# Split yvec_trie into 80% train and 20% test
y <- as.vector(y_equilibre[indices_division])
y.test <- as.vector(y_equilibre[-indices_division])
X <- X_equilibre[,indices_division]
X.test <- X_equilibre[,-indices_division]
table(y)
table(y.test)
dim(X)
dim(X.test)
clf <- terBeam_mc(sparsity = c(2,3,4,5),
max.nb.features = 1000,
seed = 1,
nCores = 1,
evalToFit = "accuracy_",
objective = "auc",
experiment.id = "terBeam_mc",
experiment.save = "nothing")
printy(clf)
load("res_clf_ovo.rda")
bests_models_parsimony_ovo <- digestmc(obj = res_clf_ovo, penalty = 0.75/100, plot = TRUE)
pop = res_clf_ovo$classifier$models
Population_voting_Aggregation <- evaluatePopulation_aggregation(pop = pop, y = y.test, X = X.test, force.re.evaluation = TRUE, clf,  aggregation = "votingAggregation")
# Sauvegarder l'objet sous forme d'un fichier .rda
save(Population_voting_Aggregation, file = "Voting_Aggregation_FBM.rda")
load("Voting_Aggregation_FBM.rda")
extract_metric_values <- function(aggregation_list, metric_name) {
values <- c()
for (i in seq_along(aggregation_list)) {
values <- c(values, aggregation_list[[i]][[1]][[metric_name]])
}
return(values)
}
data <- data.frame(
Metric = rep(c("Accuracy", "Precision", "Recall", "F1"), each = 4),
Value = c(
extract_metric_values(Voting_Aggregation_FBM, "accuracy_") * 100,
extract_metric_values(Voting_Aggregation_FBM, "precision_") * 100,
extract_metric_values(Voting_Aggregation_FBM, "recall_") * 100,
extract_metric_values(Voting_Aggregation_FBM, "f1_") * 100
),
Method = rep(c("K_2", "K_3", "K_4", "K_5"), times = 4)
)
pop = res_clf_ovo$classifier$models
Voting_Aggregation_FBM <- evaluatePopulation_aggregation(pop = pop, y = y.test, X = X.test, force.re.evaluation = TRUE, clf,  aggregation = "votingAggregation")
# Sauvegarder l'objet sous forme d'un fichier .rda
save(Voting_Aggregation_FBM, file = "Voting_Aggregation_FBM.rda")
load("Voting_Aggregation_FBM.rda")
extract_metric_values <- function(aggregation_list, metric_name) {
values <- c()
for (i in seq_along(aggregation_list)) {
values <- c(values, aggregation_list[[i]][[1]][[metric_name]])
}
return(values)
}
data <- data.frame(
Metric = rep(c("Accuracy", "Precision", "Recall", "F1"), each = 4),
Value = c(
extract_metric_values(Voting_Aggregation_FBM, "accuracy_") * 100,
extract_metric_values(Voting_Aggregation_FBM, "precision_") * 100,
extract_metric_values(Voting_Aggregation_FBM, "recall_") * 100,
extract_metric_values(Voting_Aggregation_FBM, "f1_") * 100
),
Method = rep(c("K_2", "K_3", "K_4", "K_5"), times = 4)
)
colors <- c("Accuracy" = "#1f77b4", "Precision" = "#ff7f0e", "Recall" = "#2ca02c", "F1" = "#d62728")
ggplot(data, aes(x = Method, y = Value, fill = Metric)) +
geom_boxplot(color = "black", outlier.color = "black") +
scale_fill_manual(values = colors) +  # Utiliser la palette de couleurs définie
facet_wrap(~Metric, scales = "free_y") +
labs(x = "Metrics", y = "Value", title = "Metrics by sparsity") +
ylim(0, 100) +  # Définir la limite de l'axe y de 0 à 100
theme_minimal() +
theme(panel.border = element_rect(color = "black", fill = NA),
legend.title = element_blank(),
legend.position = "bottom")
Voting_Aggregation_FBM = Voting_Aggregation_FBM[[4]][[1]]
Voting_Aggregation_FBM$accuracy_
Voting_Aggregation_FBM$confusionMatrix_
Voting_Aggregation_FBM$coeffs_
Voting_Aggregation_FBM$predictions_aggre
